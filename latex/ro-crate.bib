@article{doi:10.1109/MCC.2014.52,
author={Chard, Kyle and Tuecke, Steven and Foster, Ian},
journal={IEEE Cloud Computing}, 
title={Efficient and Secure Transfer, Synchronization, and Sharing of Big Data}, 
year={2014},
volume={1},
number={3},
pages={46-55},
doi={10.1109/MCC.2014.52}
}
@article{doi:10.1109/MCSE.2016.92,
title={A case for data commons: toward data science as a service},
author={Grossman, Robert L and Heath, Allison and Murphy, Mark and Patterson, Maria and Wells, Walt},
journal={Computing in science \& engineering},
volume={18},
doi = {10.1109/MCSE.2016.92},
number={5},
pages={10--20},
year={2016},
publisher={IEEE}
}
@article{doi:10.1093/nar/gkq429,
title = {{myExperiment}: a repository and social network for the sharing of bioinformatics workflows.},
author = {Goble, Carole A and Bhagat, Jiten and Aleksejevs, Sergejs and Cruickshank, Don and Michaelides, Danius and Newman, David and Borkum, Mark and Bechhofer, Sean and Roos, Marco and Li, Peter and De Roure, David},
pages = {W677-82},
year = {2010},
month = {jul},
day = {1},
urldate = {2021-05-04},
journal = {Nucleic Acids Research},
volume = {38},
number = {Web Server issue},
doi = {10.1093/nar/gkq429},
pmid = {20501605},
pmcid = {PMC2896080},
sciwheel-projects = {ro-crate-paper},
abstract = {{myExperiment} (http://www.myexperiment.org) is an online research environment that supports the social sharing of bioinformatics workflows. These workflows are procedures consisting of a series of computational tasks using web services, which may be performed on data from its retrieval, integration and analysis, to the visualization of the results. As a public repository of workflows, {myExperiment} allows anybody to discover those that are relevant to their research, which can then be reused and repurposed to their specific requirements. Conversely, developers can submit their workflows to {myExperiment} and enable them to be shared in a secure manner. Since its release in 2007, {myExperiment} currently has over 3500 registered users and contains more than 1000 workflows. The social aspect to the sharing of these workflows is facilitated by registered users forming virtual communities bound together by a common interest or research project. Contributors of workflows can build their reputation within these communities by receiving feedback and credit from individuals who reuse their work. Further documentation about {myExperiment} including its {REST} web service is available from http://wiki.myexperiment.org. Feedback and requests for support can be sent to bugs@myexperiment.org.}
}
@article{doi:10.1038/sdata.2016.18,
title = {The {FAIR} Guiding Principles for scientific data management and stewardship.},
author = {Wilkinson, Mark D and Dumontier, Michel and Aalbersberg, I Jsbrand Jan and Appleton, Gabrielle and Axton, Myles and Baak, Arie and Blomberg, Niklas and Boiten, Jan-Willem and da Silva Santos, Luiz Bonino and Bourne, Philip E and Bouwman, Jildau and Brookes, Anthony J and Clark, Tim and Crosas, Mercè and Dillo, Ingrid and Dumon, Olivier and Edmunds, Scott and Evelo, Chris T and Finkers, Richard and Gonzalez-Beltran, Alejandra and Gray, Alasdair J G and Groth, Paul and Goble, Carole and Grethe, Jeffrey S and Heringa, Jaap and 't Hoen, Peter A C and Hooft, Rob and Kuhn, Tobias and Kok, Ruben and Kok, Joost and Lusher, Scott J and Martone, Maryann E and Mons, Albert and Packer, Abel L and Persson, Bengt and Rocca-Serra, Philippe and Roos, Marco and van Schaik, Rene and Sansone, Susanna-Assunta and Schultes, Erik and Sengstag, Thierry and Slater, Ted and Strawn, George and Swertz, Morris A and Thompson, Mark and van der Lei, Johan and van Mulligen, Erik and Velterop, Jan and Waagmeester, Andra and Wittenburg, Peter and Wolstencroft, Katherine and Zhao, Jun and Mons, Barend},
pages = {160018},
year = {2016},
month = {mar},
day = {15},
urldate = {2018-07-13},
journal = {Scientific data},
volume = {3},
issn = {2052-4463},
doi = {10.1038/sdata.2016.18},
pmid = {26978244},
pmcid = {PMC4792175},
sciwheel-projects = {ro-crate-paper},
abstract = {There is an urgent need to improve the infrastructure supporting the reuse of scholarly data. A diverse set of stakeholders-representing academia, industry, funding agencies, and scholarly publishers-have come together to design and jointly endorse a concise and measureable set of principles that we refer to as the {FAIR} Data Principles. The intent is that these may act as a guideline for those wishing to enhance the reusability of their data holdings. Distinct from peer initiatives that focus on the human scholar, the {FAIR} Principles put specific emphasis on enhancing the ability of machines to automatically find and use the data, in addition to supporting its reuse by individuals. This Comment is the first formal publication of the {FAIR} Principles, and includes the rationale behind them, and some exemplar implementations in the community.}
}
@article{doi:10.1186/1471-2105-11-S12-S5,
title = {Community-driven computational biology with Debian Linux.},
author = {Möller, Steffen and Krabbenhöft, Hajo Nils and Tille, Andreas and Paleino, David and Williams, Alan and Wolstencroft, Katy and Goble, Carole and Holland, Richard and Belhachemi, Dominique and Plessy, Charles},
pages = {S5},
year = {2010},
month = {dec},
day = {21},
urldate = {2021-05-04},
journal = {{BMC} Bioinformatics},
volume = {11 Suppl 12},
doi = {10.1186/1471-2105-11-S12-S5},
pmid = {21210984},
pmcid = {PMC3040531},
sciwheel-projects = {ro-crate-paper},
abstract = {{BACKGROUND}: The Open Source movement and its technologies are popular in the bioinformatics community because they provide freely available tools and resources for research. In order to feed the steady demand for updates on software and associated data, a service infrastructure is required for sharing and providing these tools to heterogeneous computing environments. {RESULTS}: The Debian Med initiative provides ready and coherent software packages for medical informatics and bioinformatics. These packages can be used together in Taverna workflows via the {UseCase} plugin to manage execution on local or remote machines. If such packages are available in cloud computing environments, the underlying hardware and the analysis pipelines can be shared along with the software. {CONCLUSIONS}: Debian Med closes the gap between developers and users. It provides a simple method for offering new releases of software and data resources, thus provisioning a local infrastructure for computational biology. For geographically distributed teams it can ensure they are working on the same versions of tools, in the same conditions. This contributes to the world-wide networking of researchers.}
}
@article{doi:10.1016/j.future.2011.08.004,
title = {Why linked data is not enough for scientists},
author = {Bechhofer, Sean and Buchan, Iain and De Roure, David and Missier, Paolo and Ainsworth, John and Bhagat, Jiten and Couch, Philip and Cruickshank, Don and Delderfield, Mark and Dunlop, Ian and Gamble, Matthew and Michaelides, Danius and Owen, Stuart and Newman, David and Sufi, Shoaib and Goble, Carole},
pages = {599-611},
year = {2013},
month = {feb},
urldate = {2021-05-04},
journal = {Future Generation Computer Systems},
volume = {29},
number = {2},
issn = {0167739X},
doi = {10.1016/j.future.2011.08.004},
sciwheel-projects = {ro-crate-paper},
abstract = {Scientific data represents a significant portion of the linked open data cloud and scientists stand to benefit from the data fusion capability this will afford. Publishing linked data into the cloud, however, does not ensure the required reusability. Publishing has requirements of provenance, quality, credit, attribution and methods to provide the reproducibility that enables validation of results. In this paper we make the case for a scientific data publication model on top of linked data and introduce the notion of Research Objects as first class citizens for sharing and publishing.}
}
@article{doi:10.1016/j.future.2017.01.012,
title = {Scientific workflows for computational reproducibility in the life sciences: Status, challenges and opportunities},
author = {Cohen-Boulakia, Sarah and Belhajjame, Khalid and Collin, Olivier and Chopard, Jérôme and Froidevaux, Christine and Gaignard, Alban and Hinsen, Konrad and Larmande, Pierre and Bras, Yvan Le and Lemoine, Frédéric and Mareuil, Fabien and Ménager, Hervé and Pradal, Christophe and Blanchet, Christophe},
pages = {284-298},
year = {2017},
month = {oct},
urldate = {2018-07-13},
journal = {Future Generation Computer Systems},
volume = {75},
issn = {0167739X},
doi = {10.1016/j.future.2017.01.012},
sciwheel-projects = {ro-crate-paper},
abstract = {With the development of new experimental technologies, biologists are faced with an avalanche of data to be computationally analyzed for scientific advancements and discoveries to emerge. Faced with the complexity of analysis pipelines, the large number of computational tools, and the enormous amount of data to manage, there is compelling evidence that many if not most scientific discoveries will not stand the test of time: increasing the reproducibility of computed results is of paramount importance. The objective we set out in this paper is to place scientific workflows in the context of reproducibility. To do so, we define several kinds of repro-ducibility that can be reached when scientific workflows are used to perform experiments. We characterize and define the criteria that need to be catered for by reproducibility-friendly scientific workflow systems, and use such criteria to place several representative and widely used workflow systems and companion tools within such a framework. We also discuss the remaining challenges posed by reproducible scientific workflows in the life sciences. Our study was guided by three use cases from the life science domain involving in silico experiments.}
}
@article{doi:10.1007/s41019-017-0050-4,
title = {Robust Cross-Platform Workflows: How Technical and Scientific Communities Collaborate to Develop, Test and Share Best Practices for Data Analysis},
author = {Möller, Steffen and Prescott, Stuart W. and Wirzenius, Lars and Reinholdtsen, Petter and Chapman, Brad and Prins, Pjotr and Soiland-Reyes, Stian and Klötzl, Fabian and Bagnacani, Andrea and Kalaš, Matúš and Tille, Andreas and Crusoe, Michael R.},
pages = {232-244},
year = {2017},
month = {nov},
day = {16},
urldate = {2018-07-13},
journal = {Data Science and Engineering},
volume = {2},
number = {3},
issn = {2364-1185},
doi = {10.1007/s41019-017-0050-4},
sciwheel-projects = {ro-crate-paper},
abstract = {Information integration and workflow technologies for data analysis have always been major fields of investigation in bioinformatics. A range of popular workflow suites are available to support analyses in computational biology. Commercial providers tend to offer prepared applications remote to their clients. However, for most academic environments with local expertise, novel data collection techniques or novel data analysis, it is essential to have all the flexibility of open-source tools and open-source workflow descriptions. Workflows in data-driven science such as computational biology have considerably gained in complexity. New tools or new releases with additional features arrive at an enormous pace, and new reference data or concepts for quality control are emerging. A well-abstracted workflow and the exchange of the same across work groups have an enormous impact on the efficiency of research and the further development of the field. High-throughput sequencing adds to the avalanche of data available in the field; efficient computation and, in particular, parallel execution motivate the transition from traditional scripts and Makefiles to workflows. We here review the extant software development and distribution model with a focus on the role of integration testing and discuss the effect of common workflow language on distributions of open-source scientific software to swiftly and reliably provide the tools demanded for the execution of such formally described workflows. It is contended that, alleviated from technical differences for the execution on local machines, clusters or the cloud, communities also gain the technical means to test workflow-driven interaction across several software packages.}
}
@article{doi:10.3389/fninf.2017.00069,
title = {Re-run, Repeat, Reproduce, Reuse, Replicate: Transforming Code into Scientific Contributions.},
author = {Benureau, Fabien C Y and Rougier, Nicolas P},
pages = {69},
year = {2017},
urldate = {2021-05-04},
journal = {Frontiers in Neuroinformatics},
volume = {11},
doi = {10.3389/fninf.2017.00069},
pmid = {29354046},
pmcid = {PMC5758530},
sciwheel-projects = {ro-crate-paper},
abstract = {Scientific code is different from production software. Scientific code, by producing results that are then analyzed and interpreted, participates in the elaboration of scientific conclusions. This imposes specific constraints on the code that are often overlooked in practice. We articulate, with a small example, five characteristics that a scientific code in computational science should possess: re-runnable, repeatable, reproducible, reusable, and replicable. The code should be executable (re-runnable) and produce the same result more than once (repeatable); it should allow an investigator to reobtain the published results (reproducible) while being easy to use, understand and modify (reusable), and it should act as an available reference for any ambiguity in the algorithmic descriptions of the article (replicable).}
}
@article{doi:10.1038/s41592-018-0046-7,
title = {Bioconda: sustainable and comprehensive software distribution for the life sciences.},
author = {Grüning, Björn and Dale, Ryan and Sjödin, Andreas and Chapman, Brad A and Rowe, Jillian and Tomkins-Tinch, Christopher H and Valieris, Renan and Köster, Johannes and Bioconda Team},
pages = {475-476},
year = {2018},
urldate = {2018-07-13},
journal = {Nature Methods},
volume = {15},
number = {7},
issn = {1548-7091},
doi = {10.1038/s41592-018-0046-7},
pmid = {29967506},
sciwheel-projects = {ro-crate-paper}
}
@article{doi:10.1016/j.cels.2018.03.014,
title = {Practical computational reproducibility in the life sciences.},
author = {Grüning, Björn and Chilton, John and Köster, Johannes and Dale, Ryan and Soranzo, Nicola and van den Beek, Marius and Goecks, Jeremy and Backofen, Rolf and Nekrutenko, Anton and Taylor, James},
pages = {631-635},
year = {2018},
month = {jun},
day = {27},
urldate = {2018-07-13},
journal = {Cell Systems},
volume = {6},
number = {6},
issn = {24054712},
doi = {10.1016/j.cels.2018.03.014},
pmid = {29953862},
pmcid = {PMC6263957},
sciwheel-projects = {ro-crate-paper},
abstract = {Many areas of research suffer from poor reproducibility, particularly in computationally intensive domains where results rely on a series of complex methodological decisions that are not well captured by traditional publication approaches. Various guidelines have emerged for achieving reproducibility, but implementation of these practices remains difficult due to the challenge of assembling software tools plus associated libraries, connecting tools together into pipelines, and specifying parameters. Here, we discuss a suite of cutting-edge technologies that make computational reproducibility not just possible, but practical in both time and effort. This suite combines three well-tested components-a system for building highly portable packages of bioinformatics software, containerization and virtualization technologies for isolating reusable execution environments for these packages, and workflow systems that automatically orchestrate the composition of these packages for entire pipelines-to achieve an unprecedented level of computational reproducibility. We also provide a practical implementation and five recommendations to help set a typical researcher on the path to performing data analyses reproducibly. Copyright \copyright 2018 Elsevier Inc. All rights reserved.}
}
@techreport{doi:10.17487/rfc3987,
title = {Internationalized resource identifiers ({IRI}s)},
author = {Duerst, M. and Suignard, M.},
publisher = {{RFC} Editor},
url = {https://www.rfc-editor.org/info/rfc3987},
year = {2005},
month = {jan},
urldate = {2018-07-15},
doi = {10.17487/rfc3987},
sciwheel-projects = {ro-crate-paper}
}
@article{doi:10.1038/s41586-019-0965-1,
title = {A new genomic blueprint of the human gut microbiota.},
author = {Almeida, Alexandre and Mitchell, Alex L and Boland, Miguel and Forster, Samuel C and Gloor, Gregory B and Tarkowska, Aleksandra and Lawley, Trevor D and Finn, Robert D},
pages = {499-504},
year = {2019},
month = {feb},
day = {11},
urldate = {2021-05-04},
journal = {Nature},
volume = {568},
number = {7753},
issn = {0028-0836},
doi = {10.1038/s41586-019-0965-1},
pmid = {30745586},
pmcid = {PMC6784870},
sciwheel-projects = {ro-crate-paper},
abstract = {The composition of the human gut microbiota is linked to health and disease, but knowledge of individual microbial species is needed to decipher their biological roles. Despite extensive culturing and sequencing efforts, the complete bacterial repertoire of the human gut microbiota remains undefined. Here we identify 1,952 uncultured candidate bacterial species by reconstructing 92,143 metagenome-assembled genomes from 11,850 human gut microbiomes. These uncultured genomes substantially expand the known species repertoire of the collective human gut microbiota, with a 281\% increase in phylogenetic diversity. Although the newly identified species are less prevalent in well-studied populations compared to reference isolate genomes, they improve classification of understudied African and South American samples by more than 200\%. These candidate species encode hundreds of newly identified biosynthetic gene clusters and possess a distinctive functional capacity that might explain their elusive nature. Our work expands the known diversity of uncultured gut bacteria, which provides unprecedented resolution for taxonomic and functional characterization of the intestinal microbiota.}
}
@article{doi:10.1371/journal.pbio.3000099,
title = {Enabling precision medicine via standard communication of {HTS} provenance, analysis, and results.},
author = {Alterovitz, Gil and Dean, Dennis and Goble, Carole and Crusoe, Michael R and Soiland-Reyes, Stian and Bell, Amanda and Hayes, Anais and Suresh, Anita and Purkayastha, Anjan and King, Charles H and Taylor, Dan and Johanson, Elaine and Thompson, Elaine E and Donaldson, Eric and Morizono, Hiroki and Tsang, Hsinyi and Vora, Jeet K and Goecks, Jeremy and Yao, Jianchao and Almeida, Jonas S and Keeney, Jonathon and Addepalli, {KanakaDurga} and Krampis, Konstantinos and Smith, Krista M and Guo, Lydia and Walderhaug, Mark and Schito, Marco and Ezewudo, Matthew and Guimera, Nuria and Walsh, Paul and Kahsay, Robel and Gottipati, Srikanth and Rodwell, Timothy C and Bloom, Toby and Lai, Yuching and Simonyan, Vahan and Mazumder, Raja},
pages = {e3000099},
year = {2018},
month = {dec},
day = {31},
urldate = {2021-05-04},
journal = {{PLoS} Biology},
volume = {16},
number = {12},
doi = {10.1371/journal.pbio.3000099},
pmid = {30596645},
pmcid = {PMC6338479},
sciwheel-projects = {ro-crate-paper},
abstract = {A personalized approach based on a patient's or pathogen's unique genomic sequence is the foundation of precision medicine. Genomic findings must be robust and reproducible, and experimental data capture should adhere to findable, accessible, interoperable, and reusable ({FAIR}) guiding principles. Moreover, effective precision medicine requires standardized reporting that extends beyond wet-lab procedures to computational methods. The {BioCompute} framework (https://w3id.org/biocompute/1.3.0) enables standardized reporting of genomic sequence data provenance, including provenance domain, usability domain, execution domain, verification kit, and error domain. This framework facilitates communication and promotes interoperability. Bioinformatics computation instances that employ the {BioCompute} framework are easily relayed, repeated if needed, and compared by scientists, regulators, test developers, and clinicians. Easing the burden of performing the aforementioned tasks greatly extends the range of practical application. Large clinical trials, precision medicine, and regulatory submissions require a set of agreed upon standards that ensures efficient communication and documentation of genomic analyses. The {BioCompute} paradigm and the resulting {BioCompute} Objects ({BCOs}) offer that standard and are freely accessible as a {GitHub} organization (https://github.com/biocompute-objects) following the "Open-Stand.org principles for collaborative open standards development." With high-throughput sequencing ({HTS}) studies communicated using a {BCO}, regulatory agencies (e.g., Food and Drug Administration [{FDA}]), diagnostic test developers, researchers, and clinicians can expand collaboration to drive innovation in precision medicine, potentially decreasing the time and cost associated with next-generation sequencing workflow exchange, reporting, and regulatory reviews.}
}
@article{doi:10.3233/DS-190026,
title = {Towards {FAIR} principles for research software},
author = {Lamprecht, Anna-Lena and Garcia, Leyla and Kuzak, Mateusz and Martinez, Carlos and Arcila, Ricardo and Martin Del Pico, Eva and Dominguez Del Angel, Victoria and van de Sandt, Stephanie and Ison, Jon and Martinez, Paula Andrea and {McQuilton}, Peter and Valencia, Alfonso and Harrow, Jennifer and Psomopoulos, Fotis and Gelpi, Josep Ll. and Chue Hong, Neil and Goble, Carole and Capella-Gutierrez, Salvador},
pages = {1-23},
year = {2019},
month = {nov},
day = {13},
urldate = {2021-02-22},
journal = {Déviance et société},
issn = {24518492},
doi = {10.3233/DS-190026},
sciwheel-projects = {ro-crate-paper}
}
@article{doi:10.1162/dint_a_00033,
title = {{FAIR} Computational Workflows},
author = {Goble, Carole and Cohen-Boulakia, Sarah and Soiland-Reyes, Stian and Garijo, Daniel and Gil, Yolanda and Crusoe, Michael R. and Peters, Kristian and Schober, Daniel},
pages = {108-121},
year = {2019},
month = {nov},
day = {1},
volume = {2},
number = {1-2},
journal = {Data Intelligence},
issn = {2641-{435X}},
doi = {10.1162/dint_a_00033},
sciwheel-projects = {ro-crate-paper},
abstract = {Computational workflows describe the complex multi-step methods that are used for data collection, data preparation, analytics, predictive modelling, and simulation that lead to new data products. They can inherently contribute to the {FAIR} data principles: by processing data according to established metadata; by creating metadata themselves during the processing of data; and by tracking and recording data provenance. These properties aid data quality assessment and contribute to secondary data usage. Moreover, workflows are digital objects in their own right. This paper argues that {FAIR} principles for workflows need to address their specific nature in terms of their composition of executable software steps, their provenance, and their development.}
}
@article{doi:10.5281/zenodo.4605654,
title = {Implementing {FAIR} Digital Objects in the {EOSC}-Life Workflow Collaboratory},
author = {Goble, Carole and Soiland-Reyes, Stian and Bacall, Finn and Owen, Stuart and Williams, Alan and Eguinoa, Ignacio and Droesbeke, Bert and Leo, Simone and Pireddu, Luca and Rodríguez-Navas, Laura and Fernández, José Mª and Capella-Gutierrez, Salvador and Ménager, Hervé and Grüning, Björn and Serrano-Solano, Beatriz and Ewels, Philip and Coppens, Frederik},
year = {2021},
journal = {Zenodo},
doi = {10.5281/zenodo.4605654},
sciwheel-projects = {ro-crate-paper},
abstract = {The practice of performing computational processes using workflows has taken hold in the biosciences as the discipline becomes increasingly computational. The {COVID}-19 pandemic has spotlighted the importance of systematic and shared analysis of {SARS}-{CoV}-2 and its data processing pipelines. This is coupled with a drive in the community towards adopting {FAIR} practices (Findable, Accessible, Interoperable, and Reusable) not just for data, but also for workflows, and to improve the reproducibility of processes, both manual and computational. {EOSC}-Life brings together 13 of the Life Science {\textquoteleftESFRI\textquoteright} research infrastructures to create an open, digital and collaborative space for biological and medical research. The project is developing a cloud-based workflow collaboratory to drive implementation of {FAIR} workflows across disciplines and {RI} boundaries, and foster tool- focused collaborations and reuse between communities via the sharing of data analysis workflows. The collaboratory aims to provide a framework for researchers and workflow specialists to use and reuse workflows. As such it is an example of the Canonical Workflow Frameworks for Research ({CWFR}) vision in practice. {EOSC}-Life is made up of established research infrastructures ranging from biobanking and clinical trial management, through to coordinating biomedical imaging and plant phenotyping to multi-omic and systems-based data analysis. The heterogeneity of the disciplines is reflected in the diversity of their data analysis needs and practices and the variety of workflow management systems they use. Many have specialist platforms developed over years. Workflow management systems in common use include Galaxy, Snakemake, and Nextflow, and more specialist, domain-specific systems such as {SCIPION}. To serve the needs of this established and diverse community, {EOSC}-Life has developed {WorkflowHub} as an inclusive workflow registry, agnostic to any Workflow Management System ({WfMS}). {WorkflowHub} aims to incorporate their workflows in partnership with the {WfMS}, to embed the registration of workflows in the community processes, e.g. based on pre-existing workflow repositories. The registry adopts common practices, e.g.use of {GitHub} repositories, and supports integration with the ecosystem of tool packages, assisted by registries (bio.tools, biocontainers), and services for testing and benchmarking workflows ({OpenEBench}, {LifeMonitor}). As an umbrella registry, the Hub makes workflows Findable and Accessible by indexing workflows across workflow management systems and their native repositories, while providing rich standardized metadata. Interoperability and Reusability is supported by standardized descriptions of workflows and packaging of workflow components, developed in close collaboration with the communities. The {WorkflowHub} creates a place for registering and discovering libraries of workflows developed by collaborating teams, with suitable features for versioning, credit, analytics, and import/export needed to support the reuse of workflows, the development of sub-workflows as canonical steps and ultimately the identification of common patterns in the workflows. At the heart of the collaboratory is a Digital Object framework for documenting and exchanging workflows annotated with machine processable metadata produced and consumed by the participating platforms. The Digital Object framework is founded on several needs: Describing a workflow and its steps in a canonical, normalised and {WfMS} independent way: we use the Common Workflow Language ({CWL}), more specifically the Abstract {CWL} (non-executable) description variant to accompany the native workflow definitions. This presents the structure, composed tools and external interface in an interoperable way across workflow languages. {WfMS} can generate abstract {CWL}, already demonstrated for Galaxy, next to the \textquoteleftnative\textquoteright Galaxy workflow description. This language duality is an important retention aspect of reproducibility, as the structure and metadata of the workflow can be accessed independent of its native format as {CWL}, even if that may no longer be executable, capturing the canonical workflow in a {FAIR} format. The co-presence of the native format enables direct reuse in the specific {WfMS}, benefitting from all its features. Metadata about a workflow and its tools using a minimal information model: we use the Bioschemas profiles Computational Tool, Computational Workflow and Formal Parameter which are discipline independent, opinionated conventions for using schema.org annotations. Bioschemas enables us to capture and publish workflow registrations and their metadata as {FAIR} Digital Objects. The {EDAM} Ontology is further used to add bioinformatics-specific metadata, such as strong typing of inputs and outputs, within both Abstract {CWL} and Bioschemas annotations. Organising and packaging the definitions and components of a workflow with their associated objects such as test data: we use a Workflow profile specialisation of {RO}-Crate, a community developed standardised approach for research output packaging with rich metadata. {RO}-Crate provides us the ability to package executable workflows, their components such as example and test data, abstract {CWL}, diagrams and their documentation. This makes workflows more readily re-usable. {RO}-Crate is the base unit of upload and download at the {WorkflowHub}. As {CWFR} Digital Objects of workflows, {RO}-Crates are activation-ready and circulated between the different services for execution and testing. Identifiers for all the components: like {FAIR} Digital Objects, {RO}-Crates can be metadata-rich bags of identifiers and can themselves be assigned permanent identifiers. This enables the full description of a computational analysis, from input data, over tools and workflows, to final results. Using these components we have built an environment that supports the Workflow Life Cycle, from abstract description, through to a specific rendering in a {WfMS} to its execution and the documentation of its run provenance, results and continued testing.}
}
@misc{neylon_blog_post_2017,
title = {As a researcher…I'm a bit bloody fed up with Data Management},
author = {Neylon, Cameron},
journal = {Science In The Open},
url = {https://cameronneylon.net/blog/as-a-researcher-im-a-bit-bloody-fed-up-with-data-management/},
year = {2017},
month = {jul},
day = {16},
urldate = {2021-05-04},
sciwheel-projects = {ro-crate-paper},
type = {BLOG\_POST}
}

@techreport{doi:10.17487/rfc8493,
title = {The {BagIt} File Packaging Format (V1.0)},
author = {Kunze, J. and Littman, J. and Madden, E. and Scancella, J. and Adams, C.},
publisher = {{RFC} Editor},
url = {https://www.rfc-editor.org/info/rfc8493},
year = {2018},
month = {oct},
urldate = {2021-05-04},
doi = {10.17487/RFC8493},
sciwheel-projects = {ro-crate-paper}
}
@techreport{ocfl_2020,
title = {Oxford Common File Layout Specification},
author = {{OCFL}},
editor = {Hankinson, Andrew and Jefferies, Neil and Metz, Rosalyn and Morley, Julian and Warner, Simeon and Woods, Andrew},
publisher = {OCFL},
url = {https://ocfl.io/1.0/spec/},
year = {2020},
month = {jul},
day = {7},
urldate = {2021-05-04},
edition = {1.0},
sciwheel-projects = {ro-crate-paper},
type = {Recommendation}
}

@techreport{httprange14,
title = {Dereferencing {HTTP} {URIs}},
author = {{W3C Technical Architecture Group}},
editor = {Lewis, Rhys},
publisher = {W3C},
url = {https://www.w3.org/2001/tag/doc/httpRange-14/2007-08-31/HttpRange-14.html},
year = {2007},
month = {aug},
day = {31},
urldate = {2021-05-04},
edition = {2007-08-31},
sciwheel-projects = {ro-crate-paper},
type = {Draft Tag Finding}
}

@techreport{sporny_2014,
title = {{JSON}-{LD} 1.0},
author = {Sporny, Manu and Longley, Dave and Kellogg, Gregg and Lanthaler, Markus and Lindström, Niklas},
publisher = {W3C},
url = {https://www.w3.org/TR/2014/REC-json-ld-20140116/},
year = {2014},
month = {jan},
day = {16},
urldate = {2021-05-04},
sciwheel-projects = {ro-crate-paper},
type = {{W3C} Recommendation}
}

@techreport{doi:10.5281/zenodo.4541002,
title = {{RO}-Crate Metadata Specification 1.1.1},
author = {Sefton, Peter and Carragáin, Eoghan {\'{O}} and Soiland-Reyes, Stian and Corcho, Oscar and Garijo, Daniel and Palma, Raul and Coppens, Frederik and Goble, Carole and Fernández, José María and Chard, Kyle and Gomez-Perez, Jose Manuel and Crusoe, Michael R and Eguinoa, Ignacio and Juty, Nick and Holmes, Kristi and Clark, Jason A. and Capella-Gutierrez, Salvador and Gray, Alasdair J. G. and Owen, Stuart and Williams, Alan R. and Tartari, Giacomo and Bacall, Finn and Thelen, Thomas and Ménager, Hervé and Navas, Laura Rodríguez-Navas and Walk, Paul and Whitehead, Brandon and Wilkinson, Mark and Groth, Paul and Bremer, Erich and Castro, {LJ} Garcia and Sebby, Karl and Kanitz, Alexander and Trisovic, Ana and Kennedy, Gavin and Graves, Mark and Koehorst, Jasper and Leo, Simone and Portier, Marc},
url = {https://w3id.org/ro/crate/1.1},
year = {2021},
urldate = {2021-05-04},
journal = {Zenodo},
doi = {10.5281/zenodo.4541002},
sciwheel-projects = {ro-crate-paper},
abstract = {Web-version: https://w3id.org/ro/crate/1.1 This document specifies a method, known as {RO}-Crate (Research Object Crate), of aggregating and describing research data with associated metadata. {RO}-Crates can aggregate and describe any resource including files, {URI}-addressable resources, or use other addressing schemes to locate digital or physical data. {RO}-Crates can describe data in aggregate and at the individual resource level, with metadata to aid in discovery, re-use and long term management of data. Metadata includes the ability to describe the context of data and the entities involved in its production, use and reuse. For example: who created it, using which equipment, software and workflows, under what licenses can it be re-used, where was it collected, and/or where is it about. {RO}-Crate uses {JSON}-{LD} to to express this metadata using linked data, describing data resources as well as contextual entities such as people, organizations, software and equipment as a series of linked {JSON}-{LD} objects - using common published vocabularies, chiefly schema.org. The core of {RO}-Crate is a {JSON}-{LD} file, the {RO}-Crate Metadata File, named ro-crate-metadata.json. This file contains structured metadata about the dataset as a whole (the Root Data Entity) and, optionally, about some or all of its files. This provides a simple way to, for example, assert the authors (e.g. people, organizations) of the {RO}-Crate or one its files, or to capture more complex provenance for files, such as how they were created using software and equipment. While providing the formal specification for {RO}-Crate, this document also aims to be a practical guide for software authors to create tools for generating and consuming research data packages, with explanation by examples.}
}

@techreport{doi:10.5281/zenodo.3541888,
title = {{RO}-Crate Metadata Specification 1.0},
author = {Sefton, Peter and Carragáin, Eoghan {\'{O}} and Soiland-Reyes, Stian and Corcho, Oscar and Garijo, Daniel and Palma, Raul and Coppens, Frederik and Goble, Carole and Fernández, José María and Chard, Kyle and Gomez-Perez, Jose Manuel and Crusoe, Michael R and Eguinoa, Ignacio and Juty, Nick and Holmes, Kristi and Clark, Jason A. and Capella-Gutierrez, Salvador and Gray, Alasdair J. G. and Owen, Stuart and Williams, Alan R. and Tartari, Giacomo and Bacall, Finn and Thelen, Thomas},
year = {2019},
url = {https://w3id.org/ro/crate/1.0},
journal = {Zenodo},
doi = {10.5281/zenodo.3541888},
sciwheel-projects = {ro-crate-paper},
abstract = {Web-version: https://w3id.org/ro/crate/1.0 This document specifies a method, known as {RO}-Crate (Research Object Crate), of organizing file-based data with associated metadata, using linked data principles, in both human and machine readable formats, with the ability to include additional domain-specific metadata. The core of {RO}-Crate is a {JSON}-{LD} file, the {RO}-Crate Metadata File, named ro-crate-metadata.jsonld. This file contains structured metadata about the dataset as a whole (the Root Data Entity) and, optionally, about some or all of its files. This provides a simple way to, for example, assert the authors (e.g. people, organizations) of the {RO}-Crate or one its files, or to capture more complex provenance for files, such as how they were created using software and equipment. While providing the formal specification for {RO}-Crate, this document also aims to be a practical guide for software authors to create tools for generating and consuming research data packages, with explanation by examples.}
}
@techreport{doi:10.5281/zenodo.4031327,
title = {{RO}-Crate Metadata Specification 1.1},
author = {Sefton, Peter and Carragáin, Eoghan {\'{O}} and Soiland-Reyes, Stian and Corcho, Oscar and Garijo, Daniel and Palma, Raul and Coppens, Frederik and Goble, Carole and Fernández, José María and Chard, Kyle and Gomez-Perez, Jose Manuel and Crusoe, Michael R and Eguinoa, Ignacio and Juty, Nick and Holmes, Kristi and Clark, Jason A. and Capella-Gutierrez, Salvador and Gray, Alasdair J. G. and Owen, Stuart and Williams, Alan R. and Tartari, Giacomo and Bacall, Finn and Thelen, Thomas and Ménager, Hervé and Rodríguez-Navas, Laura and Walk, Paul and Whitehead, Brandon and Wilkinson, Mark and Groth, Paul and Bremer, Erich and Castro, {LJ} Garcia and Sebby, Karl and Kanitz, Alexander and Trisovic, Ana and Kennedy, Gavin and Graves, Mark and Koehorst, Jasper and Leo, Simone},
url = {https://w3id.org/ro/crate/1.1},
year = {2020},
journal = {Zenodo},
doi = {10.5281/zenodo.4031327},
sciwheel-projects = {ro-crate-paper},
abstract = {Web-version: https://w3id.org/ro/crate/1.1 This document specifies a method, known as {RO}-Crate (Research Object Crate), of organizing file-based data with associated metadata, using linked data principles, in both human and machine readable formats, with the ability to include additional domain-specific metadata. The core of {RO}-Crate is a {JSON}-{LD} file, the {RO}-Crate Metadata File, named ro-crate-metadata.json. This file contains structured metadata about the dataset as a whole (the Root Data Entity) and, optionally, about some or all of its files. This provides a simple way to, for example, assert the authors (e.g. people, organizations) of the {RO}-Crate or one its files, or to capture more complex provenance for files, such as how they were created using software and equipment. While providing the formal specification for {RO}-Crate, this document also aims to be a practical guide for software authors to create tools for generating and consuming research data packages, with explanation by examples.}
}
@article{doi:10.5281/zenodo.3250687,
title = {A lightweight approach to research object data packaging},
author = {Carragáin, Eoghan {\'{O}} and Goble, Carole and Sefton, Peter and Soiland-Reyes, Stian},
year = {2019},
urldate = {2021-05-04},
journal = {Zenodo},
doi = {10.5281/zenodo.3250687},
sciwheel-projects = {ro-crate-paper},
abstract = {A Research Object ({RO}) provides a machine-readable mechanism to communicate the diverse set of digital and real-world resources that contribute to an item of research. The aim of an {RO} is to evolve from traditional academic publication as a static {PDF}, to rather provide a complete and structured archive of the items (such as people, organisations, funding, equipment, software etc) that contributed to the research outcome, including their identifiers, provenance, relations and annotations. This is of particular importance as all domains of research and science are increasingly relying on computational analysis, yet we are facing a reproducibility crisis because key components are often not sufficiently tracked, archived or reported. Here we propose Research Object Crate (or {RO}-Crate for short), an emerging lightweight approach to packaging research data with their structured metadata, rephrasing the Research Object model as schema.org annotations to formalize a {JSON}-{LD} format that can be used independently of infrastructure, e.g. in {GitHub} or Zenodo archives. {RO}-Crate can be extended for domain-specific descriptions, aiming at a wide variety of applications and repositories to encourage {FAIR} sharing of reproducible datasets and analytical methods.}
}
@article{doi:10.5281/zenodo.1445817,
title = {Datacrate Submisssion To The Workshop On Research Objects},
author = {Sefton, Peter},
year = {2018},
urldate = {2021-05-04},
journal = {Zenodo},
doi = {10.5281/zenodo.1445817},
sciwheel-projects = {ro-crate-paper},
abstract = {This is a somewhat experimental approach to submitting an extended abstract to a conference. The submission describes the {DataCrate} standard and the source-files, {HTML} and {PDF} versions of the submission are included in a {DataCrate} package.}
}
@misc{describo,
title = {Arkisto Platform: Describo},
author = {La Rosa, Marco and Sefton, Peter},
url = {https://arkisto-platform.github.io/describo/},
urldate = {2021-05-04},
sciwheel-projects = {ro-crate-paper},
type = {WEBSITE}
}
@misc{describo-online,
title = {Arkisto Platform: Describo Online},
author = {La Rosa, Marco},
url = {https://arkisto-platform.github.io/describo-online/},
urldate = {2021-05-04},
sciwheel-projects = {ro-crate-paper},
type = {WEBSITE}
}
@misc{ro-crate-excel,
title = {npm: ro-crate-excel},
author = {Lynch, Mike and Sefton, Peter},
url = {https://www.npmjs.com/package/ro-crate-excel},
urldate = {2021-05-04},
sciwheel-projects = {ro-crate-paper},
type = {WEBSITE}
}
@misc{ro-crate-html-js,
title = {npm: ro-crate-html-js},
url = {https://www.npmjs.com/package/ro-crate-html-js},
urldate = {2021-05-04},
sciwheel-projects = {ro-crate-paper},
type = {WEBSITE}
}
@misc{ro-crate-js,
title = {{GitHub} - {UTS}-{eResearch}/ro-crate-js: Research Object Crate ({RO}-Crate) utilities},
url = {https://github.com/UTS-eResearch/ro-crate-js},
urldate = {2021-05-04},
journal = {GitHub},
sciwheel-projects = {ro-crate-paper},
type = {WEBSITE}
}
@misc{ro-crate-ruby,
title = {{GitHub} - {ResearchObject}/ro-crate-ruby: A Ruby gem for creating, manipulating and reading {RO}-Crates},
author = {Bacall, Finn and Whitwell, Martyn},
url = {https://github.com/ResearchObject/ro-crate-ruby},
journal = {GitHub},
urldate = {2021-05-04},
sciwheel-projects = {ro-crate-paper},
type = {WEBSITE}
}
@misc{ro-crate-py,
title = {{GitHub} - {ResearchObject}/ro-crate-py: Python library for {RO}-Crate},
author = {Leo, Simone and Eguinoa, Ignacio and Soiland-Reyes, Stian and Droesbeke, Bert and Rodríguez-Navas, Laura and Gaignard, Alban},
url = {https://github.com/researchobject/ro-crate-py},
urldate = {2021-05-04},
sciwheel-projects = {ro-crate-paper},
type = {WEBSITE}
}
@misc{about-workflowhub,
title = {{WorkflowHub} project \textbar Project pages for developing and running the {WorkflowHub}, a registry of scientific workflows},
url = {https://about.workflowhub.eu/},
urldate = {2021-05-04},
sciwheel-projects = {ro-crate-paper},
type = {WEBSITE}
}
@misc{modpdsc,
title = {{GitHub} - {CoEDL}/modpdsc},
url = {https://github.com/CoEDL/modpdsc/},
urldate = {2021-05-04},
sciwheel-projects = {ro-crate-paper},
type = {WEBSITE}
}
@misc{arkisto-data-portal,
title = {Tools: Data Portal \& Discovery},
url = {https://arkisto-platform.github.io/tools/portal/},
urldate = {2021-05-04},
sciwheel-projects = {ro-crate-paper},
type = {WEBSITE}
}
@misc{ocfl-tools,
title = {{GitHub} - {CoEDL}/ocfl-tools: Tools to process and manipulate an {OCFL} tree},
url = {https://github.com/CoEDL/ocfl-tools},
urldate = {2021-05-04},
sciwheel-projects = {ro-crate-paper},
type = {WEBSITE}
}
@misc{ro-composer,
title = {eScienceLab: RO-Composer},
author = {Bacall, Finn and Soiland-Reyes, Stian and Soares e Silva, Marina},
url = {https://esciencelab.org.uk/projects/ro-composer/},
urldate = {2021-05-04},
sciwheel-projects = {ro-crate-paper},
type = {WEBSITE}
}
@misc{galaxy2cwl,
title = {{GitHub} - workflowhub-eu/galaxy2cwl: Standalone version tool to get cwl descriptions (initially an abstract cwl interface) of galaxy workflows and Galaxy workflows executions},
url = {https://github.com/workflowhub-eu/galaxy2cwl},
urldate = {2021-05-04},
sciwheel-projects = {ro-crate-paper},
type = {WEBSITE}
}

@misc{doi:10.1109/IEEESTD.2020.9094416,
title = {IEEE Standard for Bioinformatics Analyses Generated by High-Throughput Sequencing (HTS) to Facilitate Communication},
note = {IEEE Std 2791-2020},
isbn = {978-1-5044-6466-6},
doi = {10.1109/IEEESTD.2020.9094416},
sciwheel-projects = {ro-crate-paper},
type = {Standard}
}
@article{doi:10.1016/j.patter.2020.100136,
title = {Dataset reuse: toward translating principles to practice},
author = {Koesten, Laura and Vougiouklis, Pavlos and Simperl, Elena and Groth, Paul},
pages = {100136},
year = {2020},
month = {nov},
day = {13},
journal = {Patterns (New York, N.Y.)},
volume = {1},
number = {8},
issn = {26663899},
doi = {10.1016/j.patter.2020.100136},
pmid = {33294873},
pmcid = {PMC7691392},
sciwheel-projects = {ro-crate-paper},
abstract = {The web provides access to millions of datasets that can have additional impact when used beyond their original context. We have little empirical insight into what makes a dataset more reusable than others and which of the existing guidelines and frameworks, if any, make a difference. In this paper, we explore potential reuse features through a literature review and present a case study on datasets on {GitHub}, a popular open platform for sharing code and data. We describe a corpus of more than 1.4 million data files, from over 65,000 repositories. Using {GitHub}'s engagement metrics as proxies for dataset reuse, we relate them to reuse features from the literature and devise an initial model, using deep neural networks, to predict a dataset's reusability. This demonstrates the practical gap between principles and actionable insights that allow data publishers and tools designers to implement functionalities that provably facilitate reuse. \copyright 2020 The Authors.}
}
@inproceedings{doi:10.1190/1.1822162,
title = {Electronic documents give reproducible research a new meaning},
author = {Claerbout, Jon F. and Karrenbach, Martin},
pages = {601-604},
publisher = {Society of Exploration Geophysicists},
year = {1992},
month = {jan},
urldate = {2021-05-04},
doi = {10.1190/1.1822162},
sciwheel-projects = {ro-crate-paper},
booktitle = {{SEG} Technical Program Expanded Abstracts 1992}
}
@inproceedings{newman2009,
       booktitle = {Proceedings of the Workshop on Semantic Web Applications in Scientific Discourse (SWASD 2009)},
          editor = {Clark, Tim and Luciano, Joanne S. and Marshall, M. Scott and Prud’Hommeaux, Eric and Stephens, Susie},       
          series = {CEUR Workshop Proceedings},
       collection={CEUR Workshop Proceedings},          
           month = {Oct},
           title = {{myExperiment}: An ontology for e-Research},
          author = {David Newman and Sean Bechhofer and David De Roure},
            year = {2009},
            issn = {1613-0073},
           volume = {523},
           publisher={CEUR-WS},
            note = {2009-10-25},
             url = {http://ceur-ws.org/Vol-523/Newman.pdf},
        sciwheel-projects = {ro-crate-paper},
        abstract = {myExperiment describes itself as a "Social Virtual Research Environment" that provides the ability to share Research Objects (ROs) over a social infrastructure to facilitate actioning of research. The myExperiment Ontology is a logical representation of the data model used by this environment, allowing its data to be published in a standard RDF format, whilst providing a generic extensible framework that can be reused by similar projects. ROs are data structures designed to semantically enhance research publications by capturing and preserving the research method so that it can be reproduced in the future. This paper provides some motivation for an RO specification and briefly considers how existing domain-specifific ontologies might be integrated. It concludes by discussing the future direction of the myExperiment Ontology and how it will best support these ROs.}
}

@misc{myExperimentOntology2009,
title = {{myExperiment} Ontology Modules},
url = {http://web.archive.org/web/20091115080336/http%3a%2f%2frdf.myexperiment.org/ontologies},
urldate = {2021-05-04},
year = {2009},
sciwheel-projects = {ro-crate-paper},
type = {WEBSITE}
}
@article{doi:10.3390/publications8020021,
title = {{FAIR} digital objects for science: from data pieces to actionable knowledge units},
author = {De Smedt, Koenraad and Koureas, Dimitris and Wittenburg, Peter},
pages = {21},
year = {2020},
month = {apr},
day = {11},
urldate = {2021-05-04},
journal = {Publications},
volume = {8},
number = {2},
issn = {2304-6775},
doi = {10.3390/publications8020021},
sciwheel-projects = {ro-crate-paper},
abstract = {Data science is facing the following major challenges: (1) developing scalable cross-disciplinary capabilities, (2) dealing with the increasing data volumes and their inherent complexity, (3) building tools that help to build trust, (4) creating mechanisms to efficiently operate in the domain of scientific assertions, (5) turning data into actionable knowledge units and (6) promoting data interoperability. As a way to overcome these challenges, we further develop the proposals by early Internet pioneers for Digital Objects as encapsulations of data and metadata made accessible by persistent identifiers. In the past decade, this concept was revisited by various groups within the Research Data Alliance and put in the context of the {FAIR} Guiding Principles for findable, accessible, interoperable and reusable data. The basic components of a {FAIR} Digital Object ({FDO}) as a self-contained, typed, machine-actionable data package are explained. A survey of use cases has indicated the growing interest of research communities in {FDO} solutions. We conclude that the {FDO} concept has the potential to act as the interoperable federative core of a hyperinfrastructure initiative such as the European Open Science Cloud ({EOSC}).}
}

@misc{ebi_ftp_umgs2019,
title = {{FTP} index of /pub/databases/metagenomics/umgs\_analyses/},
author = {{EMBL-EBI Microbiome Informatics Team}},
url = {http://ftp.ebi.ac.uk/pub/databases/metagenomics/umgs_analyses/},
year = {2019},
month = {sep},
day = {12},
urldate = {2021-05-04},
sciwheel-projects = {ro-crate-paper},
abstract = {{UMGS} genomes generated in this work were deposited in {ENA}, under the study accession {ERP108418}. The 92,143 {MAGs} with {QS} \textgreater50, as well as the quantification results from {BWA} and sourmash, all phylogenetic trees and the functional analysis results with {InterProScan}, {GP} and {GhostKOALA} are available at ftp://ftp.ebi.ac.uk/pub/databases/metagenomics/umgs\_analyses/. },
journal = {ftp.ebi.ac.uk},
notes = { Dataset of https://doi.org/10.1038/s41586-019-0965-1 },
type = {WEBSITE}
}


@misc{finn-lab-mgsgut,
title = {{GitHub} - Finn-Lab/{MGS}-gut: Analysing Metagenomic Species ({MGS})},
author = {{EMBL-EBI Microbiome Informatics Team}},
url = {https://github.com/Finn-Lab/MGS-gut},
urldate = {2021-05-04},
journal = {GitHub},
sciwheel-projects = {ro-crate-paper},
notes = { Scripts of https://doi.org/10.1038/s41586-019-0965-1 },
type = {WEBSITE}
}

@misc{sefton_blog_post_2021,
title = {{FAIR} Data Management; It's a lifestyle not a lifecycle - ptsefton.com},
author = {Sefton, Peter},
journal = {ptsefton.com},
url = {http://ptsefton.com/2021/04/07/rdmpic/},
year = {2021},
month = {apr},
day = {7},
urldate = {2021-05-04},
sciwheel-projects = {ro-crate-paper},
type = {BLOG\_POST}
}


@misc{goble_presentation_2016,
title = {What is Reproducibility? The R* Brouhaha},
author = {Goble, Carole},
url = {http://repscience2016.research-infrastructures.eu/img/CaroleGoble-ReproScience2016v2.pdf},
year = {2016},
month = {sep},
day = {9},
urldate = {2021-05-04},
address = {Hannover, Germany},
sciwheel-projects = {ro-crate-paper},
type = {Keynote}
}


@misc{soilandreyes_tweet_2020,
title = {I am looking for which bioinformatics journals encourage authors to submit their code/pipeline/workflow supporting data analysis},
author = {Soiland-Reyes, Stian},
journal = {Twitter},
url = {https://twitter.com/soilandreyes/status/1250721245622079488},
year = {2020},
month = {apr},
day = {16},
urldate = {2021-05-04},
sciwheel-projects = {ro-crate-paper},
abstract = {I am looking for which bioinformatics journals encourage authors to submit their code/pipeline/workflow *supporting* data analysis, but so far I have only found vague references or special software article types. Usual suspects missing. See thread below; any more..?},
type = {thread}
}



@article{doi:10.1080/14490854.2020.1796500,
title = {Digital crowdsourcing and public understandings of the past: citizen historians meet Criminal Characters},
author = {Piper, Alana},
pages = {525-541},
year = {2020},
month = {jul},
day = {2},
urldate = {2021-05-04},
journal = {History Australia},
volume = {17},
number = {3},
issn = {1449-0854},
doi = {10.1080/14490854.2020.1796500},
sciwheel-projects = {ro-crate-paper}
}

@article{doi:10.1016/j.tibtech.2012.02.002,
title = {{ELIXIR}: a distributed infrastructure for European biological data},
author = {Crosswell, Lindsey C and Thornton, Janet M},
pages = {241-242},
url = {http://dx.doi.org/10.1016/j.tibtech.2012.02.002},
year = {2012},
month = {may},
urldate = {2021-05-04},
journal = {Trends in Biotechnology},
volume = {30},
number = {5},
doi = {10.1016/j.tibtech.2012.02.002},
pmid = {22417641},
sciwheel-projects = {ro-crate-paper}
}
@incollection{doi:10.4018/978-1-60960-593-3.ch008,
booktitle = {Semantic services, interoperability and web applications: emerging concepts},
title = {Linked data: the story so far},
author = {Bizer, Christian and Heath, Tom and Berners-Lee, Tim},
editor = {Sheth, Amit},
pages = {205-227},
publisher = {{IGI} Global},
url = {http://services.igi-global.com/resolvedoi/resolve.aspx?doi=10.4018/978-1-60960-593-3.ch008},
year = {2011},
urldate = {2021-05-04},
isbn = {9781609605933},
doi = {10.4018/978-1-60960-593-3.ch008},
sciwheel-projects = {ro-crate-paper},
abstract = {The term {\textquotedblleftLinked} Data\textquotedblright refers to a set of best practices for publishing and connecting structured data on the Web. These best practices have been adopted by an increasing number of data providers over the last three years, leading to the creation of a global data space containing billions of assertions\textemdash the Web of Data. In this article, the authors present the concept and technical principles of Linked Data, and situate these within the broader context of related technological developments. They describe progress to date in publishing Linked Data on the Web, review applications that have been developed to exploit the Web of Data, and map out a research agenda for the Linked Data community as it moves forward.}
}

@article{doi:10.1016/j.websem.2015.01.003,
title = {Using a suite of ontologies for preserving workflow-centric research objects},
author = {Belhajjame, Khalid and Zhao, Jun and Garijo, Daniel and Gamble, Matthew and Hettne, Kristina and Palma, Raul and Mina, Eleni and Corcho, Oscar and Gómez-Pérez, José Manuel and Bechhofer, Sean and Klyne, Graham and Goble, Carole},
pages = {16-42},
year = {2015},
month = {may},
journal = {Web Semantics: Science, Services and Agents on the World Wide Web},
volume = {32},
number = {0},
issn = {15708268},
doi = {10.1016/j.websem.2015.01.003},
sciwheel-projects = {ro-crate-paper},
abstract = {Scientific workflows are a popular mechanism for specifying and automating data-driven in silico experiments. A significant aspect of their value lies in their potential to be reused. Once shared, workflows become useful building blocks that can be combined or modified for developing new experiments. However, previous studies have shown that storing workflow specifications alone is not sufficient to ensure that they can be successfully reused, without being able to understand what the workflows aim to achieve or to re-enact them. To gain an understanding of the workflow, and how it may be used and repurposed for their needs, scientists require access to additional resources such as annotations describing the workflow, datasets used and produced by the workflow, and provenance traces recording workflow executions.In this article, we present a novel approach to the preservation of scientific workflows through the application of research objects-aggregations of data and metadata that enrich the workflow specifications. Our approach is realised as a suite of ontologies that support the creation of workflow-centric research objects. Their design was guided by requirements elicited from previous empirical analyses of workflow decay and repair. The ontologies developed make use of and extend existing well known ontologies, namely the Object Reuse and Exchange ({ORE}) vocabulary, the Annotation Ontology ({AO}) and the {W3C} {PROV} ontology ({PROVO}). We illustrate the application of the ontologies for building Workflow Research Objects with a case-study that investigates Huntington's disease, performed in collaboration with a team from the Leiden University Medial Centre ({HG}-{LUMC}). Finally we present a number of tools developed for creating and managing workflow-centric research objects.}
}

@article{doi:10.1145/2857274.2857276,
  title={Schema.org: Evolution of Structured Data on the Web: Big data makes common schemas even more necessary},
  author={Guha, Ramanathan V and Brickley, Dan and Macbeth, Steve},
  journal={Queue},
  volume={13},
  number={9},
  doi={10.1145/2857274.2857276},
  pages={10--37},
  year={2015},
  publisher={ACM New York, NY, USA}
}


@article{doi:10.1093/bioinformatics/btx192,
title = {{BioContainers}: an open-source and community-driven framework for software standardization.},
author = {da Veiga Leprevost, Felipe and Grüning, Björn A and Alves Aflitos, Saulo and Röst, Hannes L and Uszkoreit, Julian and Barsnes, Harald and Vaudel, Marc and Moreno, Pablo and Gatto, Laurent and Weber, Jonas and Bai, Mingze and Jimenez, Rafael C and Sachsenberg, Timo and Pfeuffer, Julianus and Vera Alvarez, Roberto and Griss, Johannes and Nesvizhskii, Alexey I and Perez-Riverol, Yasset},
pages = {2580-2582},
year = {2017},
month = {aug},
day = {15},
journal = {Bioinformatics},
volume = {33},
number = {16},
doi = {10.1093/bioinformatics/btx192},
pmid = {28379341},
pmcid = {PMC5870671},
sciwheel-projects = {ro-crate-paper},
abstract = {Motivation: {BioContainers} (biocontainers.pro) is an open-source and community-driven framework which provides platform independent executable environments for bioinformatics software. {BioContainers} allows labs of all sizes to easily install bioinformatics software, maintain multiple versions of the same software and combine tools into powerful analysis pipelines. {BioContainers} is based on popular open-source projects Docker and rkt frameworks, that allow software to be installed and executed under an isolated and controlled environment. Also, it provides infrastructure and basic guidelines to create, manage and distribute bioinformatics containers with a special focus on omics technologies. These containers can be integrated into more comprehensive bioinformatics pipelines and different architectures (local desktop, cloud environments or {HPC} clusters). Availability and Implementation: The software is freely available at github.com/{BioContainers}/. Contact: yperez@ebi.ac.uk. \copyright The Author(s) 2017. Published by Oxford University Press.}
}
@inproceedings{doi:10.1109/BigData.2016.7840618,
title = {I'll take that to go: Big data bags and minimal identifiers for exchange of large, complex datasets},
author = {Chard, Kyle and D'Arcy, Mike and Heavner, Ben and Foster, Ian and Kesselman, Carl and Madduri, Ravi and Rodriguez, Alexis and Soiland-Reyes, Stian and Goble, Carole and Clark, Kristi and Deutsch, Eric W. and Dinov, Ivo and Price, Nathan and Toga, Arthur},
pages = {319-328},
publisher = {IEEE},
url = {https://static.aminer.org/pdf/fa/bigdata2016/BigD418.pdf},
year = {2016},
month = {dec},
day = {5},
urldate = {2018-07-13},
isbn = {978-1-4673-9005-7},
doi = {10.1109/BigData.2016.7840618},
sciwheel-projects = {ro-crate-paper},
abstract = {Big data workflows often require the assembly and exchange of complex, multi-element datasets. For example, in biomedical applications, the input to an analytic pipeline can be a dataset consisting thousands of images and genome sequences assembled from diverse repositories, requiring a description of the contents of the dataset in a concise and unambiguous form. Typical approaches to creating datasets for big data workflows assume that all data reside in a single location, requiring costly data marshaling and permitting errors of omission and commission because dataset members are not explicitly specified. We address these issues by proposing simple methods and tools for assembling, sharing, and analyzing large and complex datasets that scientists can easily integrate into their daily workflows. These tools combine a simple and robust method for describing data collections ({BDBags}), data descriptions (Research Objects), and simple persistent identifiers (Minids) to create a powerful ecosystem of tools and services for big data analysis and sharing. We present these tools and use biomedical case studies to illustrate their use for the rapid assembly, sharing, and analysis of large datasets.},
booktitle = {2016 {IEEE} International Conference on Big Data (Big Data)}
}
@misc{bioschemas_2017,
title = {Bioschemas: From Potato Salad to Protein Annotation},
author = {Gray, Alasdair and Goble, Carole and Jimenez, Rafael and Bioschemas Community,},
url = {https://iswc2017.semanticweb.org/paper-579/},
year = {2017},
month = {oct},
day = {23},
urldate = {2021-05-05},
address = {Vienna, Austria},
sciwheel-projects = {ro-crate-paper},
abstract = {The life sciences have a wealth of data resources with a wide range of overlapping content. Key repositories, such as {UniProt} for protein data or Entrez Gene for gene data, are well known and their content easily discovered through search engines. However, there is a long-tail of bespoke datasets with important content that are not so prominent in search results. Building on the success of Schema.org for making a wide range of structured web content more discoverable and interpretable, e.g. food recipes, the Bioschemas community (http://bioschemas.org) aim to make life sciences datasets more findable by encouraging data providers to embed Schema.org markup in their resources.},
type = {Poster}
}
@misc{pcdm,
title = {Portland Common Data Model},
author = {Cossu, Stefano and Cowles, Esmé and Estlund, Karen and Harlow, Christina and Johnson, Tom and Matienzo, Mark and Lamb, Danny and Rayle, Lynette and Sanderson, Rob and Stroop, Jon and Woods, Andrew},
url = {https://github.com/duraspace/pcdm/wiki},
year = {2018},
month = {jun},
day = {15},
urldate = {2021-05-05},
sciwheel-projects = {ro-crate-paper},
abstract = {The Portland Common Data Model ({PCDM}) is a flexible, extensible domain model that is intended to underlie a wide array of repository and {DAMS} applications. The primary objective of this model is to establish a framework that developers of tools (e.g., Samvera-based engines, such as Hyrax, Hyku, Sufia, and Avalon; Islandora; custom Fedora sites) can use for working with models in a general way, allowing adopters to easily use custom models with any tool. Given this interoperability goal, the initial work has been focused on structural metadata and access control, since these are the key actionable metadata.},
journal = {{GitHub} duraspace/pcdm Wiki},
type = {WEBSITE}
}
@article{doi:10.1126/science.aah6168,
title = {Enhancing reproducibility for computational methods},
author = {Stodden, Victoria and {McNutt}, Marcia and Bailey, David H and Deelman, Ewa and Gil, Yolanda and Hanson, Brooks and Heroux, Michael A and Ioannidis, John P A and Taufer, Michela},
pages = {1240-1241},
year = {2016},
month = {dec},
day = {9},
urldate = {2021-05-05},
journal = {Science},
volume = {354},
number = {6317},
issn = {0036-8075},
doi = {10.1126/science.aah6168},
pmid = {27940837},
sciwheel-projects = {ro-crate-paper}
}

@article{doi:10.1371/journal.pcbi.1003285,
title = {Ten simple rules for reproducible computational research.},
author = {Sandve, Geir Kjetil and Nekrutenko, Anton and Taylor, James and Hovig, Eivind},
pages = {e1003285},
year = {2013},
month = {oct},
day = {24},
urldate = {2018-07-13},
journal = {{PLoS} Computational Biology},
volume = {9},
number = {10},
doi = {10.1371/journal.pcbi.1003285},
pmid = {24204232},
pmcid = {PMC3812051},
sciwheel-projects = {ro-crate-paper}
}
@article{doi:10.1093/nar/gkl971,
title = {The worldwide Protein Data Bank ({wwPDB}): ensuring a single, uniform archive of {PDB} data.},
author = {Berman, Helen and Henrick, Kim and Nakamura, Haruki and Markley, John L},
pages = {D301-3},
year = {2007},
month = {jan},
urldate = {2021-05-05},
journal = {Nucleic Acids Research},
volume = {35},
number = {Database issue},
doi = {10.1093/nar/gkl971},
pmid = {17142228},
pmcid = {PMC1669775},
sciwheel-projects = {ro-crate-paper},
abstract = {The worldwide Protein Data Bank ({wwPDB}) is the international collaboration that manages the deposition, processing and distribution of the {PDB} archive. The online {PDB} archive is a repository for the coordinates and related information for more than 38 000 structures, including proteins, nucleic acids and large macromolecular complexes that have been determined using X-ray crystallography, {NMR} and electron microscopy techniques. The founding members of the {wwPDB} are {RCSB} {PDB} ({USA}), {MSD}-{EBI} (Europe) and {PDBj} (Japan) [H.M. Berman, K. Henrick and H. Nakamura (2003) Nature Struct. Biol., 10, 980]. The {BMRB} group ({USA}) joined the {wwPDB} in 2006. The mission of the {wwPDB} is to maintain a single archive of macromolecular structural data that are freely and publicly available to the global community. Additionally, the {wwPDB} provides a variety of services to a broad community of users. The {wwPDB} website at http://www.wwpdb.org/ provides information about services provided by the individual member organizations and about projects undertaken by the {wwPDB}.}
}
@article{doi:10.3897/biss.3.37080,
title = {Zenodo, an Archive and Publishing Repository: A tale of two herbarium specimen pilot projects},
author = {Dillen, Mathias and Groom, Quentin and Agosti, Donat and Nielsen, Lars},
year = {2019},
month = {jun},
day = {18},
journal = {Biodiversity Information Science and Standards},
volume = {3},
issn = {2535-0897},
doi = {10.3897/biss.3.37080},
sciwheel-projects = {ro-crate-paper},
abstract = {Zenodo (https://zenodo.org) is an open-access repository operated by {CERN} (European Organization for Nuclear Research), which provides researchers with an easy and stable platform to archive and publish their data and other output, such as software tools, manuals and project reports. In the context of the {ICEDIG} (Innovation and Consolidation for Large scale Digitisation of Natural Heritage) project, Zenodo was investigated for its usability as a platform where digitized images of collection specimens could be archived and published. In a production digitization pipeline, we foresee the automated archiving of daily image production. If Zenodo could be used for this purpose, such a process would also immediately mean that data and images are published {FAIR}-ly (Findable, Accessible, Interoperable and Reusable) within hours of their creation. To evaluate performance of the system, we first used a test dataset of 1800 herbarium specimen images, which was uploaded using Zenodo's {API} (Application Programming Interface) (Dillen et al. 2019). This dataset includes lossless {TIFF} images, label-segmented overlays and {JSON}-{LD} ({JavaScript} Object Notation for Linked Data) metadata using {DwC} (Darwin Core) terminology, constituting over 208 gigabytes of data. In addition, for all individual digital specimens the data about the specimen (in {DwC}) as well as metadata about its deposition on Zenodo (in Zenodo's internal data model) were available in multiple machine-readable formats. All data in {DwC} were provided as linked data with their {DwC} identifiers (e.g. http://rs.tdwg.org/dwc/terms/{basisOfRecord}). All individual specimens received minted {DOIs} (Digital Object Identifiers). A second upload of 280,000 herbarium {JPEG} images from a single institution (ca. 1 terabyte of data) with limited metadata (but using the same approach) was launched as well. In this presentation, the workflow for proper usage of the {API} will be described as well as some performance metrics, flexibilities and functionalities of the platform. Some issues and potential developments to tackle them will be discussed. Currently, the rate of ingestion into Zenodo seems only fast enough for small scale digitization pipelines. However, a modest improvement in transfer rate would make this a realistic proposition for large volume usage.}
}
@book{isbn:9781315351148,
title = {Data Stewardship for Open Science},
author = {Mons, Barend},
pages = {240},
publisher = {Taylor \& Francis},
urldate = {2021-05-05},
edition = {1},
isbn = {9781315351148},
sciwheel-projects = {ro-crate-paper}
}

@misc{doi:10.48546/workflowhub.workflow.56.1,
title = {Protein Ligand Complex {MD} Setup tutorial using {BioExcel} Building Blocks (biobb) (jupyter notebook)},
author = {Lowe, Douglas and Bayarri, Genís},
year = {2021},
journal = {WorkflowHub},
doi = {10.48546/workflowhub.workflow.56.1},
sciwheel-projects = {ro-crate-paper},
type = Workflow
}

@misc{about-lifemonitor,
author = {CRS4},
title = {{LifeMonitor}, a testing and monitoring service for scientific workflows},
url = {https://about.lifemonitor.eu/},
urldate = {2021-05-07},
type = {WEBSITE}
}


@article{doi:10.5281/zenodo.4705078,
title = {{EOSC}-Life Methodology framework to enhance reproducibility within {EOSC}-Life},
author = {Bietrix, Florence and Carazo, José Maria and Capella-Gutierrez, Salvador and Coppens, Frederik and Chiusano, Maria Luisa and David, Romain and Fernandez, Jose Maria and Fratelli, Maddalena and Heriche, Jean-Karim and Goble, Carole and Gribbon, Philip and Holub, Petr and P. Joosten, Robbie and Leo, Simone and Owen, Stuart and Parkinson, Helen and Pieruschka, Roland and Pireddu, Luca and Porcu, Luca and Raess, Michael and Rodriguez- Navas, Laura and Scherer, Andreas and Soiland-Reyes, Stian and Tang, Jing},
year = {2021},
month = {apr},
day = {30},
journal = {Zenodo},
doi = {10.5281/zenodo.4705078},
abstract = {The original scope of task 8.3 is to develop metrics to assess the impact on reproducibility of the availability of life-science open data and workflows in the cloud. A great part of the activities within {EOSC}-Life is actually related to reproducibility and provides in several ways tools that will have an impact. Therefore, we decided that it would be more informative to describe such activities and to explain why and how they will have an impact on reproducibility in life sciences, instead of providing abstract metrics for such an impact. For these reasons, we changed the title of the deliverable, from \textquotedblleftframework to assess ...\textquotedblright to \textquotedblleftframework to enhance reproducibility\textquotedblright. First of all, we reasoned on what can be the contribution of open science to improve the reproducibility of research. Publicly sharing data, protocols, tools and computational workflows makes it possible to compare or combine the data and outcomes from different studies within a discipline as well as integrate data across scientific domains. It allows conclusions to be validated and possibly corrected as well as being reinforced by meta-analyses. Replication data and test/training data can also be used in many applications to contribute to reproducible research. Moreover, new hypotheses, different from the original aims of the study, can be explored. Data sets can be re-used to develop and test new methods, to conduct scientific and technical benchmarking activities and to support training activities. Therefore, in addition to generating more value from research investments, data sharing has the potential to increase confidence in research outcomes and increase knowledge dissemination. These benefits of open sharing have long been recognized in some fields such as bioinformatics, which has a long history of publicly sharing data with, for example, public repositories for nucleotide sequences going back 30 years and the Protein Data Bank ({PDB}), a repository of information about the {3D} structures of proteins, nucleic acids and complex assemblies, that celebrates its 50th birthday this year. In this notion, any improvement in sharing of data, tools and workflows among scientists and across disciplines, that is the aim of {EOSC}-Life and the wider {EOSC}, will contribute to reproducible science. In addition to this general scope, several specific actions to frame transparency in the reporting of experimental protocols, data and analytical workflows warrant the reproducibility of each single object (experimental results, data or workflows) that is made available on the cloud. We will describe here the initiatives in {EOSC}-Life to implement existing tools for reproducibility as well as to develop new tools for its enhancement. As the final goal of {EOSC}-Life is to make data resources available to the wider community of life scientists, although necessarily technical in several points, this document aims at a general readership, including experimental in addition to data scientists.}
}

@article{doi:10.1093/gigascience/giz095,
title = {Sharing interoperable workflow provenance: A review of best practices and their practical application in {CWLProv}},
author = {Khan, Farah Zaib and Soiland-Reyes, Stian and Sinnott, Richard O and Lonie, Andrew and Goble, Carole and Crusoe, Michael R},
year = {2019},
month = {nov},
day = {1},
journal = {GigaScience},
volume = {8},
number = {11},
doi = {10.1093/gigascience/giz095},
pmid = {31675414},
pmcid = {PMC6824458},
abstract = {{BACKGROUND}: The automation of data analysis in the form of scientific workflows has become a widely adopted practice in many fields of research. Computationally driven data-intensive experiments using workflows enable automation, scaling, adaptation, and provenance support. However, there are still several challenges associated with the effective sharing, publication, and reproducibility of such workflows due to the incomplete capture of provenance and lack of interoperability between different technical (software) platforms. {RESULTS}: Based on best-practice recommendations identified from the literature on workflow design, sharing, and publishing, we define a hierarchical provenance framework to achieve uniformity in provenance and support comprehensive and fully re-executable workflows equipped with domain-specific information. To realize this framework, we present {CWLProv}, a standard-based format to represent any workflow-based computational analysis to produce workflow output artefacts that satisfy the various levels of provenance. We use open source community-driven standards, interoperable workflow definitions in Common Workflow Language ({CWL}), structured provenance representation using the {W3C} {PROV} model, and resource aggregation and sharing as workflow-centric research objects generated along with the final outputs of a given workflow enactment. We demonstrate the utility of this approach through a practical implementation of {CWLProv} and evaluation using real-life genomic workflows developed by independent groups. {CONCLUSIONS}: The underlying principles of the standards utilized by {CWLProv} enable semantically rich and executable research objects that capture computational workflows with retrospective provenance such that any platform supporting {CWL} will be able to understand the analysis, reuse the methods for partial reruns, or reproduce the analysis to validate the published findings. \copyright The Author(s) 2019. Published by Oxford University Press.}
}

@article{doi:10.3233/APC200107,
  place = {NL},
  title = {Toward Enabling Reproducibility for Data-Intensive Research Using the Whole Tale Platform},
  volume = {36},
  ISSN = {0927-5452},
  DOI = {10.3233/APC200107},
  number = {Parallel Computing: Technology Trends},
  journal = {Advances in Parallel Computing},
  publisher = {IOS Press},
  author = {Chard Kyle and Gaffney Niall and Hategan Mihael and Kowalik Kacper and Lud\"{a}scher Bertram and McPhillips Timothy and Nabrzyski Jarek and Stodden Victoria and Taylor Ian and Thelen Thomas and et al.},
  year = {2020},
  pages = {766–778}
}

@conference{doi:10.5281/zenodo.51314,
title = {Tracking Workflow Execution With {TavernaPROV}},
author = {Soiland-Reyes, Stian and Alper, Pinar and Goble, Carole},
year = {2016},
month = Jun,
day = 6,
booktitle = {{ProvenanceWeek} 2016},
note = {PROV: Three Years Later},
organizer = { W3C },
doi = {10.5281/zenodo.51314},
abstract = {Apache Taverna is a scientific workflow system for combining web services and local tools. Taverna records provenance of workflow runs, intermediate values and user interactions, both as an aid for debugging while designing the workflow, but also as a record for later reproducibility and comparison. Taverna also records provenance of the evolution of the workflow definition (including a chain of {wasDerivedFrom} relations), attributions and annotations; for brevity we here focus on how Taverna's workflow run provenance extends {PROV} and is embedded with Research Objects.}
}

@misc{doi:10.5281/zenodo.3903463,
title = {BrennerG/Ro-Crate_2_ma-DMP: v1.0.0},
author = {Brenner, Gabriel},
year = {2020},
journal = {Zenodo},
doi = {10.5281/zenodo.3903463},
abstract = {First Release of {RO}-Crate - {maDMP} Parser}
}
@conference{doi:10.4126/frl01-006423291,
title = {Research Object Crates and Machine-actionable Data Management Plans},
booktitle = {1st Workshop on Research Data Management for Linked Open Science},
author = {Miksa, Tomasz and Jaoua, Maroua and Arfaoui, Ghaith},
year = {2020},
journal = {PUBLISSO},
doi = {10.4126/frl01-006423291},
sciwheel-projects = {ro-crate-paper}
}


@article{doi:10.15497/rda00039,
title = {{RDA} {DMP} Common Standard for Machine-actionable Data Management Plans},
author = {Walk, Paul and Miksa, Tomasz and Neish, Peter},
year = {2019},
journal = {Research Data Alliance},
doi = {10.15497/rda00039},
sciwheel-projects = {ro-crate-paper}
}
@article{doi:10.1371/journal.pcbi.1006750,
title = {Ten principles for machine-actionable data management plans.},
author = {Miksa, Tomasz and Simms, Stephanie and Mietchen, Daniel and Jones, Sarah},
pages = {e1006750},
year = {2019},
month = {mar},
day = {28},
journal = {{PLoS} Computational Biology},
volume = {15},
number = {3},
doi = {10.1371/journal.pcbi.1006750},
pmid = {30921316},
pmcid = {PMC6438441},
sciwheel-projects = {ro-crate-paper},
abstract = {Data management plans ({DMPs}) are documents accompanying research proposals and project outputs. {DMPs} are created as free-form text and describe the data and tools employed in scientific investigations. They are often seen as an administrative exercise and not as an integral part of research practice. There is now widespread recognition that the {DMP} can have more thematic, machine-actionable richness with added value for all stakeholders: researchers, funders, repository managers, research administrators, data librarians, and others. The research community is moving toward a shared goal of making {DMPs} machine-actionable to improve the experience for all involved by exchanging information across research tools and systems and embedding {DMPs} in existing workflows. This will enable parts of the {DMP} to be automatically generated and shared, thus reducing administrative burdens and improving the quality of information within a {DMP}. This paper presents 10 principles to put machine-actionable {DMPs} ({maDMPs}) into practice and realize their benefits. The principles contain specific actions that various stakeholders are already undertaking or should undertake in order to work together across research communities to achieve the larger aims of the principles themselves. We describe existing initiatives to highlight how much progress has already been made toward achieving the goals of {maDMPs} as well as a call to action for those who wish to get involved.}
}
@article{doi:10.5281/zenodo.3922136,
title = {{RO}-Crate {RDA} {maDMP} Mapper},
author = {Arfaoui, Ghaith and Jaoua, Maroua},
year = {2020},
journal = {Zenodo},
doi = {10.5281/zenodo.3922136}
}

@article{arxiv:2103.13138v1,
title = {{SCHeMa}: Scheduling Scientific Containers on a Cluster of Heterogeneous Machines},
author = {Vergoulis, Thanasis and Zagganas, Konstantinos and Kavouras, Loukas and Reczko, Martin and Sartzetakis, Stelios and Dalamagas, Theodore},
pages = {2103.13138},
url = {https://arxiv.org/abs/2103.13138v1},
year = {2021},
month = {mar},
day = {24},
journal = {arXiv},
abstract = {In the era of data-driven science, conducting computational experiments thatinvolve analysing large datasets using heterogeneous computational clusters, is part of the everyday routine for many scientists. Moreover, to ensure the credibility of their results, it is very important for these analyses to be easily reproducible by other researchers. Although various technologies, that could facilitate the work of scientists in this direction, have been introduced in the recent years, there is still a lack of open source platforms that combine them to this end. In this work, we describe and demonstrate {SCHeMa}, an open-source platform that facilitates the execution and reproducibility of computational analysis on heterogeneous clusters, leveraging containerization,experiment packaging, workflow management, and machine learning technologies.}
}

@inproceedings{doi:10.1109/eScience.2019.00068,
title = {Application of {BagIt}-Serialized Research Object Bundles for Packaging and Re-Execution of Computational Analyses},
author = {Chard, Kyle and Gaffney, Niall and Jones, Matthew B. and Kowalik, Kacper and Ludascher, Bertram and {McPhillips}, Timothy and Nabrzyski, Jarek and Stodden, Victoria and Taylor, Ian and Thelen, Thomas and Turk, Matthew J. and Willis, Craig},
pages = {514-521},
publisher = {IEEE},
url = {https://ieeexplore.ieee.org/document/9041738/},
year = {2019},
month = {sep},
day = {24},
urldate = {2021-04-27},
isbn = {978-1-7281-2451-3},
doi = {10.1109/eScience.2019.00068},
sciwheel-projects = {ro-crate-paper},
booktitle = {15th International Conference on {eScience} ({eScience} 2019)}
}
@article{doi:10.3897/rio.6.e57602,
title = {Landscape analysis for the Specimen Data Refinery},
author = {Walton, Stephanie and Livermore, Laurence and Bánki, Olaf and Cubey, Robert and Drinkwater, Robyn and Englund, Markus and Goble, Carole and Groom, Quentin and Kermorvant, Christopher and Rey, Isabel and Santos, Celia and Scott, Ben and Williams, Alan and Wu, Zhengzhe},
year = {2020},
month = {aug},
day = {14},
journal = {Research Ideas and Outcomes},
volume = {6},
issn = {2367-7163},
doi = {10.3897/rio.6.e57602},
abstract = {This report reviews the current state-of-the-art applied approaches on automated tools, services and workflows for extracting information from images of natural history specimens and their labels. We consider the potential for repurposing existing tools, including workflow management systems; and areas where more development is required. This paper was written as part of the {SYNTHESYS}+ project for software development teams and informatics teams working on new software-based approaches to improve mass digitisation of natural history specimens.}
}
@techreport{dcat2,
title = {Data Catalog Vocabulary ({DCAT}) - Version 2},
author = {Albertoni, Riccardo and Browning, David and Cox, Simon and Gonzalez Beltran, Alejandra and Perego, Andrea and Winstanley, Peter and Dataset Exchange Working Group,},
publisher = {W3C},
url = {https://www.w3.org/TR/2020/REC-vocab-dcat-2-20200204/},
year = {2020},
month = {feb},
day = {4},
urldate = {2021-05-10},
sciwheel-projects = {ro-crate-paper},
type = {{W3C} Recommendation}
}

@article{doi:10.1145/3486897,
      title={Methods Included: Standardizing Computational Reuse and Portability with the Common Workflow Language}, 
      author={Michael R. Crusoe and Sanne Abeln and Alexandru Iosup and Peter Amstutz and John Chilton and Nebojša Tijanić and Hervé Ménager and Stian Soiland-Reyes and Carole Goble},
      year={2021},
      journal={Communications of the ACM},
      note={Accepted},
      doi={10.1145/3486897},
}

@misc{CheckMyCrate,
title = {{KockataEPich}/{CheckMyCrate}: A command line application for validating a {RO}-Crate object against a {JSON} profile.},
author = {Belchev, Kostadin},
publisher = {GitHub},
url = {https://github.com/KockataEPich/CheckMyCrate},
month = {May},
year = {2021},
urldate = {2021-05-19},
abstract = {A command line application for validating a {RO}-Crate object against a {JSON} profile.},
type = {{SOFTWARE}.{COMPUTER\_SOFTWARE}}
}

@article{doi:10.4126/frl01-006423289,
title = {Towards semantic representation of machine-actionable Data Management Plans},
author = {Cardoso, João and Garcia Castro, Leyla Jael and Ekaputra, Fajar and Jacquemot-Perbal, Marie-Christine and Miksa, Tomasz and Borbinha, José},
url = {https://repository.publisso.de/resource/frl:6423289},
year = {2020},
urldate = {2021-05-19},
journal = {PUBLISSO},
doi = {10.4126/frl01-006423289},
sciwheel-projects = {ro-crate-paper}
}
@InProceedings{doi:10.1007/978-3-030-45442-5_15,
author="Cardoso, Jo{\~a}o
and Proen{\c{c}}a, Diogo
and Borbinha, Jos{\'e}",
editor="Jose, Joemon M.
and Yilmaz, Emine
and Magalh{\~a}es, Jo{\~a}o
and Castells, Pablo
and Ferro, Nicola
and Silva, M{\'a}rio J.
and Martins, Fl{\'a}vio",
title="Machine-Actionable Data Management Plans: A Knowledge Retrieval Approach to Automate the Assessment of Funders' Requirements",
booktitle="Advances in Information Retrieval",
year="2020",
publisher="Springer International Publishing",
address="Cham",
pages="118--125",
abstract="Funding bodies and other policy-makers are increasingly more concerned with Research Data Management (RDM). The Data Management Plan (DMP) is one of the tools available to perform RDM tasks, however it is not a perfect concept. The Machine-Actionable Data Management Plan (maDMP) is a concept that aims to make the DMP interoperable, automated and increasingly standardised. In this paper we showcase that through the usage of semantic technologies, it is possible to both express and exploit the features of the maDMP. In particular, we focus on showing how a maDMP formalised as an ontology can be used automate the assessment of a funder's requirements for a given organisation.",
doi={10.1007/978-3-030-45442-5_15},
isbn="978-3-030-45442-5"
}

@book{doi:10.2200/S00334ED1V01Y201102WBE001,
title = {Linked Data: Evolving the Web into a Global Data Space},
author = {Heath, Tom and Bizer, Christian},
pages = {1-136},
url = {http://www.morganclaypool.com/doi/abs/10.2200/S00334ED1V01Y201102WBE001},
year = {2011},
month = {feb},
day = {9},
urldate = {2021-05-19},
journal = {Synthesis Lectures on the Semantic Web: Theory and Technology},
volume = {1},
number = {1},
issn = {2160-4711},
isbn = {9781608454310},
doi = {10.2200/S00334ED1V01Y201102WBE001}
}
@techreport{PROVO,
author = {
    Timothy Lebo and
    Satya Sahoo and
    Deborah McGuinness and
    Khalid Belhajjame and
    James Cheney and
    David Corsar and
    Daniel Garijo and
    Stian Soiland-Reyes and
    Stephan Zednik and
    Jun Zhao
},
title = {PROV-O: The PROV Ontology},
note = {W3C Recommendation Note 30 April 2013},
url = {http://www.w3.org/TR/2013/REC-prov-o-20130430/},
day = {30},
month = {apr},
year = {2013},
}
@manual{doi:10.5281/zenodo.12586,
  title        = {Research Object Bundle 1.0},
  author       = {Soiland-Reyes, Stian and
                  Gamble, Matthew and
                  Haines, Robert},
  month        = nov,
  year         = 2014,
  note         = {See https://w3id.org/bundle/2014-11-05/},
  doi          = {10.5281/zenodo.12586},
  url          = {https://w3id.org/bundle/2014-11-05/}
}
@article{doi:10.1038/s41587-020-0439-x,
title = {The nf-core framework for community-curated bioinformatics pipelines.},
author = {Ewels, Philip A and Peltzer, Alexander and Fillinger, Sven and Patel, Harshil and Alneberg, Johannes and Wilm, Andreas and Garcia, Maxime Ulysse and Di Tommaso, Paolo and Nahnsen, Sven},
pages = {276-278},
year = {2020},
journal = {Nature Biotechnology},
volume = {38},
number = {3},
issn = {1087-0156},
doi = {10.1038/s41587-020-0439-x},
pmid = {32055031},
sciwheel-projects = {ro-crate-paper}
}
@article{doi:10.1371/journal.ppat.1008643,
title = {No more business as usual: Agile and effective responses to emerging pathogen threats require open data and open analytics.},
author = {Baker, Dannon and van den Beek, Marius and Blankenberg, Daniel and Bouvier, Dave and Chilton, John and Coraor, Nate and Coppens, Frederik and Eguinoa, Ignacio and Gladman, Simon and Grüning, Björn and Keener, Nicholas and Larivière, Delphine and Lonie, Andrew and Kosakovsky Pond, Sergei and Maier, Wolfgang and Nekrutenko, Anton and Taylor, James and Weaver, Steven},
pages = {e1008643},
year = {2020},
month = {aug},
day = {13},
journal = {{PLoS} Pathogens},
volume = {16},
number = {8},
doi = {10.1371/journal.ppat.1008643},
pmid = {32790776},
pmcid = {PMC7425854},
sciwheel-projects = {ro-crate-paper},
abstract = {The current state of much of the Wuhan pneumonia virus (severe acute respiratory syndrome coronavirus 2 [{SARS}-{CoV}-2]) research shows a regrettable lack of data sharing and considerable analytical obfuscation. This impedes global research cooperation, which is essential for tackling public health emergencies and requires unimpeded access to data, analysis tools, and computational infrastructure. Here, we show that community efforts in developing open analytical software tools over the past 10 years, combined with national investments into scientific computational infrastructure, can overcome these deficiencies and provide an accessible platform for tackling global health emergencies in an open and transparent manner. Specifically, we use all {SARS}-{CoV}-2 genomic data available in the public domain so far to (1) underscore the importance of access to raw data and (2) demonstrate that existing community efforts in curation and deployment of biomedical software can reliably support rapid, reproducible research during global health crises. All our analyses are fully documented at https://github.com/galaxyproject/{SARS}-{CoV}-2.}
}
@article{doi:10.1093/bioinformatics/bts480,
title = {Snakemake--a scalable bioinformatics workflow engine.},
author = {Köster, Johannes and Rahmann, Sven},
pages = {2520-2522},
year = {2012},
month = {oct},
day = {1},
urldate = {2017-09-07},
journal = {Bioinformatics},
volume = {28},
number = {19},
doi = {10.1093/bioinformatics/bts480},
pmid = {22908215},
sciwheel-projects = {{CWL} and ro-crate-paper},
abstract = {{SUMMARY}: Snakemake is a workflow engine that provides a readable Python-based workflow definition language and a powerful execution environment that scales from single-core workstations to compute clusters without modifying the workflow. It is the first system to support the use of automatically inferred multiple named wildcards (or variables) in input and output filenames. {AVAILABILITY}: http://snakemake.googlecode.com. {CONTACT}: johannes.koester@uni-due.de.}
}
@article{doi:10.1038/nbt.3820,
title = {Nextflow enables reproducible computational workflows.},
author = {Di Tommaso, Paolo and Chatzou, Maria and Floden, Evan W and Barja, Pablo Prieto and Palumbo, Emilio and Notredame, Cedric},
pages = {316-319},
year = {2017},
month = {apr},
day = {11},
journal = {Nature Biotechnology},
volume = {35},
number = {4},
doi = {10.1038/nbt.3820},
pmid = {28398311},
sciwheel-projects = {{CWL} and ro-crate-paper}
}
@misc{doi:10.5281/zenodo.4633732,
  author       = {Stian Soiland-Reyes},
  title        = {{Describing and packaging workflows using RO-Crate 
                   and BioCompute Objects}},
  month        = apr,
  year         = 2021,
  note         = {{Webinar for U.S. Food and 
                   Drug Administration (FDA), 2021-05-12}},
  publisher    = {Zenodo},
  doi          = {10.5281/zenodo.4633732}
}


@article{doi:10.1045/january2011-crosas,
title = {The Dataverse Network: An Open-Source Application for Sharing, Discovering and Preserving Data},
author = {Crosas, Mercè},
year = {2011},
month = {jan},
journal = {D-Lib Magazine},
volume = {17},
number = {1/2},
issn = {1082-9873},
doi = {10.1045/january2011-crosas},
sciwheel-projects = {ro-crate-paper}
}
@article{doi:10.1182/blood-2017-03-735654,
title = {The {NCI} Genomic Data Commons as an engine for precision medicine.},
author = {Jensen, Mark A and Ferretti, Vincent and Grossman, Robert L and Staudt, Louis M},
pages = {453-459},
year = {2017},
month = {jul},
day = {27},
journal = {Blood},
volume = {130},
number = {4},
doi = {10.1182/blood-2017-03-735654},
pmid = {28600341},
pmcid = {PMC5533202},
sciwheel-projects = {ro-crate-paper},
abstract = {The National Cancer Institute Genomic Data Commons ({GDC}) is an information system for storing, analyzing, and sharing genomic and clinical data from patients with cancer. The recent high-throughput sequencing of cancer genomes and transcriptomes has produced a big data problem that precludes many cancer biologists and oncologists from gleaning knowledge from these data regarding the nature of malignant processes and the relationship between tumor genomic profiles and treatment response. The {GDC} aims to democratize access to cancer genomic data and to foster the sharing of these data to promote precision medicine approaches to the diagnosis and treatment of cancer.}
}
@article{doi:10.5334/dsj-2019-044,
title = {The Australian Research Data Commons},
author = {Barker, Michelle and Wilkinson, Ross and Treloar, Andrew},
url = {http://datascience.codata.org/articles/10.5334/dsj-2019-044/},
year = {2019},
month = {sep},
day = {5},
journal = {Data Science Journal},
volume = {18},
issn = {1683-1470},
doi = {10.5334/dsj-2019-044},
sciwheel-projects = {ro-crate-paper}
}
@misc{doi:10.7557/5.5422,
title = {Harvard Data Commons},
author = {Crosas, Mercè},
year = {2020},
month = {mar},
day = {20},
journal = {Septentrio Conference Series},
number = {2},
issn = {2387-3086},
doi = {10.7557/5.5422},
sciwheel-projects = {ro-crate-paper},
note = {European Dataverse Workshop 2020, Tromsø, Norway. 2020-01-23/--24}
}
@article{doi:10.1371/journal.pbio.2001414,
title = {Identifiers for the 21st century: How to design, provision, and reuse persistent identifiers to maximize utility and impact of life science data.},
author = {{McMurry}, Julie A and Juty, Nick and Blomberg, Niklas and Burdett, Tony and Conlin, Tom and Conte, Nathalie and Courtot, Mélanie and Deck, John and Dumontier, Michel and Fellows, Donal K and Gonzalez-Beltran, Alejandra and Gormanns, Philipp and Grethe, Jeffrey and Hastings, Janna and Hériché, Jean-Karim and Hermjakob, Henning and Ison, Jon C and Jimenez, Rafael C and Jupp, Simon and Kunze, John and Laibe, Camille and Le Novère, Nicolas and Malone, James and Martin, Maria Jesus and {McEntyre}, Johanna R and Morris, Chris and Muilu, Juha and Müller, Wolfgang and Rocca-Serra, Philippe and Sansone, Susanna-Assunta and Sariyar, Murat and Snoep, Jacky L and Soiland-Reyes, Stian and Stanford, Natalie J and Swainston, Neil and Washington, Nicole and Williams, Alan R and Wimalaratne, Sarala M and Winfree, Lilly M and Wolstencroft, Katherine and Goble, Carole and Mungall, Christopher J and Haendel, Melissa A and Parkinson, Helen},
pages = {e2001414},
year = {2017},
month = {jun},
day = {29},
journal = {{PLoS} Biology},
volume = {15},
number = {6},
doi = {10.1371/journal.pbio.2001414},
pmid = {28662064},
pmcid = {PMC5490878},
keywords = {Stian},
sciwheel-projects = {{CWL} and test},
abstract = {In many disciplines, data are highly decentralized across thousands of online databases (repositories, registries, and knowledgebases). Wringing value from such databases depends on the discipline of data science and on the humble bricks and mortar that make integration possible; identifiers are a core component of this integration infrastructure. Drawing on our experience and on work by other groups, we outline 10 lessons we have learned about the identifier qualities and best practices that facilitate large-scale data integration. Specifically, we propose actions that identifier practitioners (database providers) should take in the design, provision and reuse of identifiers. We also outline the important considerations for those referencing identifiers in various circumstances, including by authors and data generators. While the importance and relevance of each lesson will vary by context, there is a need for increased awareness about how to avoid and manage common identifier problems, especially those related to persistence and web-accessibility/resolvability. We focus strongly on web-based identifiers in the life sciences; however, the principles are broadly relevant to other disciplines.}
}

@article{doi:10.1093/nar/gky379,
title = {The Galaxy platform for accessible, reproducible and collaborative biomedical analyses: 2018 update.},
author = {Afgan, Enis and Baker, Dannon and Batut, Bérénice and van den Beek, Marius and Bouvier, Dave and Cech, Martin and Chilton, John and Clements, Dave and Coraor, Nate and Grüning, Björn A and Guerler, Aysam and Hillman-Jackson, Jennifer and Hiltemann, Saskia and Jalili, Vahid and Rasche, Helena and Soranzo, Nicola and Goecks, Jeremy and Taylor, James and Nekrutenko, Anton and Blankenberg, Daniel},
pages = {W537-W544},
year = {2018},
month = {jul},
day = {2},
journal = {Nucleic Acids Research},
volume = {46},
number = {W1},
issn = {0305-1048},
doi = {10.1093/nar/gky379},
pmid = {29790989},
pmcid = {PMC6030816},
sciwheel-projects = {CWL},
abstract = {Galaxy (homepage: https://galaxyproject.org, main public server: https://usegalaxy.org) is a web-based scientific analysis platform used by tens of thousands of scientists across the world to analyze large biomedical datasets such as those found in genomics, proteomics, metabolomics and imaging. Started in 2005, Galaxy continues to focus on three key challenges of data-driven biomedical science: making analyses accessible to all researchers, ensuring analyses are completely reproducible, and making it simple to communicate analyses so that they can be reused and extended. During the last two years, the Galaxy team and the open-source community around Galaxy have made substantial improvements to Galaxy's core framework, user interface, tools, and training materials. Framework and user interface improvements now enable Galaxy to be used for analyzing tens of thousands of datasets, and \textgreater5500 tools are now available from the Galaxy {ToolShed}. The Galaxy community has led an effort to create numerous high-quality tutorials focused on common types of genomic analyses. The Galaxy developer and user communities continue to grow and be integral to Galaxy's development. The number of Galaxy public servers, developers contributing to the Galaxy framework and its tools, and users of the main Galaxy server have all increased substantially.}
}
@article{doi:10.1002/cpe.1228,
title = {Provenance trails in the Wings/Pegasus system},
author = {Kim, Jihie and Deelman, Ewa and Gil, Yolanda and Mehta, Gaurang and Ratnakar, Varun},
pages = {587-597},
url = {http://doi.wiley.com/10.1002/cpe.1228},
year = {2008},
month = {apr},
day = {10},
urldate = {2021-06-02},
journal = {Concurrency and Computation: Practice and Experience},
volume = {20},
number = {5},
issn = {15320626},
doi = {10.1002/cpe.1228},
sciwheel-projects = {ro-crate-paper}
}

@article{vandesompel_2007,
title = {Interoperability for the Discovery, Use, and Re-Use of Units of Scholarly Communication},
author = {Van de Sompel, Herbert and Lagoze, Carl},
url = {http://icl.utk.edu/ctwatch/quarterly/articles/2007/08/interoperability-for-the-discovery-use-and-re-use-of-units-of-scholarly-communication/},
year = {2007},
month = {aug},
urldate = {2021-08-12},
journal = {{CTWatch} Quarterly},
volume = {3},
number = {3},
sciwheel-projects = {2021-ro-crate-paper},
abstract = {Improvements in computing and network technologies, digital data capture, and data mining techniques are enabling research methods that are highly collaborative, network-based, and data-intensive. These methods challenge existing scholarly communication mechanisms, which are largely based on physical (paper, ink, and voice) rather than digital technologies. One major challenge to the existing system is the change in the nature of the unit of scholarly communication. In the established scholarly communication system, the dominant communication units are journals and their contained articles. This established system generally fails to deal with other types of research results in the sciences and humanities, including datasets, simulations, software, dynamic knowledge representations, annotations, and aggregates thereof, all of which should be considered units of scholarly communication. Another challenge is the increasing importance of machine agents (e.g., web crawlers, data mining applications) as consumers of scholarly materials. The established system by and large targets human consumers. However, all communication units (including the journal publications) should be available as source materials for machine-based applications that mine, interpret, and visualize these materials to generate new units of communication and new knowledge. Yet another challenge to the existing system lies in the changing nature of the social activity that is scholarly communication. Increasingly, this social activity extends beyond traditional journals and conference proceedings, and even beyond more recent phenomena such as preprint systems, institutional repositories, and dataset repositories. It now includes less formal and more dynamic communication such as blogging. Scholarly communication is suddenly all over the web, both in traditional publication portals and in new social networking venues, and is interlinked with the broader social network of the web. Dealing adequately with this communication revolution requires fundamental changes in the scholarly communication system. Many of the required changes in response to these challenges are of a socio-cultural nature and relate directly to the question of what constitutes the scholarly record in this new environment. This raises the fundamental issue of how the crucial functions of scholarly communication 2 – registration, certification, awareness, archiving, rewarding – should be re-implemented in the new context. The solutions to these socio-cultural questions rely in part on the development of basic technical infrastructure to support an innately digital scholarly communication system. This paper describes the work of the Object Re-Use and Exchange ({ORE}) project of the Open Archives Initiative ({OAI}) to develop one component of this new infrastructure in order to support the revolutionized scholarly communication paradigm – standards to facilitate discovery, use and re-use of new types of compound scholarly communication units by networked services and applications. Compound units are aggregations of distinct information units that, when combined, form a logical whole. Some examples of these are a digitized book that is an aggregation of chapters, where each chapter is an aggregation of scanned pages, and a scholarly publication that is an aggregation of text and supporting materials such as datasets, software tools, and video recordings of an experiment. The {ORE} work aims to develop mechanisms for representing and referencing compound information units in a machine-readable manner that is independent of both the actual content of the information unit and nature of the re-using application.}
}

@inproceedings{doi:10.3233/978-1-61499-660-6-9,
title = {What Is Ontology Reuse?},
author = {Katsumi, Megan and Grüninger, Michael},
editor = {Ferrario, Roberta and Kuhn, Werner},
series = {Frontiers in Artificial Intelligence and Applications},
pages = {9-22},
publisher = {{IOS} Press},
doi = {10.3233/978-1-61499-660-6-9},
url = {https://doi.org/10.3233/978-1-61499-660-6-9},
year = {2016},
volume = {283},
isbn = {978-1-61499-660-6},
sciwheel-projects = {2021-ro-crate-paper},
booktitle = {Formal Ontology in Information Systems}
}

@article{doi:10.1016/j.future.2019.03.046,
title = {Enabling {FAIR} research in Earth Science through research objects},
author = {Garcia-Silva, Andres and Gomez-Perez, Jose Manuel and Palma, Raul and Krystek, Marcin and Mantovani, Simone and Foglini, Federica and Grande, Valentina and De Leo, Francesco and Salvi, Stefano and Trasatti, Elisa and Romaniello, Vito and Albani, Mirko and Silvagni, Cristiano and Leone, Rosemarie and Marelli, Fulvio and Albani, Sergio and Lazzarini, Michele and Napier, Hazel J. and Glaves, Helen M. and Aldridge, Timothy and Meertens, Charles and Boler, Fran and Loescher, Henry W. and Laney, Christine and Genazzio, Melissa A. and Crawl, Daniel and Altintas, Ilkay},
pages = {550-564},
year = {2019},
month = {sep},
journal = {Future Generation Computer Systems},
volume = {98},
issn = {0167739X},
doi = {10.1016/j.future.2019.03.046},
sciwheel-projects = {2021-ro-crate-paper},
abstract = {Abstract Data-intensive science communities are progressively adopting {FAIR} practices that enhance the visibility of scientific breakthroughs and enable reuse. At the core of this movement, research objects contain and describe scientific information and resources in a way compliant with the {FAIR} principles and sustain the development of key infrastructure and tools. This paper provides an account of the challenges, experiences and solutions involved in the adoption of {FAIR} around research objects over several Earth Science disciplines. During this journey, our work has been comprehensive, with outcomes including: an extended research object model adapted to the needs of earth scientists; the provisioning of digital object identifiers ({DOI}) to enable persistent identification and to give due credit to authors; the generation of content-based, semantically rich, research object metadata through natural language processing, enhancing visibility and reuse through recommendation systems and third-party search engines; and various types of checklists that provide a compact representation of research object quality as a key enabler of scientific reuse. All these results have been integrated in {ROHub}, a platform that provides research object management functionality to a wealth of applications and interfaces across different scientific communities. To monitor and quantify the community uptake of research objects, we have defined indicators and obtained measures via {ROHub} that are also discussed herein.}
}
@article{doi:10.1007/s10209-016-0475-y,
title = {A comparison of research data management platforms: architecture, flexible metadata and interoperability},
author = {Amorim, Ricardo Carvalho and Castro, João Aguiar and Rocha da Silva, João and Ribeiro, Cristina},
pages = {1-12},
year = {2016},
month = {jun},
day = {11},
journal = {Universal Access in the Information Society},
issn = {1615-5289},
doi = {10.1007/s10209-016-0475-y},
sciwheel-projects = {2021-ro-crate-paper},
abstract = {Research data management is rapidly becoming a regular concern for researchers, and institutions need to provide them with platforms to support data organization and preparation for publication. Some institutions have adopted institutional repositories as the basis for data deposit, whereas others are experimenting with richer environments for data description, in spite of the diversity of existing workflows. This paper is a synthetic overview of current platforms that can be used for data management purposes. Adopting a pragmatic view on data management, the paper focuses on solutions that can be adopted in the long tail of science, where investments in tools and manpower are modest. First, a broad set of data management platforms is presented\textemdashsome designed for institutional repositories and digital libraries\textemdashto select a short list of the more promising ones for data management. These platforms are compared considering their architecture, support for metadata, existing programming interfaces, as well as their search mechanisms and community acceptance. In this process, the stakeholders\textquoteright requirements are also taken into account. The results show that there is still plenty of room for improvement, mainly regarding the specificity of data description in different domains, as well as the potential for integration of the data management platforms with existing research management tools. Nevertheless, depending on the context, some platforms can meet all or part of the stakeholders\textquoteright requirements.}
}
@inproceedings{farnel_2014,
title = {Metadata for Research Data: Current Practices and Trends},
author = {Farnel, Sharon and Shiri, Ali},
editor = {Moen, William and Rushing, Amy},
publisher = {Dublin Core Metadata Initiative},
url = {https://dcpapers.dublincore.org/pubs/article/view/3714},
year = {2014},
urldate = {2021-08-13},
issn = {1939-1366},
booktitle = {2014 Proceedings of the International Conference on Dublin Core and Metadata Applications}
}

@techreport{doi:10.2777/620649,
title = {{EOSC} Interoperability Framework},
author = {Kurowski, Krzysztof and Corcho, Oscar and Choirat, Christine and Eriksson, Magnus and Coppens, Frederik and van de Sanden, Mark and Ojsteršek, Milan},
series = {Report from the {EOSC} Executive Board Working Groups {FAIR} and Architecture},
publisher = {Publications Office of the {EU}},
doi = {10.2777/620649},
year = {2021},
month = {feb},
day = {5},
keywords = {{EOSC} and metadata},
abstract = {This document has been developed by the Interoperability Task Force of the {EOSC} Executive Board {FAIR} Working Group, with participation from the Architecture {WG}. Achieving interoperability within {EOSC} is essential in order for the federation of services that will compose {EOSC} to provide added value for service users. In the context of the {FAIR} principles, interoperability is discussed in relation to the fact that \textquotedblleftresearch data usually need to be integrated with other data; in addition, the data need to interoperate with applications or workflows for analysis, storage, and processing\textquotedblright. Our view on interoperability does not only consider data but also the many other research artefacts that may be used in the context of research activity, such as software code, scientific workflows, laboratory protocols, open hardware designs, etc. It also considers the need to make services and e-infrastructures as interoperable as possible. This document identifies the general principles that should drive the creation of the {EOSC} Interoperability Framework ({EOSC} {IF}), and organises them into the four layers that are commonly considered in other interoperability frameworks (e.g., the European Interoperability Framework - {EIF}): technical, semantic, organisational and legal interoperability. For each of these layers, a catalogue of problems and needs, as well as challenges and high-level recommendations have been proposed, which should be considered in the further development and implementation of the {EOSC} {IF} components. Such requirements and recommendations have been developed after an extensive review of related literature as well as by running interviews with stakeholders from {ERICs} (European Research Infrastructure Consortia), {ESFRI} (European Strategy Forum on Research Infrastructures) projects, service providers and research communities. Some examples of such requirements are: \textquotedblleftevery semantic artefact that is being maintained in {EOSC} must have sufficient associated documentation, with clear examples of usage and conceptual diagrams\textquotedblright, or {\textquotedblleftCoarse}-grained and fine-grained dataset (and other research object) search tools need to be made available\textquotedblright, etc. The document finally contains a proposal for the management of {FAIR} Digital Objects in the context of {EOSC} and a reference architecture for the {EOSC} Interoperability Framework that is inspired by and extends the European Interoperability Reference Architecture ({EIRA}), identifying the main building blocks required.}
}
@book{chan_1995,
title = {Library of Congress Subject Headings: Principles and Application},
author = {Chan, Lois Mai},
pages = {556},
publisher = {Libraries Unlimited},
url = {https://eric.ed.gov/?id={ED387146}},
year = {1995},
month = {sep},
day = {1},
urldate = {2020-05-21},
edition = {3},
isbn = {9781563081910},
address = {Englewood, Colo},
sciwheel-projects = {2020-reproducible-research},
abstract = {The Library of Congress Subject Headings ({LCSH}) system has become a primary subject retrieval tool in an environment that is very different from that for which it was designed and developed. This book is a comprehensive guide to the principles and application of {LCSH} that reflects changes and developments in the field. The text is divided into three parts. Part 1 gives a brief history of the system, analyzes its principles, and describes the vocabulary and subject authority control. Part 2 deals with the application of {LC} subject headings on {LC} {MARC} (Machine-readable cataloging) records. {LC} policies are outlined with regard to the assignment of subject headings in general and the treatment of certain types of materials in particular. Part 3 discusses the future prospects of the system as an online retrieval tool, drawing on recent literature. Appendices contain sample {MARC} subject authority records and bibliographic records, lists of the most frequently used free-floating}
}
@book{doi:10.1515/9783598441844,
title = {National bibliographies in the digital age: guidance and new directions: {IFLA} working group on guidelines for national bibliographies},
editor = {Žumer, Maja and Ifla,},
series = {{IFLA} series on bibliographic control},
publisher = {Walter de Gruyter – K. G. Saur},
url = {https://www.ifla.org/node/7858},
year = {2009},
month = {jan},
day = {28},
isbn = {9783598441844},
issn = {1868-8438},
doi = {10.1515/9783598441844},
sciwheel-projects = {2021-ro-crate-paper}
}

@article{doi:10.1007/s00267-014-0258-2,
title = {Why is data sharing in collaborative natural resource efforts so hard and what can we do to improve it?},
author = {Volk, Carol J and Lucero, Yasmin and Barnas, Katie},
pages = {883-893},
year = {2014},
month = {may},
urldate = {2021-08-13},
journal = {Environmental Management},
volume = {53},
number = {5},
doi = {10.1007/s00267-014-0258-2},
pmid = {24604667},
sciwheel-projects = {2021-ro-crate-paper},
abstract = {Increasingly, research and management in natural resource science rely on very large datasets compiled from multiple sources. While it is generally good to have more data, utilizing large, complex datasets has introduced challenges in data sharing, especially for collaborating researchers in disparate locations ("distributed research teams"). We surveyed natural resource scientists about common data-sharing problems. The major issues identified by our survey respondents (n = 118) when providing data were lack of clarity in the data request (including format of data requested). When receiving data, survey respondents reported various insufficiencies in documentation describing the data (e.g., no data collection description/no protocol, data aggregated, or summarized without explanation). Since metadata, or "information about the data," is a central obstacle in efficient data handling, we suggest documenting metadata through data dictionaries, protocols, read-me files, explicit null value documentation, and process metadata as essential to any large-scale research program. We advocate for all researchers, but especially those involved in distributed teams to alleviate these problems with the use of several readily available communication strategies including the use of organizational charts to define roles, data flow diagrams to outline procedures and timelines, and data update cycles to guide data-handling expectations. In particular, we argue that distributed research teams magnify data-sharing challenges making data management training even more crucial for natural resource scientists. If natural resource scientists fail to overcome communication and metadata documentation issues, then negative data-sharing experiences will likely continue to undermine the success of many large-scale collaborative projects.}
}
@article{doi:10.1038/s41597-020-0524-5,
title = {{COVID}-19 pandemic reveals the peril of ignoring metadata standards.},
author = {Schriml, Lynn M and Chuvochina, Maria and Davies, Neil and Eloe-Fadrosh, Emiley A and Finn, Robert D and Hugenholtz, Philip and Hunter, Christopher I and Hurwitz, Bonnie L and Kyrpides, Nikos C and Meyer, Folker and Mizrachi, Ilene Karsch and Sansone, Susanna-Assunta and Sutton, Granger and Tighe, Scott and Walls, Ramona},
pages = {188},
year = {2020},
month = {jun},
day = {19},
journal = {Scientific data},
volume = {7},
number = {1},
issn = {2052-4463},
doi = {10.1038/s41597-020-0524-5},
pmid = {32561801},
pmcid = {PMC7305141},
sciwheel-projects = {2021-ro-crate-paper}
}

@article{doi:10.1016/j.ijhcs.2020.102562,
title = {Talking datasets – Understanding data sensemaking behaviours},
author = {Koesten, Laura and Gregory, Kathleen and Groth, Paul and Simperl, Elena},
pages = {102562},
year = {2021},
month = {feb},
journal = {International journal of human-computer studies},
volume = {146},
issn = {10715819},
doi = {10.1016/j.ijhcs.2020.102562},
sciwheel-projects = {2021-ro-crate-paper}
}

@Article{natmet_2019_swcit,
title="Giving software its due",
journal="Nature Methods",
year="2019",
volume="16",
number="3",
pages="207--207",
abstract="Software and algorithm development is crucial for scientific progress; we discuss how to improve the impact and recognition of these tools.",
issn="1548-7105",
doi="10.1038/s41592-019-0350-x",
url="https://doi.org/10.1038/s41592-019-0350-x"
}

@article{rettberg_2015_openaire,
author = {Rettberg, Najla and Schmidt, Birgit},
title = {OpenAIRE},
journal = {College \& Research Libraries News},
year = {2015},
volume = {76},
number = {6},
pages = {306-310},
abstract = {Research outcomes resulting from taxpayers’ investment in research are a common good and should be made openly available for all. According to the European Commission (EC), open access (OA) is defined as the “practice of providing online access to reusable scientific information that is free of charge to the end user.” The EC is a significant funder of research and facilitates collaborative and cross-disciplinary scientific activities. In 2008, the EC launched the Open Access Pilot, requiring beneficiaries of its previous funding program, the 7th Framework Programme (FP7), to make their best effort to ensure OA to peer-reviewed articles. Its new funding program, Horizon2020, will invest nearly €80 billion in competitive research. Here the mandate was strengthened to stipulate that the publication output of all EC-funded projects be made open.},
keywords = {OpenAIRE; open access; repositories; e-infrastructures},
url = {http://resolver.sub.uni-goettingen.de/purl?gs-1/11942}
}
