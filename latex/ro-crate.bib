
@article{doi:10.1093/nar/gkq429,
title = {{myExperiment}: a repository and social network for the sharing of bioinformatics workflows.},
author = {Goble, Carole A and Bhagat, Jiten and Aleksejevs, Sergejs and Cruickshank, Don and Michaelides, Danius and Newman, David and Borkum, Mark and Bechhofer, Sean and Roos, Marco and Li, Peter and De Roure, David},
pages = {W677-82},
year = {2010},
month = {jul},
day = {1},
urldate = {2021-05-04},
journal = {Nucleic Acids Research},
volume = {38},
number = {Web Server issue},
doi = {10.1093/nar/gkq429},
pmid = {20501605},
pmcid = {PMC2896080},
sciwheel-projects = {ro-crate-paper},
abstract = {{myExperiment} (http://www.myexperiment.org) is an online research environment that supports the social sharing of bioinformatics workflows. These workflows are procedures consisting of a series of computational tasks using web services, which may be performed on data from its retrieval, integration and analysis, to the visualization of the results. As a public repository of workflows, {myExperiment} allows anybody to discover those that are relevant to their research, which can then be reused and repurposed to their specific requirements. Conversely, developers can submit their workflows to {myExperiment} and enable them to be shared in a secure manner. Since its release in 2007, {myExperiment} currently has over 3500 registered users and contains more than 1000 workflows. The social aspect to the sharing of these workflows is facilitated by registered users forming virtual communities bound together by a common interest or research project. Contributors of workflows can build their reputation within these communities by receiving feedback and credit from individuals who reuse their work. Further documentation about {myExperiment} including its {REST} web service is available from http://wiki.myexperiment.org. Feedback and requests for support can be sent to bugs@myexperiment.org.}
}
@article{doi:10.1038/sdata.2016.18,
title = {The {FAIR} Guiding Principles for scientific data management and stewardship.},
author = {Wilkinson, Mark D and Dumontier, Michel and Aalbersberg, I Jsbrand Jan and Appleton, Gabrielle and Axton, Myles and Baak, Arie and Blomberg, Niklas and Boiten, Jan-Willem and da Silva Santos, Luiz Bonino and Bourne, Philip E and Bouwman, Jildau and Brookes, Anthony J and Clark, Tim and Crosas, Mercè and Dillo, Ingrid and Dumon, Olivier and Edmunds, Scott and Evelo, Chris T and Finkers, Richard and Gonzalez-Beltran, Alejandra and Gray, Alasdair J G and Groth, Paul and Goble, Carole and Grethe, Jeffrey S and Heringa, Jaap and 't Hoen, Peter A C and Hooft, Rob and Kuhn, Tobias and Kok, Ruben and Kok, Joost and Lusher, Scott J and Martone, Maryann E and Mons, Albert and Packer, Abel L and Persson, Bengt and Rocca-Serra, Philippe and Roos, Marco and van Schaik, Rene and Sansone, Susanna-Assunta and Schultes, Erik and Sengstag, Thierry and Slater, Ted and Strawn, George and Swertz, Morris A and Thompson, Mark and van der Lei, Johan and van Mulligen, Erik and Velterop, Jan and Waagmeester, Andra and Wittenburg, Peter and Wolstencroft, Katherine and Zhao, Jun and Mons, Barend},
pages = {160018},
year = {2016},
month = {mar},
day = {15},
urldate = {2018-07-13},
journal = {Scientific data},
volume = {3},
issn = {2052-4463},
doi = {10.1038/sdata.2016.18},
pmid = {26978244},
pmcid = {PMC4792175},
sciwheel-projects = {ro-crate-paper},
abstract = {There is an urgent need to improve the infrastructure supporting the reuse of scholarly data. A diverse set of stakeholders-representing academia, industry, funding agencies, and scholarly publishers-have come together to design and jointly endorse a concise and measureable set of principles that we refer to as the {FAIR} Data Principles. The intent is that these may act as a guideline for those wishing to enhance the reusability of their data holdings. Distinct from peer initiatives that focus on the human scholar, the {FAIR} Principles put specific emphasis on enhancing the ability of machines to automatically find and use the data, in addition to supporting its reuse by individuals. This Comment is the first formal publication of the {FAIR} Principles, and includes the rationale behind them, and some exemplar implementations in the community.}
}
@article{doi:10.1186/1471-2105-11-S12-S5,
title = {Community-driven computational biology with Debian Linux.},
author = {Möller, Steffen and Krabbenhöft, Hajo Nils and Tille, Andreas and Paleino, David and Williams, Alan and Wolstencroft, Katy and Goble, Carole and Holland, Richard and Belhachemi, Dominique and Plessy, Charles},
pages = {S5},
year = {2010},
month = {dec},
day = {21},
urldate = {2021-05-04},
journal = {{BMC} Bioinformatics},
volume = {11 Suppl 12},
doi = {10.1186/1471-2105-11-S12-S5},
pmid = {21210984},
pmcid = {PMC3040531},
sciwheel-projects = {ro-crate-paper},
abstract = {{BACKGROUND}: The Open Source movement and its technologies are popular in the bioinformatics community because they provide freely available tools and resources for research. In order to feed the steady demand for updates on software and associated data, a service infrastructure is required for sharing and providing these tools to heterogeneous computing environments. {RESULTS}: The Debian Med initiative provides ready and coherent software packages for medical informatics and bioinformatics. These packages can be used together in Taverna workflows via the {UseCase} plugin to manage execution on local or remote machines. If such packages are available in cloud computing environments, the underlying hardware and the analysis pipelines can be shared along with the software. {CONCLUSIONS}: Debian Med closes the gap between developers and users. It provides a simple method for offering new releases of software and data resources, thus provisioning a local infrastructure for computational biology. For geographically distributed teams it can ensure they are working on the same versions of tools, in the same conditions. This contributes to the world-wide networking of researchers.}
}
@article{doi:10.1016/j.future.2011.08.004,
title = {Why linked data is not enough for scientists},
author = {Bechhofer, Sean and Buchan, Iain and De Roure, David and Missier, Paolo and Ainsworth, John and Bhagat, Jiten and Couch, Philip and Cruickshank, Don and Delderfield, Mark and Dunlop, Ian and Gamble, Matthew and Michaelides, Danius and Owen, Stuart and Newman, David and Sufi, Shoaib and Goble, Carole},
pages = {599-611},
year = {2013},
month = {feb},
urldate = {2021-05-04},
journal = {Future Generation Computer Systems},
volume = {29},
number = {2},
issn = {0167739X},
doi = {10.1016/j.future.2011.08.004},
sciwheel-projects = {ro-crate-paper},
abstract = {Scientific data represents a significant portion of the linked open data cloud and scientists stand to benefit from the data fusion capability this will afford. Publishing linked data into the cloud, however, does not ensure the required reusability. Publishing has requirements of provenance, quality, credit, attribution and methods to provide the reproducibility that enables validation of results. In this paper we make the case for a scientific data publication model on top of linked data and introduce the notion of Research Objects as first class citizens for sharing and publishing.}
}
@article{doi:10.1016/j.future.2017.01.012,
title = {Scientific workflows for computational reproducibility in the life sciences: Status, challenges and opportunities},
author = {Cohen-Boulakia, Sarah and Belhajjame, Khalid and Collin, Olivier and Chopard, Jérôme and Froidevaux, Christine and Gaignard, Alban and Hinsen, Konrad and Larmande, Pierre and Bras, Yvan Le and Lemoine, Frédéric and Mareuil, Fabien and Ménager, Hervé and Pradal, Christophe and Blanchet, Christophe},
pages = {284-298},
year = {2017},
month = {oct},
urldate = {2018-07-13},
journal = {Future Generation Computer Systems},
volume = {75},
issn = {0167739X},
doi = {10.1016/j.future.2017.01.012},
sciwheel-projects = {ro-crate-paper},
abstract = {With the development of new experimental technologies, biologists are faced with an avalanche of data to be computationally analyzed for scientific advancements and discoveries to emerge. Faced with the complexity of analysis pipelines, the large number of computational tools, and the enormous amount of data to manage, there is compelling evidence that many if not most scientific discoveries will not stand the test of time: increasing the reproducibility of computed results is of paramount importance. The objective we set out in this paper is to place scientific workflows in the context of reproducibility. To do so, we define several kinds of repro-ducibility that can be reached when scientific workflows are used to perform experiments. We characterize and define the criteria that need to be catered for by reproducibility-friendly scientific workflow systems, and use such criteria to place several representative and widely used workflow systems and companion tools within such a framework. We also discuss the remaining challenges posed by reproducible scientific workflows in the life sciences. Our study was guided by three use cases from the life science domain involving in silico experiments.}
}
@article{doi:10.1007/s41019-017-0050-4,
title = {Robust Cross-Platform Workflows: How Technical and Scientific Communities Collaborate to Develop, Test and Share Best Practices for Data Analysis},
author = {Möller, Steffen and Prescott, Stuart W. and Wirzenius, Lars and Reinholdtsen, Petter and Chapman, Brad and Prins, Pjotr and Soiland-Reyes, Stian and Klötzl, Fabian and Bagnacani, Andrea and Kalaš, Matúš and Tille, Andreas and Crusoe, Michael R.},
pages = {232-244},
year = {2017},
month = {nov},
day = {16},
urldate = {2018-07-13},
journal = {Data Science and Engineering},
volume = {2},
number = {3},
issn = {2364-1185},
doi = {10.1007/s41019-017-0050-4},
sciwheel-projects = {ro-crate-paper},
abstract = {Information integration and workflow technologies for data analysis have always been major fields of investigation in bioinformatics. A range of popular workflow suites are available to support analyses in computational biology. Commercial providers tend to offer prepared applications remote to their clients. However, for most academic environments with local expertise, novel data collection techniques or novel data analysis, it is essential to have all the flexibility of open-source tools and open-source workflow descriptions. Workflows in data-driven science such as computational biology have considerably gained in complexity. New tools or new releases with additional features arrive at an enormous pace, and new reference data or concepts for quality control are emerging. A well-abstracted workflow and the exchange of the same across work groups have an enormous impact on the efficiency of research and the further development of the field. High-throughput sequencing adds to the avalanche of data available in the field; efficient computation and, in particular, parallel execution motivate the transition from traditional scripts and Makefiles to workflows. We here review the extant software development and distribution model with a focus on the role of integration testing and discuss the effect of common workflow language on distributions of open-source scientific software to swiftly and reliably provide the tools demanded for the execution of such formally described workflows. It is contended that, alleviated from technical differences for the execution on local machines, clusters or the cloud, communities also gain the technical means to test workflow-driven interaction across several software packages.}
}
@article{doi:10.3389/fninf.2017.00069,
title = {Re-run, Repeat, Reproduce, Reuse, Replicate: Transforming Code into Scientific Contributions.},
author = {Benureau, Fabien C Y and Rougier, Nicolas P},
pages = {69},
year = {2017},
urldate = {2021-05-04},
journal = {Frontiers in Neuroinformatics},
volume = {11},
doi = {10.3389/fninf.2017.00069},
pmid = {29354046},
pmcid = {PMC5758530},
sciwheel-projects = {ro-crate-paper},
abstract = {Scientific code is different from production software. Scientific code, by producing results that are then analyzed and interpreted, participates in the elaboration of scientific conclusions. This imposes specific constraints on the code that are often overlooked in practice. We articulate, with a small example, five characteristics that a scientific code in computational science should possess: re-runnable, repeatable, reproducible, reusable, and replicable. The code should be executable (re-runnable) and produce the same result more than once (repeatable); it should allow an investigator to reobtain the published results (reproducible) while being easy to use, understand and modify (reusable), and it should act as an available reference for any ambiguity in the algorithmic descriptions of the article (replicable).}
}
@article{doi:10.1038/s41592-018-0046-7,
title = {Bioconda: sustainable and comprehensive software distribution for the life sciences.},
author = {Grüning, Björn and Dale, Ryan and Sjödin, Andreas and Chapman, Brad A and Rowe, Jillian and Tomkins-Tinch, Christopher H and Valieris, Renan and Köster, Johannes and Bioconda Team},
pages = {475-476},
year = {2018},
urldate = {2018-07-13},
journal = {Nature Methods},
volume = {15},
number = {7},
issn = {1548-7091},
doi = {10.1038/s41592-018-0046-7},
pmid = {29967506},
sciwheel-projects = {ro-crate-paper}
}
@article{doi:10.1016/j.cels.2018.03.014,
title = {Practical computational reproducibility in the life sciences.},
author = {Grüning, Björn and Chilton, John and Köster, Johannes and Dale, Ryan and Soranzo, Nicola and van den Beek, Marius and Goecks, Jeremy and Backofen, Rolf and Nekrutenko, Anton and Taylor, James},
pages = {631-635},
year = {2018},
month = {jun},
day = {27},
urldate = {2018-07-13},
journal = {Cell Systems},
volume = {6},
number = {6},
issn = {24054712},
doi = {10.1016/j.cels.2018.03.014},
pmid = {29953862},
pmcid = {PMC6263957},
sciwheel-projects = {ro-crate-paper},
abstract = {Many areas of research suffer from poor reproducibility, particularly in computationally intensive domains where results rely on a series of complex methodological decisions that are not well captured by traditional publication approaches. Various guidelines have emerged for achieving reproducibility, but implementation of these practices remains difficult due to the challenge of assembling software tools plus associated libraries, connecting tools together into pipelines, and specifying parameters. Here, we discuss a suite of cutting-edge technologies that make computational reproducibility not just possible, but practical in both time and effort. This suite combines three well-tested components-a system for building highly portable packages of bioinformatics software, containerization and virtualization technologies for isolating reusable execution environments for these packages, and workflow systems that automatically orchestrate the composition of these packages for entire pipelines-to achieve an unprecedented level of computational reproducibility. We also provide a practical implementation and five recommendations to help set a typical researcher on the path to performing data analyses reproducibly. Copyright \copyright 2018 Elsevier Inc. All rights reserved.}
}
@techreport{doi:10.17487/rfc3987,
title = {Internationalized resource identifiers ({IRI}s)},
author = {Duerst, M. and Suignard, M.},
publisher = {{RFC} Editor},
url = {https://www.rfc-editor.org/info/rfc3987},
year = {2005},
month = {jan},
urldate = {2018-07-15},
doi = {10.17487/rfc3987},
sciwheel-projects = {ro-crate-paper}
}
@article{doi:10.1038/s41586-019-0965-1,
title = {A new genomic blueprint of the human gut microbiota.},
author = {Almeida, Alexandre and Mitchell, Alex L and Boland, Miguel and Forster, Samuel C and Gloor, Gregory B and Tarkowska, Aleksandra and Lawley, Trevor D and Finn, Robert D},
pages = {499-504},
year = {2019},
month = {feb},
day = {11},
urldate = {2021-05-04},
journal = {Nature},
volume = {568},
number = {7753},
issn = {0028-0836},
doi = {10.1038/s41586-019-0965-1},
pmid = {30745586},
pmcid = {PMC6784870},
sciwheel-projects = {ro-crate-paper},
abstract = {The composition of the human gut microbiota is linked to health and disease, but knowledge of individual microbial species is needed to decipher their biological roles. Despite extensive culturing and sequencing efforts, the complete bacterial repertoire of the human gut microbiota remains undefined. Here we identify 1,952 uncultured candidate bacterial species by reconstructing 92,143 metagenome-assembled genomes from 11,850 human gut microbiomes. These uncultured genomes substantially expand the known species repertoire of the collective human gut microbiota, with a 281\% increase in phylogenetic diversity. Although the newly identified species are less prevalent in well-studied populations compared to reference isolate genomes, they improve classification of understudied African and South American samples by more than 200\%. These candidate species encode hundreds of newly identified biosynthetic gene clusters and possess a distinctive functional capacity that might explain their elusive nature. Our work expands the known diversity of uncultured gut bacteria, which provides unprecedented resolution for taxonomic and functional characterization of the intestinal microbiota.}
}
@article{doi:10.1371/journal.pbio.3000099,
title = {Enabling precision medicine via standard communication of {HTS} provenance, analysis, and results.},
author = {Alterovitz, Gil and Dean, Dennis and Goble, Carole and Crusoe, Michael R and Soiland-Reyes, Stian and Bell, Amanda and Hayes, Anais and Suresh, Anita and Purkayastha, Anjan and King, Charles H and Taylor, Dan and Johanson, Elaine and Thompson, Elaine E and Donaldson, Eric and Morizono, Hiroki and Tsang, Hsinyi and Vora, Jeet K and Goecks, Jeremy and Yao, Jianchao and Almeida, Jonas S and Keeney, Jonathon and Addepalli, {KanakaDurga} and Krampis, Konstantinos and Smith, Krista M and Guo, Lydia and Walderhaug, Mark and Schito, Marco and Ezewudo, Matthew and Guimera, Nuria and Walsh, Paul and Kahsay, Robel and Gottipati, Srikanth and Rodwell, Timothy C and Bloom, Toby and Lai, Yuching and Simonyan, Vahan and Mazumder, Raja},
pages = {e3000099},
year = {2018},
month = {dec},
day = {31},
urldate = {2021-05-04},
journal = {{PLoS} Biology},
volume = {16},
number = {12},
doi = {10.1371/journal.pbio.3000099},
pmid = {30596645},
pmcid = {PMC6338479},
sciwheel-projects = {ro-crate-paper},
abstract = {A personalized approach based on a patient's or pathogen's unique genomic sequence is the foundation of precision medicine. Genomic findings must be robust and reproducible, and experimental data capture should adhere to findable, accessible, interoperable, and reusable ({FAIR}) guiding principles. Moreover, effective precision medicine requires standardized reporting that extends beyond wet-lab procedures to computational methods. The {BioCompute} framework (https://w3id.org/biocompute/1.3.0) enables standardized reporting of genomic sequence data provenance, including provenance domain, usability domain, execution domain, verification kit, and error domain. This framework facilitates communication and promotes interoperability. Bioinformatics computation instances that employ the {BioCompute} framework are easily relayed, repeated if needed, and compared by scientists, regulators, test developers, and clinicians. Easing the burden of performing the aforementioned tasks greatly extends the range of practical application. Large clinical trials, precision medicine, and regulatory submissions require a set of agreed upon standards that ensures efficient communication and documentation of genomic analyses. The {BioCompute} paradigm and the resulting {BioCompute} Objects ({BCOs}) offer that standard and are freely accessible as a {GitHub} organization (https://github.com/biocompute-objects) following the "Open-Stand.org principles for collaborative open standards development." With high-throughput sequencing ({HTS}) studies communicated using a {BCO}, regulatory agencies (e.g., Food and Drug Administration [{FDA}]), diagnostic test developers, researchers, and clinicians can expand collaboration to drive innovation in precision medicine, potentially decreasing the time and cost associated with next-generation sequencing workflow exchange, reporting, and regulatory reviews.}
}
@article{doi:10.6084/m9.figshare.3115156.v2,
title = {Common Workflow Language, v1.0},
author = {Amstutz, Peter and Crusoe, Michael R. and Tijanić, Nebojša and Chapman, Brad and Chilton, John and Heuer, Michael and Kartashov, Andrey and Leehr, Dan and Ménager, Hervé and Nedeljkovich, Maya and Scales, Matt and Soiland-Reyes, Stian and Stojanovic, Luka},
url = {https://www.commonwl.org/v1.0/},
year = {2016},
urldate = {2021-05-04},
journal = {Figshare},
doi = {10.6084/m9.figshare.3115156.v2},
sciwheel-projects = {ro-crate-paper},
abstract = {The Common Workflow Language ({CWL}) is an informal, multi-vendor working group consisting of various organizations and individuals that have an interest in portability of data analysis workflows. Our goal is to create specifications that enable data scientists to describe analysis tools and workflows that are powerful, easy to use, portable, and support reproducibility.{CWL} builds on technologies such as {JSON}-{LD} and Avro for data modeling and Docker for portable runtime environments. {CWL} is designed to express workflows for data-intensive science, such as Bioinformatics, Medical Imaging, Chemistry, Physics, and Astronomy.This is v1.0 of the {CWL} tool and workflow specification, released on 2016-07-08.The specification, in {HTML} format, is in the draft-3/docs folder.}
}
@article{doi:10.3233/DS-190026,
title = {Towards {FAIR} principles for research software},
author = {Lamprecht, Anna-Lena and Garcia, Leyla and Kuzak, Mateusz and Martinez, Carlos and Arcila, Ricardo and Martin Del Pico, Eva and Dominguez Del Angel, Victoria and van de Sandt, Stephanie and Ison, Jon and Martinez, Paula Andrea and {McQuilton}, Peter and Valencia, Alfonso and Harrow, Jennifer and Psomopoulos, Fotis and Gelpi, Josep Ll. and Chue Hong, Neil and Goble, Carole and Capella-Gutierrez, Salvador},
pages = {1-23},
year = {2019},
month = {nov},
day = {13},
urldate = {2021-02-22},
journal = {Déviance et société},
issn = {24518492},
doi = {10.3233/{DS}-190026},
sciwheel-projects = {ro-crate-paper}
}
@article{doi:10.1162/dint_a_00033,
title = {{FAIR} Computational Workflows},
author = {Goble, Carole and Cohen-Boulakia, Sarah and Soiland-Reyes, Stian and Garijo, Daniel and Gil, Yolanda and Crusoe, Michael R. and Peters, Kristian and Schober, Daniel},
pages = {108-121},
year = {2019},
month = {nov},
day = {1},
urldate = {2021-05-04},
journal = {Data Intelligence},
issn = {2641-{435X}},
doi = {10.1162/dint\_a\_00033},
sciwheel-projects = {ro-crate-paper},
abstract = {Computational workflows describe the complex multi-step methods that are used for data collection, data preparation, analytics, predictive modelling, and simulation that lead to new data products. They can inherently contribute to the {FAIR} data principles: by processing data according to established metadata; by creating metadata themselves during the processing of data; and by tracking and recording data provenance. These properties aid data quality assessment and contribute to secondary data usage. Moreover, workflows are digital objects in their own right. This paper argues that {FAIR} principles for workflows need to address their specific nature in terms of their composition of executable software steps, their provenance, and their development.}
}
@article{doi:10.5281/zenodo.4605654,
title = {Implementing {FAIR} Digital Objects in the {EOSC}-Life Workflow Collaboratory},
author = {Goble, Carole and Soiland-Reyes, Stian and Bacall, Finn and Owen, Stuart and Williams, Alan and Eguinoa, Ignacio and Droesbeke, Bert and Leo, Simone and Pireddu, Luca and Rodríguez-Navas, Laura and Fernández, José Mª and Capella-Gutierrez, Salvador and Ménager, Hervé and Grüning, Björn and Serrano-Solano, Beatriz and Ewels, Philip and Coppens, Frederik},
year = {2021},
urldate = {2021-03-29},
journal = {Zenodo},
doi = {10.5281/zenodo.4605654},
sciwheel-projects = {ro-crate-paper},
abstract = {The practice of performing computational processes using workflows has taken hold in the biosciences as the discipline becomes increasingly computational. The {COVID}-19 pandemic has spotlighted the importance of systematic and shared analysis of {SARS}-{CoV}-2 and its data processing pipelines. This is coupled with a drive in the community towards adopting {FAIR} practices (Findable, Accessible, Interoperable, and Reusable) not just for data, but also for workflows, and to improve the reproducibility of processes, both manual and computational. {EOSC}-Life brings together 13 of the Life Science {\textquoteleftESFRI\textquoteright} research infrastructures to create an open, digital and collaborative space for biological and medical research. The project is developing a cloud-based workflow collaboratory to drive implementation of {FAIR} workflows across disciplines and {RI} boundaries, and foster tool- focused collaborations and reuse between communities via the sharing of data analysis workflows. The collaboratory aims to provide a framework for researchers and workflow specialists to use and reuse workflows. As such it is an example of the Canonical Workflow Frameworks for Research ({CWFR}) vision in practice. {EOSC}-Life is made up of established research infrastructures ranging from biobanking and clinical trial management, through to coordinating biomedical imaging and plant phenotyping to multi-omic and systems-based data analysis. The heterogeneity of the disciplines is reflected in the diversity of their data analysis needs and practices and the variety of workflow management systems they use. Many have specialist platforms developed over years. Workflow management systems in common use include Galaxy, Snakemake, and Nextflow, and more specialist, domain-specific systems such as {SCIPION}. To serve the needs of this established and diverse community, {EOSC}-Life has developed {WorkflowHub} as an inclusive workflow registry, agnostic to any Workflow Management System ({WfMS}). {WorkflowHub} aims to incorporate their workflows in partnership with the {WfMS}, to embed the registration of workflows in the community processes, e.g. based on pre-existing workflow repositories. The registry adopts common practices, e.g.use of {GitHub} repositories, and supports integration with the ecosystem of tool packages, assisted by registries (bio.tools, biocontainers), and services for testing and benchmarking workflows ({OpenEBench}, {LifeMonitor}). As an umbrella registry, the Hub makes workflows Findable and Accessible by indexing workflows across workflow management systems and their native repositories, while providing rich standardized metadata. Interoperability and Reusability is supported by standardized descriptions of workflows and packaging of workflow components, developed in close collaboration with the communities. The {WorkflowHub} creates a place for registering and discovering libraries of workflows developed by collaborating teams, with suitable features for versioning, credit, analytics, and import/export needed to support the reuse of workflows, the development of sub-workflows as canonical steps and ultimately the identification of common patterns in the workflows. At the heart of the collaboratory is a Digital Object framework for documenting and exchanging workflows annotated with machine processable metadata produced and consumed by the participating platforms. The Digital Object framework is founded on several needs: Describing a workflow and its steps in a canonical, normalised and {WfMS} independent way: we use the Common Workflow Language ({CWL}), more specifically the Abstract {CWL} (non-executable) description variant to accompany the native workflow definitions. This presents the structure, composed tools and external interface in an interoperable way across workflow languages. {WfMS} can generate abstract {CWL}, already demonstrated for Galaxy, next to the \textquoteleftnative\textquoteright Galaxy workflow description. This language duality is an important retention aspect of reproducibility, as the structure and metadata of the workflow can be accessed independent of its native format as {CWL}, even if that may no longer be executable, capturing the canonical workflow in a {FAIR} format. The co-presence of the native format enables direct reuse in the specific {WfMS}, benefitting from all its features. Metadata about a workflow and its tools using a minimal information model: we use the Bioschemas profiles Computational Tool, Computational Workflow and Formal Parameter which are discipline independent, opinionated conventions for using schema.org annotations. Bioschemas enables us to capture and publish workflow registrations and their metadata as {FAIR} Digital Objects. The {EDAM} Ontology is further used to add bioinformatics-specific metadata, such as strong typing of inputs and outputs, within both Abstract {CWL} and Bioschemas annotations. Organising and packaging the definitions and components of a workflow with their associated objects such as test data: we use a Workflow profile specialisation of {RO}-Crate, a community developed standardised approach for research output packaging with rich metadata. {RO}-Crate provides us the ability to package executable workflows, their components such as example and test data, abstract {CWL}, diagrams and their documentation. This makes workflows more readily re-usable. {RO}-Crate is the base unit of upload and download at the {WorkflowHub}. As {CWFR} Digital Objects of workflows, {RO}-Crates are activation-ready and circulated between the different services for execution and testing. Identifiers for all the components: like {FAIR} Digital Objects, {RO}-Crates can be metadata-rich bags of identifiers and can themselves be assigned permanent identifiers. This enables the full description of a computational analysis, from input data, over tools and workflows, to final results. Using these components we have built an environment that supports the Workflow Life Cycle, from abstract description, through to a specific rendering in a {WfMS} to its execution and the documentation of its run provenance, results and continued testing.}
}
@misc{neylon_blog_post_2017,
title = {As a researcher…I'm a bit bloody fed up with Data Management},
author = {Neylon, Cameron},
journal = {Science In The Open},
url = {https://cameronneylon.net/blog/as-a-researcher-im-a-bit-bloody-fed-up-with-data-management/},
year = {2017},
month = {jul},
day = {16},
urldate = {2021-05-04},
sciwheel-projects = {ro-crate-paper},
type = {BLOG\_POST}
}

@misc{chipseq/nf-core,
title = {chipseq/nf-core outputs},
url = {https://nf-co.re/chipseq/1.2.2/output},
urldate = {2021-05-04},
sciwheel-projects = {ro-crate-paper},
type = {WEBSITE}
}
@article{doi:10.5281/zenodo.4711243,
title = {nf-core/chipseq: nf-core/chipseq v1.2.2 - Rusty Mole},
author = {Patel, Harshil and Wang, Chuan and Ewels, Phil and Silva, Tiago Chedraoui and Peltzer, Alexander and Behrens, Drew and Garcia, Maxime and Mashehu and Rotholandus and Haglund, Sofia and Kretzschmar, Winni},
year = {2021},
urldate = {2021-05-04},
journal = {Zenodo},
doi = {10.5281/zenodo.4711243},
sciwheel-projects = {ro-crate-paper},
abstract = {[1.2.2] - 2021-04-22 \#206 - Minor patch release to fix Conda environment Dependencies Update r-base 3.6.2 -\textgreater 3.6.3 Update r-xfun 0.15 -\textgreater 0.20}
}
@misc{ro-crate-metadata-file,
title = {Root Data Entity - Research Object Crate ({RO}-Crate) Root Data Entity \textbar Research Object Crate ({RO}-Crate)},
url = {https://www.researchobject.org/ro-crate/1.1/root-data-entity.html\#ro-crate-metadata-file-descriptor},
urldate = {2021-05-04},
sciwheel-projects = {ro-crate-paper},
type = {WEBSITE}
}
@misc{rocrate-root,
title = {Root Data Entity - Research Object Crate ({RO}-Crate) Root Data Entity \textbar Research Object Crate ({RO}-Crate)},
url = {https://www.researchobject.org/ro-crate/1.1/root-data-entity.html\#direct-properties-of-the-root-data-entity},
urldate = {2021-05-04},
sciwheel-projects = {ro-crate-paper},
type = {WEBSITE}
}
@techreport{doi:10.17487/rfc8493,
title = {The {BagIt} File Packaging Format (V1.0)},
author = {Kunze, J. and Littman, J. and Madden, E. and Scancella, J. and Adams, C.},
publisher = {{RFC} Editor},
url = {https://www.rfc-editor.org/info/rfc8493},
year = {2018},
month = {oct},
urldate = {2021-05-04},
doi = {10.17487/{RFC8493}},
sciwheel-projects = {ro-crate-paper}
}
@techreport{ocfl_2020,
title = {Oxford Common File Layout Specification},
author = {{OCFL}},
editor = {Hankinson, Andrew and Jefferies, Neil and Metz, Rosalyn and Morley, Julian and Warner, Simeon and Woods, Andrew},
publisher = {OCFL},
url = {https://ocfl.io/1.0/spec/},
year = {2020},
month = {jul},
day = {7},
urldate = {2021-05-04},
edition = {1.0},
sciwheel-projects = {ro-crate-paper},
type = {Recommendation}
}

@misc{rocrate-contextualentities,
title = {Contextual Entities - Research Object Crate ({RO}-Crate) Contextual Entities \textbar Research Object Crate ({RO}-Crate)},
url = {https://www.researchobject.org/ro-crate/1.1/contextual-entities.html\#contextual-vs-data-entities},
urldate = {2021-05-04},
sciwheel-projects = {ro-crate-paper},
type = {WEBSITE}
}

@techreport{httprange14,
title = {Dereferencing {HTTP} {URIs}},
author = {{W3C Technical Architecture Group}},
editor = {Lewis, Rhys},
publisher = {W3C},
url = {https://www.w3.org/2001/tag/doc/{httpRange}-14/2007-08-31/{HttpRange}-14.html},
year = {2007},
month = {aug},
day = {31},
urldate = {2021-05-04},
edition = {2007-08-31},
sciwheel-projects = {ro-crate-paper},
type = {Draft Tag Finding}
}

@techreport{sporny_2014,
title = {{JSON}-{LD} 1.0},
author = {Sporny, Manu and Longley, Dave and Kellogg, Gregg and Lanthaler, Markus and Lindström, Niklas},
publisher = {W3C},
url = {https://www.w3.org/{TR}/2014/{REC}-json-ld-20140116/},
year = {2014},
month = {jan},
day = {16},
urldate = {2021-05-04},
sciwheel-projects = {ro-crate-paper},
type = {{W3C} Recommendation}
}

@misc{schema.org,
title = {Schema.org - Schema.org},
url = {https://schema.org/},
urldate = {2021-05-04},
sciwheel-projects = {ro-crate-paper},
type = {WEBSITE}
}

@techreport{doi:10.5281/zenodo.4541002,
title = {{RO}-Crate Metadata Specification 1.1.1},
author = {Sefton, Peter and Carragáin, Eoghan {\'{O}} and Soiland-Reyes, Stian and Corcho, Oscar and Garijo, Daniel and Palma, Raul and Coppens, Frederik and Goble, Carole and Fernández, José María and Chard, Kyle and Gomez-Perez, Jose Manuel and Crusoe, Michael R and Eguinoa, Ignacio and Juty, Nick and Holmes, Kristi and Clark, Jason A. and Capella-Gutierrez, Salvador and Gray, Alasdair J. G. and Owen, Stuart and Williams, Alan R. and Tartari, Giacomo and Bacall, Finn and Thelen, Thomas and Ménager, Hervé and Navas, Laura Rodríguez-Navas and Walk, Paul and Whitehead, Brandon and Wilkinson, Mark and Groth, Paul and Bremer, Erich and Castro, {LJ} Garcia and Sebby, Karl and Kanitz, Alexander and Trisovic, Ana and Kennedy, Gavin and Graves, Mark and Koehorst, Jasper and Leo, Simone and Portier, Marc},
url = {https://w3id.org/ro/crate/1.1},
year = {2021},
urldate = {2021-05-04},
journal = {Zenodo},
doi = {10.5281/zenodo.4541002},
sciwheel-projects = {ro-crate-paper},
abstract = {Web-version: https://w3id.org/ro/crate/1.1 This document specifies a method, known as {RO}-Crate (Research Object Crate), of aggregating and describing research data with associated metadata. {RO}-Crates can aggregate and describe any resource including files, {URI}-addressable resources, or use other addressing schemes to locate digital or physical data. {RO}-Crates can describe data in aggregate and at the individual resource level, with metadata to aid in discovery, re-use and long term management of data. Metadata includes the ability to describe the context of data and the entities involved in its production, use and reuse. For example: who created it, using which equipment, software and workflows, under what licenses can it be re-used, where was it collected, and/or where is it about. {RO}-Crate uses {JSON}-{LD} to to express this metadata using linked data, describing data resources as well as contextual entities such as people, organizations, software and equipment as a series of linked {JSON}-{LD} objects - using common published vocabularies, chiefly schema.org. The core of {RO}-Crate is a {JSON}-{LD} file, the {RO}-Crate Metadata File, named ro-crate-metadata.json. This file contains structured metadata about the dataset as a whole (the Root Data Entity) and, optionally, about some or all of its files. This provides a simple way to, for example, assert the authors (e.g. people, organizations) of the {RO}-Crate or one its files, or to capture more complex provenance for files, such as how they were created using software and equipment. While providing the formal specification for {RO}-Crate, this document also aims to be a practical guide for software authors to create tools for generating and consuming research data packages, with explanation by examples.}
}

@techreport{doi:10.5281/zenodo.3541888,
title = {{RO}-Crate Metadata Specification 1.0},
author = {Sefton, Peter and Carragáin, Eoghan {\'{O}} and Soiland-Reyes, Stian and Corcho, Oscar and Garijo, Daniel and Palma, Raul and Coppens, Frederik and Goble, Carole and Fernández, José María and Chard, Kyle and Gomez-Perez, Jose Manuel and Crusoe, Michael R and Eguinoa, Ignacio and Juty, Nick and Holmes, Kristi and Clark, Jason A. and Capella-Gutierrez, Salvador and Gray, Alasdair J. G. and Owen, Stuart and Williams, Alan R. and Tartari, Giacomo and Bacall, Finn and Thelen, Thomas},
year = {2019},
url = {https://w3id.org/ro/crate/1.0},
journal = {Zenodo},
doi = {10.5281/zenodo.3541888},
sciwheel-projects = {ro-crate-paper},
abstract = {Web-version: https://w3id.org/ro/crate/1.0 This document specifies a method, known as {RO}-Crate (Research Object Crate), of organizing file-based data with associated metadata, using linked data principles, in both human and machine readable formats, with the ability to include additional domain-specific metadata. The core of {RO}-Crate is a {JSON}-{LD} file, the {RO}-Crate Metadata File, named ro-crate-metadata.jsonld. This file contains structured metadata about the dataset as a whole (the Root Data Entity) and, optionally, about some or all of its files. This provides a simple way to, for example, assert the authors (e.g. people, organizations) of the {RO}-Crate or one its files, or to capture more complex provenance for files, such as how they were created using software and equipment. While providing the formal specification for {RO}-Crate, this document also aims to be a practical guide for software authors to create tools for generating and consuming research data packages, with explanation by examples.}
}
@techreport{doi:10.5281/zenodo.3406497,
title = {{RO}-Crate Metadata Specifications},
author = {Sefton, Peter and Carragáin, Eoghan {\'{O}} and Soiland-Reyes, Stian and Corcho, Oscar and Garijo, Daniel and Palma, Raul and Coppens, Frederik and Goble, Carole and Fernández, José María and Chard, Kyle and Gomez-Perez, Jose Manuel and Crusoe, Michael R and Eguinoa, Ignacio and Juty, Nick and Holmes, Kristi and Clark, Jason A. and Capella-Gutierrez, Salvador and Gray, Alasdair J. G. and Owen, Stuart and Williams, Alan R. and Tartari, Giacomo and Bacall, Finn and Thelen, Thomas and Ménager, Hervé and Rodríguez-Navas, Laura and Walk, Paul and Whitehead, Brandon and Wilkinson, Mark and Groth, Paul and Bremer, Erich and Castro, {LJ} Garcia and Sebby, Karl and Kanitz, Alexander and Trisovic, Ana and Kennedy, Gavin and Graves, Mark and Koehorst, Jasper and Leo, Simone and Portier, Marc},
year = {2021},
url = {https://w3id.org/ro/crate/},
journal = {Zenodo},
doi = {10.5281/zenodo.3406497},
sciwheel-projects = {ro-crate-paper},
abstract = {Web-version: https://w3id.org/ro/crate/1.1 This document specifies a method, known as {RO}-Crate (Research Object Crate), of aggregating and describing research data with associated metadata. {RO}-Crates can aggregate and describe any resource including files, {URI}-addressable resources, or use other addressing schemes to locate digital or physical data. {RO}-Crates can describe data in aggregate and at the individual resource level, with metadata to aid in discovery, re-use and long term management of data. Metadata includes the ability to describe the context of data and the entities involved in its production, use and reuse. For example: who created it, using which equipment, software and workflows, under what licenses can it be re-used, where was it collected, and/or where is it about. {RO}-Crate uses {JSON}-{LD} to to express this metadata using linked data, describing data resources as well as contextual entities such as people, organizations, software and equipment as a series of linked {JSON}-{LD} objects - using common published vocabularies, chiefly schema.org. The core of {RO}-Crate is a {JSON}-{LD} file, the {RO}-Crate Metadata File, named ro-crate-metadata.json. This file contains structured metadata about the dataset as a whole (the Root Data Entity) and, optionally, about some or all of its files. This provides a simple way to, for example, assert the authors (e.g. people, organizations) of the {RO}-Crate or one its files, or to capture more complex provenance for files, such as how they were created using software and equipment. While providing the formal specification for {RO}-Crate, this document also aims to be a practical guide for software authors to create tools for generating and consuming research data packages, with explanation by examples.}
}
@techreport{doi:10.5281/zenodo.4031327,
title = {{RO}-Crate Metadata Specification 1.1},
author = {Sefton, Peter and Carragáin, Eoghan {\'{O}} and Soiland-Reyes, Stian and Corcho, Oscar and Garijo, Daniel and Palma, Raul and Coppens, Frederik and Goble, Carole and Fernández, José María and Chard, Kyle and Gomez-Perez, Jose Manuel and Crusoe, Michael R and Eguinoa, Ignacio and Juty, Nick and Holmes, Kristi and Clark, Jason A. and Capella-Gutierrez, Salvador and Gray, Alasdair J. G. and Owen, Stuart and Williams, Alan R. and Tartari, Giacomo and Bacall, Finn and Thelen, Thomas and Ménager, Hervé and Rodríguez-Navas, Laura and Walk, Paul and Whitehead, Brandon and Wilkinson, Mark and Groth, Paul and Bremer, Erich and Castro, {LJ} Garcia and Sebby, Karl and Kanitz, Alexander and Trisovic, Ana and Kennedy, Gavin and Graves, Mark and Koehorst, Jasper and Leo, Simone},
url = {https://w3id.org/ro/crate/1.1},
year = {2020},
journal = {Zenodo},
doi = {10.5281/zenodo.4031327},
sciwheel-projects = {ro-crate-paper},
abstract = {Web-version: https://w3id.org/ro/crate/1.1 This document specifies a method, known as {RO}-Crate (Research Object Crate), of organizing file-based data with associated metadata, using linked data principles, in both human and machine readable formats, with the ability to include additional domain-specific metadata. The core of {RO}-Crate is a {JSON}-{LD} file, the {RO}-Crate Metadata File, named ro-crate-metadata.json. This file contains structured metadata about the dataset as a whole (the Root Data Entity) and, optionally, about some or all of its files. This provides a simple way to, for example, assert the authors (e.g. people, organizations) of the {RO}-Crate or one its files, or to capture more complex provenance for files, such as how they were created using software and equipment. While providing the formal specification for {RO}-Crate, this document also aims to be a practical guide for software authors to create tools for generating and consuming research data packages, with explanation by examples.}
}


@misc{ro-crate-issue-71,
title = {Use Case: As a researcher I want to be able to define ad hoc properties and use them in a crate without resorting to the verbose {PropertyValue} thing · Issue \#71 · {ResearchObject}/ro-crate · {GitHub}},
journal = {GitHub},
url = {https://github.com/{ResearchObject}/ro-crate/issues/71},
urldate = {2021-05-04},
sciwheel-projects = {ro-crate-paper},
type = {GitHub issue}
}
@misc{ro-crate-jsonld,
title = {{RO}-Crate {JSON}-{LD} - Research Object Crate ({RO}-Crate) {RO}-Crate {JSON}-{LD} \textbar Research Object Crate ({RO}-Crate)},
url = {https://www.researchobject.org/ro-crate/1.1/appendix/jsonld.html},
urldate = {2021-05-04},
sciwheel-projects = {ro-crate-paper},
type = {WEBSITE}
}
@misc{ro2018,
title = {Workshop on Research Objects ({RO2018})},
url = {https://www.researchobject.org/ro2018/},
urldate = {2021-05-04},
sciwheel-projects = {ro-crate-paper},
type = {WEBSITE}
}
@misc{rda11-data-packaging,
title = {Approaches to Research Data Packaging - {RDA} 11th Plenary {BoF} meeting \textbar {RDA}},
url = {https://rd-alliance.org/approaches-research-data-packaging-rda-11th-plenary-bof-meeting},
urldate = {2021-05-04},
sciwheel-projects = {ro-crate-paper},
type = {WEBSITE}
}
@article{doi:10.5281/zenodo.3250687,
title = {A lightweight approach to research object data packaging},
author = {Carragáin, Eoghan {\'{O}} and Goble, Carole and Sefton, Peter and Soiland-Reyes, Stian},
year = {2019},
urldate = {2021-05-04},
journal = {Zenodo},
doi = {10.5281/zenodo.3250687},
sciwheel-projects = {ro-crate-paper},
abstract = {A Research Object ({RO}) provides a machine-readable mechanism to communicate the diverse set of digital and real-world resources that contribute to an item of research. The aim of an {RO} is to evolve from traditional academic publication as a static {PDF}, to rather provide a complete and structured archive of the items (such as people, organisations, funding, equipment, software etc) that contributed to the research outcome, including their identifiers, provenance, relations and annotations. This is of particular importance as all domains of research and science are increasingly relying on computational analysis, yet we are facing a reproducibility crisis because key components are often not sufficiently tracked, archived or reported. Here we propose Research Object Crate (or {RO}-Crate for short), an emerging lightweight approach to packaging research data with their structured metadata, rephrasing the Research Object model as schema.org annotations to formalize a {JSON}-{LD} format that can be used independently of infrastructure, e.g. in {GitHub} or Zenodo archives. {RO}-Crate can be extended for domain-specific descriptions, aiming at a wide variety of applications and repositories to encourage {FAIR} sharing of reproducible datasets and analytical methods.}
}
@article{doi:10.5281/zenodo.1445817,
title = {Datacrate Submisssion To The Workshop On Research Objects},
author = {Sefton, Peter},
year = {2018},
urldate = {2021-05-04},
journal = {Zenodo},
doi = {10.5281/zenodo.1445817},
sciwheel-projects = {ro-crate-paper},
abstract = {This is a somewhat experimental approach to submitting an extended abstract to a conference. The submission describes the {DataCrate} standard and the source-files, {HTML} and {PDF} versions of the submission are included in a {DataCrate} package.}
}
@misc{researchobject/ro-crate,
title = {{GitHub} - {ResearchObject}/ro-crate: Research Object Crate},
url = {https://github.com/researchobject/ro-crate/},
urldate = {2021-05-04},
sciwheel-projects = {ro-crate-paper},
type = {WEBSITE}
}
@misc{ro-crate-community,
title = {Community - Research Object Crate ({RO}-Crate) Community \textbar Research Object Crate ({RO}-Crate)},
url = {https://www.researchobject.org/ro-crate/community},
urldate = {2021-05-04},
sciwheel-projects = {ro-crate-paper},
type = {WEBSITE}
}
@misc{criminalcharacters,
title = {Criminal Characters},
author = {Alana Piper and others},
url = {https://criminalcharacters.com/},
urldate = {2021-05-04},
sciwheel-projects = {ro-crate-paper},
type = {WEBSITE}
}
@misc{describo,
title = {Arkisto Platform: Describo},
author = {La Rosa, Marco and Sefton, Peter},
url = {https://arkisto-platform.github.io/describo/},
urldate = {2021-05-04},
sciwheel-projects = {ro-crate-paper},
type = {WEBSITE}
}
@misc{describo-online,
title = {Arkisto Platform: Describo Online},
author = {La Rosa, Marco},
url = {https://arkisto-platform.github.io/describo-online/},
urldate = {2021-05-04},
sciwheel-projects = {ro-crate-paper},
type = {WEBSITE}
}
@misc{ro-crate-excel,
title = {npm: ro-crate-excel},
author = {Lynch, Mike and Sefton, Peter},
url = {https://www.npmjs.com/package/ro-crate-excel},
urldate = {2021-05-04},
sciwheel-projects = {ro-crate-paper},
type = {WEBSITE}
}
@misc{ro-crate-html-js,
title = {npm: ro-crate-html-js},
url = {https://www.npmjs.com/package/ro-crate-html-js},
urldate = {2021-05-04},
sciwheel-projects = {ro-crate-paper},
type = {WEBSITE}
}
@misc{ro-crate-js,
title = {{GitHub} - {UTS}-{eResearch}/ro-crate-js: Research Object Crate ({RO}-Crate) utilities},
url = {https://github.com/{UTS}-{eResearch}/ro-crate-js},
urldate = {2021-05-04},
journal = {GitHub},
sciwheel-projects = {ro-crate-paper},
type = {WEBSITE}
}
@misc{ro-crate-ruby,
title = {{GitHub} - {ResearchObject}/ro-crate-ruby: A Ruby gem for creating, manipulating and reading {RO}-Crates.},
author = {Bacall, Finn and Whitwell, Martyn},
url = {https://github.com/{ResearchObject}/ro-crate-ruby},
journal = {GitHub},
urldate = {2021-05-04},
sciwheel-projects = {ro-crate-paper},
type = {WEBSITE}
}
@misc{ro-crate-py,
title = {{GitHub} - {ResearchObject}/ro-crate-py: Python library for {RO}-Crate},
author = {Leo, Simone and Eguinoa, Ignacio and Soiland-Reyes, Stian and Droesbeke, Bert and Rodríguez-Navas, Laura and Gaignard, Alban},
url = {https://github.com/researchobject/ro-crate-py},
urldate = {2021-05-04},
sciwheel-projects = {ro-crate-paper},
type = {WEBSITE}
}
@misc{about-workflowhub,
title = {{WorkflowHub} project \textbar Project pages for developing and running the {WorkflowHub}, a registry of scientific workflows.},
url = {https://about.workflowhub.eu/},
urldate = {2021-05-04},
sciwheel-projects = {ro-crate-paper},
type = {WEBSITE}
}
@misc{modpdsc,
title = {{GitHub} - {CoEDL}/modpdsc},
url = {https://github.com/{CoEDL}/modpdsc/},
urldate = {2021-05-04},
sciwheel-projects = {ro-crate-paper},
type = {WEBSITE}
}
@misc{arkisto-data-portal,
title = {Tools: Data Portal \& Discovery},
url = {https://arkisto-platform.github.io/tools/portal/},
urldate = {2021-05-04},
sciwheel-projects = {ro-crate-paper},
type = {WEBSITE}
}
@misc{ocfl-tools,
title = {{GitHub} - {CoEDL}/ocfl-tools: Tools to process and manipulate an {OCFL} tree},
url = {https://github.com/{CoEDL}/ocfl-tools},
urldate = {2021-05-04},
sciwheel-projects = {ro-crate-paper},
type = {WEBSITE}
}
@misc{ro-composer,
title = {eScienceLab: RO-Composer},
author = {Bacall, Finn and Soiland-Reyes, Stian and Soares e Silva, Marina},
url = {https://esciencelab.org.uk/projects/ro-composer/},
urldate = {2021-05-04},
sciwheel-projects = {ro-crate-paper},
type = {WEBSITE}
}
@misc{galaxy2cwl,
title = {{GitHub} - workflowhub-eu/galaxy2cwl: Standalone version tool to get cwl descriptions (initially an abstract cwl interface) of galaxy workflows and Galaxy workflows executions.},
url = {https://github.com/workflowhub-eu/galaxy2cwl},
urldate = {2021-05-04},
sciwheel-projects = {ro-crate-paper},
type = {WEBSITE}
}
@misc{bioschemas-computationalworkflow,
title = {Bioschemas ComputationalWorkflow 1.0},
url = {https://bioschemas.org/profiles/{ComputationalWorkflow}/1.0-{RELEASE}/},
urldate = {2021-05-04},
sciwheel-projects = {ro-crate-paper},
type = {WEBSITE}
}
@misc{rocrate-workflows,
title = {Workflows and scripts - Research Object Crate ({RO}-Crate) Workflows and scripts \textbar Research Object Crate ({RO}-Crate)},
url = {https://www.researchobject.org/ro-crate/1.1/workflows.html},
urldate = {2021-05-04},
sciwheel-projects = {ro-crate-paper},
type = {WEBSITE}
}
@misc{workflow-ro-crate,
title = {Workflow {RO}-Crate ({DRAFT}) \textbar {WorkflowHub} project},
url = {https://about.workflowhub.eu/Workflow-{RO}-Crate/},
urldate = {2021-05-04},
sciwheel-projects = {ro-crate-paper},
type = {WEBSITE}
}
@misc{elixir,
title = {{ELIXIR} \textbar A distributed infrastructure for life-science information},
url = {https://elixir-europe.org/},
urldate = {2021-05-04},
sciwheel-projects = {ro-crate-paper},
type = {WEBSITE}
}
@misc{eosc-life,
title = {Home - {EOSC} Life},
url = {https://www.eosc-life.eu/},
urldate = {2021-05-04},
sciwheel-projects = {ro-crate-paper},
type = {WEBSITE}
}
@misc{workflowhub,
title = {The {WorkflowHub}},
url = {https://workflowhub.eu/},
urldate = {2021-05-04},
sciwheel-projects = {ro-crate-paper},
type = {WEBSITE}
}
@misc{bco,
title = {{BioCompute} Objects},
url = {https://www.biocomputeobject.org/},
urldate = {2021-05-04},
sciwheel-projects = {ro-crate-paper},
type = {WEBSITE}
}
@misc{doi:10.1109/IEEESTD.2020.9094416,
title = {IEEE Standard for Bioinformatics Analyses Generated by High-Throughput Sequencing (HTS) to Facilitate Communication},
note = {IEEE Std 2791-2020},
isbn = {978-1-5044-6466-6},
doi = {10.1109/{IEEESTD}.2020.9094416},
sciwheel-projects = {ro-crate-paper},
type = {Standard}
}
@misc{ieee-2791-schema,
title = {2791 object / ieee-2791-schema · {GitLab}},
url = {https://opensource.ieee.org/2791-object/ieee-2791-schema/},
urldate = {2021-05-04},
sciwheel-projects = {ro-crate-paper},
type = {WEBSITE}
}
@misc{bco-ro-crate,
title = {{BioCompute} Objects {RO}-Crate · Tutorial and specification for packaging {IEEE} 2791-2020 as {RO}-Crate Research ...},
url = {https://biocompute-objects.github.io/bco-ro-crate/},
urldate = {2021-05-04},
sciwheel-projects = {ro-crate-paper},
type = {WEBSITE}
}
@misc{ro-crate-implementation-notes,
title = {Implementation notes - Research Object Crate ({RO}-Crate) Implementation notes \textbar Research Object Crate ({RO}-Crate)},
url = {https://www.researchobject.org/ro-crate/1.1/appendix/implementation-notes.html\#adding-ro-crate-to-bagit},
urldate = {2021-05-04},
sciwheel-projects = {ro-crate-paper},
type = {WEBSITE}
}
@misc{paradisec,
title = {{PARADISEC} – Safeguarding research in Australia's region},
url = {https://www.paradisec.org.au/},
urldate = {2021-05-04},
sciwheel-projects = {ro-crate-paper},
type = {WEBSITE}
}
@misc{case-study-paradisec,
title = {Case Study: Modern {PARADISEC}},
url = {https://arkisto-platform.github.io/case-studies/paradisec/},
urldate = {2021-05-04},
sciwheel-projects = {ro-crate-paper},
type = {WEBSITE}
}
@article{doi:10.1016/j.patter.2020.100136,
title = {Dataset reuse: toward translating principles to practice.},
author = {Koesten, Laura and Vougiouklis, Pavlos and Simperl, Elena and Groth, Paul},
pages = {100136},
year = {2020},
month = {nov},
day = {13},
journal = {Patterns (New York, N.Y.)},
volume = {1},
number = {8},
issn = {26663899},
doi = {10.1016/j.patter.2020.100136},
pmid = {33294873},
pmcid = {PMC7691392},
sciwheel-projects = {ro-crate-paper},
abstract = {The web provides access to millions of datasets that can have additional impact when used beyond their original context. We have little empirical insight into what makes a dataset more reusable than others and which of the existing guidelines and frameworks, if any, make a difference. In this paper, we explore potential reuse features through a literature review and present a case study on datasets on {GitHub}, a popular open platform for sharing code and data. We describe a corpus of more than 1.4 million data files, from over 65,000 repositories. Using {GitHub}'s engagement metrics as proxies for dataset reuse, we relate them to reuse features from the literature and devise an initial model, using deep neural networks, to predict a dataset's reusability. This demonstrates the practical gap between principles and actionable insights that allow data publishers and tools designers to implement functionalities that provably facilitate reuse. \copyright 2020 The Authors.}
}
@inproceedings{doi:10.1190/1.1822162,
title = {Electronic documents give reproducible research a new meaning},
author = {Claerbout, Jon F. and Karrenbach, Martin},
pages = {601-604},
publisher = {Society of Exploration Geophysicists},
year = {1992},
month = {jan},
urldate = {2021-05-04},
doi = {10.1190/1.1822162},
sciwheel-projects = {ro-crate-paper},
booktitle = {{SEG} Technical Program Expanded Abstracts 1992}
}
@inproceedings{newman2009,
       booktitle = {Proceedings of the Workshop on Semantic Web Applications in Scientific Discourse (SWASD 2009)},
          editor = {Clark, Tim and Luciano, Joanne S. and Marshall, M. Scott and Prud’Hommeaux, Eric and Stephens, Susie},       
          series = {CEUR Workshop Proceedings},
       collection={CEUR Workshop Proceedings},          
           month = {Oct},
           title = {{myExperiment}: An ontology for e-Research},
          author = {David Newman and Sean Bechhofer and David De Roure},
            year = {2009},
            issn = {1613-0073},
           volume = {523},
           publisher={CEUR-WS},
            note = {2009-10-25},
             url = {http://ceur-ws.org/Vol-523/Newman.pdf},
        sciwheel-projects = {ro-crate-paper},
        abstract = {myExperiment describes itself as a "Social Virtual Research Environment" that provides the ability to share Research Objects (ROs) over a social infrastructure to facilitate actioning of research. The myExperiment Ontology is a logical representation of the data model used by this environment, allowing its data to be published in a standard RDF format, whilst providing a generic extensible framework that can be reused by similar projects. ROs are data structures designed to semantically enhance research publications by capturing and preserving the research method so that it can be reproduced in the future. This paper provides some motivation for an RO specification and briefly considers how existing domain-specifific ontologies might be integrated. It concludes by discussing the future direction of the myExperiment Ontology and how it will best support these ROs.}
}

@misc{myExperimentOntology2009,
title = {{myExperiment} Ontology Modules},
url = {http://web.archive.org/web/20091115080336/http\%3a\%2f\%2frdf.myexperiment.org/ontologies},
urldate = {2021-05-04},
year = {2009},
sciwheel-projects = {ro-crate-paper},
type = {WEBSITE}
}
@article{doi:10.3390/publications8020021,
title = {{FAIR} digital objects for science: from data pieces to actionable knowledge units},
author = {De Smedt, Koenraad and Koureas, Dimitris and Wittenburg, Peter},
pages = {21},
year = {2020},
month = {apr},
day = {11},
urldate = {2021-05-04},
journal = {Publications},
volume = {8},
number = {2},
issn = {2304-6775},
doi = {10.3390/publications8020021},
sciwheel-projects = {ro-crate-paper},
abstract = {Data science is facing the following major challenges: (1) developing scalable cross-disciplinary capabilities, (2) dealing with the increasing data volumes and their inherent complexity, (3) building tools that help to build trust, (4) creating mechanisms to efficiently operate in the domain of scientific assertions, (5) turning data into actionable knowledge units and (6) promoting data interoperability. As a way to overcome these challenges, we further develop the proposals by early Internet pioneers for Digital Objects as encapsulations of data and metadata made accessible by persistent identifiers. In the past decade, this concept was revisited by various groups within the Research Data Alliance and put in the context of the {FAIR} Guiding Principles for findable, accessible, interoperable and reusable data. The basic components of a {FAIR} Digital Object ({FDO}) as a self-contained, typed, machine-actionable data package are explained. A survey of use cases has indicated the growing interest of research communities in {FDO} solutions. We conclude that the {FDO} concept has the potential to act as the interoperable federative core of a hyperinfrastructure initiative such as the European Open Science Cloud ({EOSC}).}
}

@misc{doip2.0,
title = {Digital Object Interface Protocol Specification 2.0},
author = {{DONA} Foundation},
url = {https://www.dona.net/sites/default/files/2018-11/{DOIPv2Spec\_1}.pdf},
year = {2018},
month = {nov},
day = {14},
urldate = {2021-05-04},
sciwheel-projects = {ro-crate-paper},
abstract = {This document is a specification for the Digital Object Interface Protocol ({DOIP}), a core protocol of the Digital Object Architecture ({DO} Architecture; or {DOA}). The {DO} Architecture is a logical extension of the Internet architecture that addresses the need to support information management more generally than just conveying information in digital form from one location in the Internet to another. It is a non-proprietaryarchitecture and is publicly availablewithout charge. It is an outgrowth of earlier work on mobile programs,1and security for packet radio systems. The {DOIP} is intended to enable interoperability across heterogeneous information systems.},
journal = {{DONA} Foundation},
type = {WEBSITE}
}

@misc{fdof,
title = {{FAIR} Digital Object Framework Documentation},
url = {https://fairdigitalobjectframework.org/},
urldate = {2021-05-04},
sciwheel-projects = {ro-crate-paper},
type = {WEBSITE}
}

@misc{ebi_ftp_umgs2019,
title = {{FTP} index of /pub/databases/metagenomics/umgs\_analyses/},
author = {{EMBL-EBI Microbiome Informatics Team}},
url = {http://ftp.ebi.ac.uk/pub/databases/metagenomics/umgs\_analyses/},
year = {2019},
month = {sep},
day = {12},
urldate = {2021-05-04},
sciwheel-projects = {ro-crate-paper},
abstract = {{UMGS} genomes generated in this work were deposited in {ENA}, under the study accession {ERP108418}. The 92,143 {MAGs} with {QS} \textgreater50, as well as the quantification results from {BWA} and sourmash, all phylogenetic trees and the functional analysis results with {InterProScan}, {GP} and {GhostKOALA} are available at ftp://ftp.ebi.ac.uk/pub/databases/metagenomics/umgs\_analyses/. },
journal = {ftp.ebi.ac.uk},
notes = { Dataset of https://doi.org/10.1038/s41586-019-0965-1 },
type = {WEBSITE}
}


@misc{finn-lab-mgsgut,
title = {{GitHub} - Finn-Lab/{MGS}-gut: Analysing Metagenomic Species ({MGS})},
author = {{EMBL-EBI Microbiome Informatics Team}},
url = {https://github.com/Finn-Lab/{MGS}-gut},
urldate = {2021-05-04},
journal = {GitHub},
sciwheel-projects = {ro-crate-paper},
notes = { Scripts of https://doi.org/10.1038/s41586-019-0965-1 },
type = {WEBSITE}
}

@misc{sefton_blog_post_2021,
title = {{FAIR} Data Management; It's a lifestyle not a lifecycle - ptsefton.com},
author = {Sefton, Peter},
journal = {ptsefton.com},
url = {http://ptsefton.com/2021/04/07/rdmpic/},
year = {2021},
month = {apr},
day = {7},
urldate = {2021-05-04},
sciwheel-projects = {ro-crate-paper},
type = {BLOG\_POST}
}


@misc{goble_presentation_2016,
title = {What is Reproducibility? The R* Brouhaha},
author = {Goble, Carole},
url = {http://repscience2016.research-infrastructures.eu/img/{CaroleGoble}-{ReproScience2016v2}.pdf},
year = {2016},
month = {sep},
day = {9},
urldate = {2021-05-04},
address = {Hannover, Germany},
sciwheel-projects = {ro-crate-paper},
type = {Keynote}
}


@misc{soilandreyes_tweet_2020,
title = {I am looking for which bioinformatics journals encourage authors to submit their code/pipeline/workflow supporting data analysis},
author = {Soiland-Reyes, Stian},
journal = {Twitter},
url = {https://twitter.com/soilandreyes/status/1250721245622079488},
year = {2020},
month = {apr},
day = {16},
urldate = {2021-05-04},
sciwheel-projects = {ro-crate-paper},
abstract = {I am looking for which bioinformatics journals encourage authors to submit their code/pipeline/workflow *supporting* data analysis, but so far I have only found vague references or special software article types. Usual suspects missing. See thread below; any more..?},
type = {thread}
}

@misc{anaconda-bioconda,
title = {bioconda :: Anaconda.org},
url = {https://anaconda.org/bioconda/},
urldate = {2021-05-04},
sciwheel-projects = {ro-crate-paper},
type = {WEBSITE}
}
@misc{biocontainers-registry,
title = {{BioContainers} Community},
url = {https://biocontainers.pro/\#/registry},
urldate = {2021-05-04},
sciwheel-projects = {ro-crate-paper},
type = {WEBSITE}
}
@misc{ro-crate-terminology,
title = {Terminology - Research Object Crate ({RO}-Crate) Terminology \textbar Research Object Crate ({RO}-Crate)},
url = {https://www.researchobject.org/ro-crate/1.1/terminology},
urldate = {2021-05-04},
sciwheel-projects = {ro-crate-paper},
type = {WEBSITE}
}
@misc{ro-crate-issue-122,
title = {Use Case: {RO}-Crate as collection of contextual items · Issue \#122 · {ResearchObject}/ro-crate · {GitHub}},
url = {https://github.com/{ResearchObject}/ro-crate/issues/122},
urldate = {2021-05-04},
sciwheel-projects = {ro-crate-paper},
type = {WEBSITE}
}
@article{doi:10.1080/14490854.2020.1796500,
title = {Digital crowdsourcing and public understandings of the past: citizen historians meet Criminal Characters},
author = {Piper, Alana},
pages = {525-541},
url = {https://www.tandfonline.com/doi/full/10.1080/14490854.2020.1796500},
year = {2020},
month = {jul},
day = {2},
urldate = {2021-05-04},
journal = {History Australia},
volume = {17},
number = {3},
issn = {1449-0854},
doi = {10.1080/14490854.2020.1796500},
sciwheel-projects = {ro-crate-paper}
}
@incollection{doi:10.1515/9783110636352-017,
booktitle = {Making Histories},
author = {Piper, Alana},
title = {Chapter 16. crowdsourcing: citizen history and criminal characters},
editor = {Ashton, Paul and Evans, Tanya and Hamilton, Paula},
pages = {199-210},
publisher = {De Gruyter Oldenbourg},
year = {2020},
month = {sep},
day = {21},
urldate = {2021-05-04},
isbn = {9783110636352},
doi = {10.1515/9783110636352-017},
sciwheel-projects = {ro-crate-paper}
}
@misc{research-object-composer,
title = {{GitHub} - {ResearchObject}/research-object-composer: Research Object Composer, an {API} for incremental building and depositing research objects},
url = {https://github.com/researchobject/research-object-composer/},
urldate = {2021-05-04},
sciwheel-projects = {ro-crate-paper},
type = {WEBSITE}
}

@article{doi:10.1016/j.tibtech.2012.02.002,
title = {{ELIXIR}: a distributed infrastructure for European biological data.},
author = {Crosswell, Lindsey C and Thornton, Janet M},
pages = {241-242},
url = {http://dx.doi.org/10.1016/j.tibtech.2012.02.002},
year = {2012},
month = {may},
urldate = {2021-05-04},
journal = {Trends in Biotechnology},
volume = {30},
number = {5},
doi = {10.1016/j.tibtech.2012.02.002},
pmid = {22417641},
sciwheel-projects = {ro-crate-paper}
}
@incollection{doi:10.4018/978-1-60960-593-3.ch008,
booktitle = {Semantic services, interoperability and web applications: emerging concepts},
title = {Linked data: the story so far},
author = {Bizer, Christian and Heath, Tom and Berners-Lee, Tim},
editor = {Sheth, Amit},
pages = {205-227},
publisher = {{IGI} Global},
url = {http://services.igi-global.com/resolvedoi/resolve.aspx?doi=10.4018/978-1-60960-593-3.ch008},
year = {2011},
urldate = {2021-05-04},
isbn = {9781609605933},
doi = {10.4018/978-1-60960-593-3.ch008},
sciwheel-projects = {ro-crate-paper},
abstract = {The term {\textquotedblleftLinked} Data\textquotedblright refers to a set of best practices for publishing and connecting structured data on the Web. These best practices have been adopted by an increasing number of data providers over the last three years, leading to the creation of a global data space containing billions of assertions\textemdash the Web of Data. In this article, the authors present the concept and technical principles of Linked Data, and situate these within the broader context of related technological developments. They describe progress to date in publishing Linked Data on the Web, review applications that have been developed to exploit the Web of Data, and map out a research agenda for the Linked Data community as it moves forward.}
}
@article{doi:10.14454/3w3z-sa82,
title = {{DataCite} Metadata Schema Documentation for the Publication and Citation of Research Data and Other Research Outputs v4.4},
author = {{DataCite} Metadata Working Group},
url = {https://schema.datacite.org/meta/kernel-4.4/},
year = {2021},
urldate = {2021-05-05},
journal = {DataCite},
doi = {10.14454/3w3z-sa82},
sciwheel-projects = {ro-crate-paper},
abstract = {1 Introduction 1.1 The {DataCite} Consortium 1.2 {DataCite} Community Participation 1.3 The Metadata Schema 1.4 Version 4.4 Update 2 {DataCite} Metadata Properties 2.1 Overview 2.2 Citation 2.3 {DataCite} Properties 3 {XML} Example 4 {XML} Schema 5 Other {DataCite} Services Appendices Appendix 1: Controlled List Definitions Appendix 2: Earlier Version Update Notes Appendix 3: Standard values for unknown information Appendix 4: Version 4.1 Changes in support of software citation Appendix 5: {FORCE11} Software Citation Principles Mapping}
}
@article{doi:10.1016/j.websem.2015.01.003,
title = {Using a suite of ontologies for preserving workflow-centric research objects},
author = {Belhajjame, Khalid and Zhao, Jun and Garijo, Daniel and Gamble, Matthew and Hettne, Kristina and Palma, Raul and Mina, Eleni and Corcho, Oscar and G{\'{O}mez-Pérez, José Manuel and Bechhofer, Sean and Klyne, Graham and Goble, Carole},
pages = {16-42},
year = {2015},
month = {may},
journal = {Web Semantics: Science, Services and Agents on the World Wide Web},
volume = {32},
number = {0},
issn = {15708268},
doi = {10.1016/j.websem.2015.01.003},
sciwheel-projects = {ro-crate-paper},
abstract = {Scientific workflows are a popular mechanism for specifying and automating data-driven in silico experiments. A significant aspect of their value lies in their potential to be reused. Once shared, workflows become useful building blocks that can be combined or modified for developing new experiments. However, previous studies have shown that storing workflow specifications alone is not sufficient to ensure that they can be successfully reused, without being able to understand what the workflows aim to achieve or to re-enact them. To gain an understanding of the workflow, and how it may be used and repurposed for their needs, scientists require access to additional resources such as annotations describing the workflow, datasets used and produced by the workflow, and provenance traces recording workflow executions.In this article, we present a novel approach to the preservation of scientific workflows through the application of research objects-aggregations of data and metadata that enrich the workflow specifications. Our approach is realised as a suite of ontologies that support the creation of workflow-centric research objects. Their design was guided by requirements elicited from previous empirical analyses of workflow decay and repair. The ontologies developed make use of and extend existing well known ontologies, namely the Object Reuse and Exchange ({ORE}) vocabulary, the Annotation Ontology ({AO}) and the {W3C} {PROV} ontology ({PROVO}). We illustrate the application of the ontologies for building Workflow Research Objects with a case-study that investigates Huntington's disease, performed in collaboration with a team from the Leiden University Medial Centre ({HG}-{LUMC}). Finally we present a number of tools developed for creating and managing workflow-centric research objects.}
}
@article{doi:10.1093/bioinformatics/btx192,
title = {{BioContainers}: an open-source and community-driven framework for software standardization.},
author = {da Veiga Leprevost, Felipe and Grüning, Björn A and Alves Aflitos, Saulo and Röst, Hannes L and Uszkoreit, Julian and Barsnes, Harald and Vaudel, Marc and Moreno, Pablo and Gatto, Laurent and Weber, Jonas and Bai, Mingze and Jimenez, Rafael C and Sachsenberg, Timo and Pfeuffer, Julianus and Vera Alvarez, Roberto and Griss, Johannes and Nesvizhskii, Alexey I and Perez-Riverol, Yasset},
pages = {2580-2582},
year = {2017},
month = {aug},
day = {15},
journal = {Bioinformatics},
volume = {33},
number = {16},
doi = {10.1093/bioinformatics/btx192},
pmid = {28379341},
pmcid = {PMC5870671},
sciwheel-projects = {ro-crate-paper},
abstract = {Motivation: {BioContainers} (biocontainers.pro) is an open-source and community-driven framework which provides platform independent executable environments for bioinformatics software. {BioContainers} allows labs of all sizes to easily install bioinformatics software, maintain multiple versions of the same software and combine tools into powerful analysis pipelines. {BioContainers} is based on popular open-source projects Docker and rkt frameworks, that allow software to be installed and executed under an isolated and controlled environment. Also, it provides infrastructure and basic guidelines to create, manage and distribute bioinformatics containers with a special focus on omics technologies. These containers can be integrated into more comprehensive bioinformatics pipelines and different architectures (local desktop, cloud environments or {HPC} clusters). Availability and Implementation: The software is freely available at github.com/{BioContainers}/. Contact: yperez@ebi.ac.uk. \copyright The Author(s) 2017. Published by Oxford University Press.}
}
@inproceedings{doi:10.1109/BigData.2016.7840618,
title = {I'll take that to go: Big data bags and minimal identifiers for exchange of large, complex datasets},
author = {Chard, Kyle and D'Arcy, Mike and Heavner, Ben and Foster, Ian and Kesselman, Carl and Madduri, Ravi and Rodriguez, Alexis and Soiland-Reyes, Stian and Goble, Carole and Clark, Kristi and Deutsch, Eric W. and Dinov, Ivo and Price, Nathan and Toga, Arthur},
pages = {319-328},
publisher = {IEEE},
url = {https://static.aminer.org/pdf/fa/bigdata2016/BigD418.pdf},
year = {2016},
month = {dec},
day = {5},
urldate = {2018-07-13},
isbn = {978-1-4673-9005-7},
doi = {10.1109/{BigData}.2016.7840618},
sciwheel-projects = {ro-crate-paper},
abstract = {Big data workflows often require the assembly and exchange of complex, multi-element datasets. For example, in biomedical applications, the input to an analytic pipeline can be a dataset consisting thousands of images and genome sequences assembled from diverse repositories, requiring a description of the contents of the dataset in a concise and unambiguous form. Typical approaches to creating datasets for big data workflows assume that all data reside in a single location, requiring costly data marshaling and permitting errors of omission and commission because dataset members are not explicitly specified. We address these issues by proposing simple methods and tools for assembling, sharing, and analyzing large and complex datasets that scientists can easily integrate into their daily workflows. These tools combine a simple and robust method for describing data collections ({BDBags}), data descriptions (Research Objects), and simple persistent identifiers (Minids) to create a powerful ecosystem of tools and services for big data analysis and sharing. We present these tools and use biomedical case studies to illustrate their use for the rapid assembly, sharing, and analysis of large datasets.},
booktitle = {2016 {IEEE} International Conference on Big Data (Big Data)}
}
@misc{bioschemas_2017,
title = {Bioschemas: From Potato Salad to Protein Annotation},
author = {Gray, Alasdair and Goble, Carole and Jimenez, Rafael and Bioschemas Community,},
url = {https://iswc2017.semanticweb.org/paper-579/},
year = {2017},
month = {oct},
day = {23},
urldate = {2021-05-05},
address = {Vienna, Austria},
sciwheel-projects = {ro-crate-paper},
abstract = {The life sciences have a wealth of data resources with a wide range of overlapping content. Key repositories, such as {UniProt} for protein data or Entrez Gene for gene data, are well known and their content easily discovered through search engines. However, there is a long-tail of bespoke datasets with important content that are not so prominent in search results. Building on the success of Schema.org for making a wide range of structured web content more discoverable and interpretable, e.g. food recipes, the Bioschemas community (http://bioschemas.org) aim to make life sciences datasets more findable by encouraging data providers to embed Schema.org markup in their resources.},
type = {Poster}
}
@misc{pcdm,
title = {Portland Common Data Model},
author = {Cossu, Stefano and Cowles, Esmé and Estlund, Karen and Harlow, Christina and Johnson, Tom and Matienzo, Mark and Lamb, Danny and Rayle, Lynette and Sanderson, Rob and Stroop, Jon and Woods, Andrew},
url = {https://github.com/duraspace/pcdm/wiki},
year = {2018},
month = {jun},
day = {15},
urldate = {2021-05-05},
sciwheel-projects = {ro-crate-paper},
abstract = {The Portland Common Data Model ({PCDM}) is a flexible, extensible domain model that is intended to underlie a wide array of repository and {DAMS} applications. The primary objective of this model is to establish a framework that developers of tools (e.g., Samvera-based engines, such as Hyrax, Hyku, Sufia, and Avalon; Islandora; custom Fedora sites) can use for working with models in a general way, allowing adopters to easily use custom models with any tool. Given this interoperability goal, the initial work has been focused on structural metadata and access control, since these are the key actionable metadata.},
journal = {{GitHub} duraspace/pcdm Wiki},
type = {WEBSITE}
}
@article{doi:10.1126/science.aah6168,
title = {Enhancing reproducibility for computational methods},
author = {Stodden, Victoria and {McNutt}, Marcia and Bailey, David H and Deelman, Ewa and Gil, Yolanda and Hanson, Brooks and Heroux, Michael A and Ioannidis, John P A and Taufer, Michela},
pages = {1240-1241},
year = {2016},
month = {dec},
day = {9},
urldate = {2021-05-05},
journal = {Science},
volume = {354},
number = {6317},
issn = {0036-8075},
doi = {10.1126/science.aah6168},
pmid = {27940837},
sciwheel-projects = {ro-crate-paper}
}

@article{doi:10.1371/journal.pcbi.1003285,
title = {Ten simple rules for reproducible computational research.},
author = {Sandve, Geir Kjetil and Nekrutenko, Anton and Taylor, James and Hovig, Eivind},
pages = {e1003285},
year = {2013},
month = {oct},
day = {24},
urldate = {2018-07-13},
journal = {{PLoS} Computational Biology},
volume = {9},
number = {10},
doi = {10.1371/journal.pcbi.1003285},
pmid = {24204232},
pmcid = {PMC3812051},
sciwheel-projects = {ro-crate-paper}
}
@article{doi:10.1093/nar/gkl971,
title = {The worldwide Protein Data Bank ({wwPDB}): ensuring a single, uniform archive of {PDB} data.},
author = {Berman, Helen and Henrick, Kim and Nakamura, Haruki and Markley, John L},
pages = {D301-3},
year = {2007},
month = {jan},
urldate = {2021-05-05},
journal = {Nucleic Acids Research},
volume = {35},
number = {Database issue},
doi = {10.1093/nar/gkl971},
pmid = {17142228},
pmcid = {PMC1669775},
sciwheel-projects = {ro-crate-paper},
abstract = {The worldwide Protein Data Bank ({wwPDB}) is the international collaboration that manages the deposition, processing and distribution of the {PDB} archive. The online {PDB} archive is a repository for the coordinates and related information for more than 38 000 structures, including proteins, nucleic acids and large macromolecular complexes that have been determined using X-ray crystallography, {NMR} and electron microscopy techniques. The founding members of the {wwPDB} are {RCSB} {PDB} ({USA}), {MSD}-{EBI} (Europe) and {PDBj} (Japan) [H.M. Berman, K. Henrick and H. Nakamura (2003) Nature Struct. Biol., 10, 980]. The {BMRB} group ({USA}) joined the {wwPDB} in 2006. The mission of the {wwPDB} is to maintain a single archive of macromolecular structural data that are freely and publicly available to the global community. Additionally, the {wwPDB} provides a variety of services to a broad community of users. The {wwPDB} website at http://www.wwpdb.org/ provides information about services provided by the individual member organizations and about projects undertaken by the {wwPDB}.}
}
@article{doi:10.3897/biss.3.37080,
title = {Zenodo, an Archive and Publishing Repository: A tale of two herbarium specimen pilot projects},
author = {Dillen, Mathias and Groom, Quentin and Agosti, Donat and Nielsen, Lars},
year = {2019},
month = {jun},
day = {18},
journal = {Biodiversity Information Science and Standards},
volume = {3},
issn = {2535-0897},
doi = {10.3897/biss.3.37080},
sciwheel-projects = {ro-crate-paper},
abstract = {Zenodo (https://zenodo.org) is an open-access repository operated by {CERN} (European Organization for Nuclear Research), which provides researchers with an easy and stable platform to archive and publish their data and other output, such as software tools, manuals and project reports. In the context of the {ICEDIG} (Innovation and Consolidation for Large scale Digitisation of Natural Heritage) project, Zenodo was investigated for its usability as a platform where digitized images of collection specimens could be archived and published. In a production digitization pipeline, we foresee the automated archiving of daily image production. If Zenodo could be used for this purpose, such a process would also immediately mean that data and images are published {FAIR}-ly (Findable, Accessible, Interoperable and Reusable) within hours of their creation. To evaluate performance of the system, we first used a test dataset of 1800 herbarium specimen images, which was uploaded using Zenodo's {API} (Application Programming Interface) (Dillen et al. 2019). This dataset includes lossless {TIFF} images, label-segmented overlays and {JSON}-{LD} ({JavaScript} Object Notation for Linked Data) metadata using {DwC} (Darwin Core) terminology, constituting over 208 gigabytes of data. In addition, for all individual digital specimens the data about the specimen (in {DwC}) as well as metadata about its deposition on Zenodo (in Zenodo's internal data model) were available in multiple machine-readable formats. All data in {DwC} were provided as linked data with their {DwC} identifiers (e.g. http://rs.tdwg.org/dwc/terms/{basisOfRecord}). All individual specimens received minted {DOIs} (Digital Object Identifiers). A second upload of 280,000 herbarium {JPEG} images from a single institution (ca. 1 terabyte of data) with limited metadata (but using the same approach) was launched as well. In this presentation, the workflow for proper usage of the {API} will be described as well as some performance metrics, flexibilities and functionalities of the platform. Some issues and potential developments to tackle them will be discussed. Currently, the rate of ingestion into Zenodo seems only fast enough for small scale digitization pipelines. However, a modest improvement in transfer rate would make this a realistic proposition for large volume usage.}
}
@book{isbn:9781315351148,
title = {Data Stewardship for Open Science},
author = {Mons, Barend},
pages = {240},
publisher = {Taylor \& Francis},
urldate = {2021-05-05},
edition = {1},
isbn = {9781315351148},
sciwheel-projects = {ro-crate-paper}
}

@misc{doi:10.48546/workflowhub.workflow.56.1,
title = {Protein Ligand Complex {MD} Setup tutorial using {BioExcel} Building Blocks (biobb) (jupyter notebook)},
author = {Lowe, Douglas and Bayarri, Genís},
year = {2021},
journal = {WorkflowHub},
doi = {10.48546/workflowhub.workflow.56.1},
sciwheel-projects = {ro-crate-paper},
abstract = {\# Summary This tutorial aims to illustrate the process of setting up a simulation system containing a protein in complex with a ligand, step by step, using the {BioExcel} Building Blocks library (biobb). The particular example used is the T4 lysozyme {L99A}/{M102Q} protein ({PDB} code {3HTB}), in complex with the 2-propylphenol small molecule (3-letter Code {JZ4}). Workflow engine is a jupyter notebook. It can be run in binder, following the link given, or locally. Auxiliar libraries used are: nb\textbackslash\_conda\_kernels, nglview, ipywidgets, os, plotly, and simpletraj. Environment can be setup using the included environment.yml file. \# Parameters \#\# Inputs Parameters needed to configure the workflow: * **{pdbCode}**: {PDB} code of the protein-ligand complex structure (e.g. {3HTB}) * **{ligandCode}**: Small molecule 3-letter code for the ligand structure (e.g. {JZ4}) * **mol\textbackslash\_charge**: Charge of the small molecule, needed to add hydrogen atoms. \#\# Outputs Output files generated (named according to the input parameters given above): * **output\textbackslash\_md\textbackslash\_gro**: final structure of the {MD} setup protocol * **output\textbackslash\_md\textbackslash\_trr**: final trajectory of the {MD} setup protocol * **output\textbackslash\_md\textbackslash\_cpt**: final checkpoint file * **output\textbackslash\_gppmd\textbackslash\_tpr**: final tpr file * **output\textbackslash\_genion\textbackslash\_top\textbackslash\_zip**: final topology of the {MD} system},
type = Workflow
}

@article{doi:10.5281/zenodo.3381754,
title = {Application of {BagIt}-Serialized Research Object Bundles for Packaging and Re-execution of Computational Analyses},
author = {Chard, Kyle and Gaffney, Niall and Jones, Matthew B. and Kowalik, Kacper and Ludaescher, Bertram and {McPhillips}, Timothy and Nabrzyski, Jarek and Stodden, Victoria and Taylor, Ian and Thelen, Thomas and Turk, Matthew J. and Willis, Craig},
year = {2019},
urldate = {2021-05-05},
journal = {Zenodo},
doi = {10.5281/zenodo.3381754},
sciwheel-projects = {ro-crate-paper},
abstract = {In this paper we describe our experience adopting the Research Object Bundle ({RO}-Bundle) format with {BagIt} serialization ({BagIt}-{RO}) for the design and implementation of "tales" in the Whole Tale platform. A tale is an executable research object intended for the dissemination of computational scientific findings that captures information needed to facilitate understanding, transparency, and re-execution for review and computational reproducibility at the time of publication. We describe the Whole Tale platform and requirements that led to our adoption of {BagIt}-{RO}, specifics of our implementation, and discuss migrating to the emerging Research Object Crate ({RO}-Crate) standard.}
}

@misc{about-lifemonitor,
author = {CRS4},
title = {{LifeMonitor}, a testing and monitoring service for scientific workflows},
url = {https://crs4.github.io/life_monitor/},
urldate = {2021-05-07},
type = {WEBSITE}
}


@article{doi:10.5281/zenodo.4705074,
title = {{EOSC}-Life Common Provenance Model},
author = {Wittner, Rudolf and Mascia, Cecilia and Frexia, Francesca and Müller, Heimo and Geiger, Jörg and Exter, Katrina and Holub, Petr},
year = {2021},
journal = {Zenodo},
doi = {10.5281/zenodo.4705074},
sciwheel-projects = {ro-crate-paper},
abstract = {The exchange of research data and physical specimens has become an issue of major importance for modern research. Many reports indicate problems with quality, trustworthiness and reproducibility of research results, mainly due to poor documentation of the data generation or the collection of specimens. The significant impact of flawed research results on health, economics and political decisions has frequently been stated. Consequently, professional societies and research initiatives call for improved and standardised documentation of the data and specimens used in research studies. Provenance information documents the evolution of an object and can be used to assess its quality and reliability. This deliverable defines components of distributed provenance information to enable interlinking of provenance information generated in different organisations involved in the research process, such as biobanks, research centres, universities or analytical laboratories. The distributed provenance information model builds on an existing provenance information standard, {W3C} {PROV}, and follows a general provenance composition pattern. Both {W3C} {PROV} and provenance composition pattern is described in this document. Since understanding of the term \textquotedblleftprovenance information\textquotedblright differs across different domains and research communities, this deliverable firstly harmonises this understanding by providing a general explanation of how provenance information is generated and used. In particular, this deliverable defines a connector, that is a provenance component containing technical information to traverse through provenance information. The connector is subsequently added to provenance information generated by different organisations. This deliverable also defines how to interpret identifiers of provenance structures in a distributed environment and how to include and interpret persistent identifiers of documented objects. This deliverable deals with the common provenance model developed as a part of a standardisation process in the International Organisation for Standardisation ({ISO}) technical committee {\textquotedblleftBiotechnology\textquotedblright} {ISO}/{TC} 276, and which is registered as project {ISO} 23494 in the working group 5 {\textquotedblleftData} processing and Integration\textquotedblright. Because this work is copyrighted by {ISO} and cannot be published as a public deliverable, this text describes the essential design of the provenance model, and the actual {ISO} document is provided as a non-public supplement. This is in line with the work plan of {EOSC}-Life {WP6} in order to support adoption of the standard both in academia and in industry. The Common Provenance Model has been accepted as a Preliminary Work Item under 23494 Part 2 and it is being proposed for moving it into the next phase, the New Work Item at the time of submitting the deliverable.}
}
@article{doi:10.5281/zenodo.4705078,
title = {{EOSC}-Life Methodology framework to enhance reproducibility within {EOSC}-Life},
author = {Bietrix, Florence and Carazo, José Maria and Capella-Gutierrez, Salvador and Coppens, Frederik and Chiusano, Maria Luisa and David, Romain and Fernandez, Jose Maria and Fratelli, Maddalena and Heriche, Jean-Karim and Goble, Carole and Gribbon, Philip and Holub, Petr and P. Joosten, Robbie and Leo, Simone and Owen, Stuart and Parkinson, Helen and Pieruschka, Roland and Pireddu, Luca and Porcu, Luca and Raess, Michael and Rodriguez- Navas, Laura and Scherer, Andreas and Soiland-Reyes, Stian and Tang, Jing},
year = {2021},
month = {apr},
day = {30},
journal = {Zenodo},
doi = {10.5281/zenodo.4705078},
abstract = {The original scope of task 8.3 is to develop metrics to assess the impact on reproducibility of the availability of life-science open data and workflows in the cloud. A great part of the activities within {EOSC}-Life is actually related to reproducibility and provides in several ways tools that will have an impact. Therefore, we decided that it would be more informative to describe such activities and to explain why and how they will have an impact on reproducibility in life sciences, instead of providing abstract metrics for such an impact. For these reasons, we changed the title of the deliverable, from \textquotedblleftframework to assess ...\textquotedblright to \textquotedblleftframework to enhance reproducibility\textquotedblright. First of all, we reasoned on what can be the contribution of open science to improve the reproducibility of research. Publicly sharing data, protocols, tools and computational workflows makes it possible to compare or combine the data and outcomes from different studies within a discipline as well as integrate data across scientific domains. It allows conclusions to be validated and possibly corrected as well as being reinforced by meta-analyses. Replication data and test/training data can also be used in many applications to contribute to reproducible research. Moreover, new hypotheses, different from the original aims of the study, can be explored. Data sets can be re-used to develop and test new methods, to conduct scientific and technical benchmarking activities and to support training activities. Therefore, in addition to generating more value from research investments, data sharing has the potential to increase confidence in research outcomes and increase knowledge dissemination. These benefits of open sharing have long been recognized in some fields such as bioinformatics, which has a long history of publicly sharing data with, for example, public repositories for nucleotide sequences going back 30 years and the Protein Data Bank ({PDB}), a repository of information about the {3D} structures of proteins, nucleic acids and complex assemblies, that celebrates its 50th birthday this year. In this notion, any improvement in sharing of data, tools and workflows among scientists and across disciplines, that is the aim of {EOSC}-Life and the wider {EOSC}, will contribute to reproducible science. In addition to this general scope, several specific actions to frame transparency in the reporting of experimental protocols, data and analytical workflows warrant the reproducibility of each single object (experimental results, data or workflows) that is made available on the cloud. We will describe here the initiatives in {EOSC}-Life to implement existing tools for reproducibility as well as to develop new tools for its enhancement. As the final goal of {EOSC}-Life is to make data resources available to the wider community of life scientists, although necessarily technical in several points, this document aims at a general readership, including experimental in addition to data scientists.}
}


@article{doi:10.1093/gigascience/giz095,
title = {Sharing interoperable workflow provenance: A review of best practices and their practical application in {CWLProv}},
author = {Khan, Farah Zaib and Soiland-Reyes, Stian and Sinnott, Richard O and Lonie, Andrew and Goble, Carole and Crusoe, Michael R},
year = {2019},
month = {nov},
day = {1},
journal = {GigaScience},
volume = {8},
number = {11},
doi = {10.1093/gigascience/giz095},
pmid = {31675414},
pmcid = {PMC6824458},
abstract = {{BACKGROUND}: The automation of data analysis in the form of scientific workflows has become a widely adopted practice in many fields of research. Computationally driven data-intensive experiments using workflows enable automation, scaling, adaptation, and provenance support. However, there are still several challenges associated with the effective sharing, publication, and reproducibility of such workflows due to the incomplete capture of provenance and lack of interoperability between different technical (software) platforms. {RESULTS}: Based on best-practice recommendations identified from the literature on workflow design, sharing, and publishing, we define a hierarchical provenance framework to achieve uniformity in provenance and support comprehensive and fully re-executable workflows equipped with domain-specific information. To realize this framework, we present {CWLProv}, a standard-based format to represent any workflow-based computational analysis to produce workflow output artefacts that satisfy the various levels of provenance. We use open source community-driven standards, interoperable workflow definitions in Common Workflow Language ({CWL}), structured provenance representation using the {W3C} {PROV} model, and resource aggregation and sharing as workflow-centric research objects generated along with the final outputs of a given workflow enactment. We demonstrate the utility of this approach through a practical implementation of {CWLProv} and evaluation using real-life genomic workflows developed by independent groups. {CONCLUSIONS}: The underlying principles of the standards utilized by {CWLProv} enable semantically rich and executable research objects that capture computational workflows with retrospective provenance such that any platform supporting {CWL} will be able to understand the analysis, reuse the methods for partial reruns, or reproduce the analysis to validate the published findings. \copyright The Author(s) 2019. Published by Oxford University Press.}
}

@conference{doi:10.5281/zenodo.51314,
title = {Tracking Workflow Execution With {TavernaPROV}},
author = {Soiland-Reyes, Stian and Alper, Pinar and Goble, Carole},
year = {2016},
month = Jun,
day = 6,
booktitle = {{ProvenanceWeek} 2016},
note = {PROV: Three Years Later},
organizer = { W3C },
doi = {10.5281/zenodo.51314},
abstract = {Apache Taverna is a scientific workflow system for combining web services and local tools. Taverna records provenance of workflow runs, intermediate values and user interactions, both as an aid for debugging while designing the workflow, but also as a record for later reproducibility and comparison. Taverna also records provenance of the evolution of the workflow definition (including a chain of {wasDerivedFrom} relations), attributions and annotations; for brevity we here focus on how Taverna's workflow run provenance extends {PROV} and is embedded with Research Objects.}
}

@article{doi:10.5281/zenodo.3903463,
title = {{BrennerG}/Ro-Crate_2_ma-{DMP}: v1.0.0},
author = {Brenner, Gabriel},
year = {2020},
journal = {Zenodo},
doi = {10.5281/zenodo.3903463},
abstract = {First Release of {RO}-Crate - {maDMP} Parser}
}
@conference{doi:10.4126/frl01-006423291,
title = {Research Object Crates and Machine-actionable Data Management Plans},
booktitle = {1st Workshop on Research Data Management for Linked Open Science},
author = {Miksa, Tomasz and Jaoua, Maroua and Arfaoui, Ghaith},
year = {2020},
journal = {PUBLISSO},
doi = {10.4126/frl01-006423291},
sciwheel-projects = {ro-crate-paper}
}

@article{doi:10.15497/rda00039,
title = {{RDA} {DMP} Common Standard for Machine-actionable Data Management Plans},
author = {Walk, Paul and Miksa, Tomasz and Neish, Peter},
year = {2019},
journal = {Research Data Alliance},
doi = {10.15497/rda00039},
sciwheel-projects = {ro-crate-paper}
}
@article{doi:10.1371/journal.pcbi.1006750,
title = {Ten principles for machine-actionable data management plans.},
author = {Miksa, Tomasz and Simms, Stephanie and Mietchen, Daniel and Jones, Sarah},
pages = {e1006750},
year = {2019},
month = {mar},
day = {28},
journal = {{PLoS} Computational Biology},
volume = {15},
number = {3},
doi = {10.1371/journal.pcbi.1006750},
pmid = {30921316},
pmcid = {PMC6438441},
sciwheel-projects = {ro-crate-paper},
abstract = {Data management plans ({DMPs}) are documents accompanying research proposals and project outputs. {DMPs} are created as free-form text and describe the data and tools employed in scientific investigations. They are often seen as an administrative exercise and not as an integral part of research practice. There is now widespread recognition that the {DMP} can have more thematic, machine-actionable richness with added value for all stakeholders: researchers, funders, repository managers, research administrators, data librarians, and others. The research community is moving toward a shared goal of making {DMPs} machine-actionable to improve the experience for all involved by exchanging information across research tools and systems and embedding {DMPs} in existing workflows. This will enable parts of the {DMP} to be automatically generated and shared, thus reducing administrative burdens and improving the quality of information within a {DMP}. This paper presents 10 principles to put machine-actionable {DMPs} ({maDMPs}) into practice and realize their benefits. The principles contain specific actions that various stakeholders are already undertaking or should undertake in order to work together across research communities to achieve the larger aims of the principles themselves. We describe existing initiatives to highlight how much progress has already been made toward achieving the goals of {maDMPs} as well as a call to action for those who wish to get involved.}
}

@article{doi:10.5281/zenodo.3922136,
title = {{RO}-Crate {RDA} {maDMP} Mapper},
author = {Arfaoui, Ghaith and Jaoua, Maroua},
year = {2020},
journal = {Zenodo},
doi = {10.5281/zenodo.3922136}
}

@article{doi:10.5281/zenodo.3944464,
title = {New version of the {DMP} Common Standard Ontology},
author = {Cardoso, João and Ekaputra, Fajar J. and Garcia, Leyla and Jacquemot, Christine},
year = {2020},
journal = {Zenodo},
doi = {10.5281/zenodo.3944464},
sciwheel-projects = {ro-crate-paper},
abstract = {The development of the {DMP} Common Standard Ontology ({DCSO}) is an ongoing effort intended to provide semantic technology representation of the {DCS} application profile. The objective of this Issue was to create a new version of the {DCSO} that complies with the best practices in the community. For this hackathon there were 3 main sub-issues to tackle: (1) Third-party concept integration; (2) controlled vocabularies; and (3) constraint representation.}
}
@article{arxiv:2103.13138v1,
title = {{SCHeMa}: Scheduling Scientific Containers on a Cluster of Heterogeneous Machines},
author = {Vergoulis, Thanasis and Zagganas, Konstantinos and Kavouras, Loukas and Reczko, Martin and Sartzetakis, Stelios and Dalamagas, Theodore},
pages = {2103.13138},
url = {https://arxiv.org/abs/2103.13138v1},
year = {2021},
month = {mar},
day = {24},
journal = {arXiv},
abstract = {In the era of data-driven science, conducting computational experiments thatinvolve analysing large datasets using heterogeneous computational clusters, is part of the everyday routine for many scientists. Moreover, to ensure the credibility of their results, it is very important for these analyses to be easily reproducible by other researchers. Although various technologies, that could facilitate the work of scientists in this direction, have been introduced in the recent years, there is still a lack of open source platforms that combine them to this end. In this work, we describe and demonstrate {SCHeMa}, an open-source platform that facilitates the execution and reproducibility of computational analysis on heterogeneous clusters, leveraging containerization,experiment packaging, workflow management, and machine learning technologies.}
}

@inproceedings{doi:10.1109/eScience.2019.00068,
title = {Application of {BagIt}-Serialized Research Object Bundles for Packaging and Re-Execution of Computational Analyses},
author = {Chard, Kyle and Gaffney, Niall and Jones, Matthew B. and Kowalik, Kacper and Ludascher, Bertram and {McPhillips}, Timothy and Nabrzyski, Jarek and Stodden, Victoria and Taylor, Ian and Thelen, Thomas and Turk, Matthew J. and Willis, Craig},
pages = {514-521},
publisher = {IEEE},
url = {https://ieeexplore.ieee.org/document/9041738/},
year = {2019},
month = {sep},
day = {24},
urldate = {2021-04-27},
isbn = {978-1-7281-2451-3},
doi = {10.1109/{eScience}.2019.00068},
sciwheel-projects = {ro-crate-paper},
booktitle = {15th International Conference on {eScience} ({eScience} 2019)}
}
@article{doi:10.3897/rio.6.e57602,
title = {Landscape analysis for the Specimen Data Refinery},
author = {Walton, Stephanie and Livermore, Laurence and Bánki, Olaf and Cubey, Robert and Drinkwater, Robyn and Englund, Markus and Goble, Carole and Groom, Quentin and Kermorvant, Christopher and Rey, Isabel and Santos, Celia and Scott, Ben and Williams, Alan and Wu, Zhengzhe},
year = {2020},
month = {aug},
day = {14},
journal = {Research Ideas and Outcomes},
volume = {6},
issn = {2367-7163},
doi = {10.3897/rio.6.e57602},
abstract = {This report reviews the current state-of-the-art applied approaches on automated tools, services and workflows for extracting information from images of natural history specimens and their labels. We consider the potential for repurposing existing tools, including workflow management systems; and areas where more development is required. This paper was written as part of the {SYNTHESYS}+ project for software development teams and informatics teams working on new software-based approaches to improve mass digitisation of natural history specimens.}
}
